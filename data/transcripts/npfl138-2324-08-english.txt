Good afternoon, everybody. It's my great joy to welcome you on the next lecture of our course.
We are actually already in the middle because six courses or six lectures are passed.
And there are only seven more including in the today's one.
So congrats for getting into half of the course.
So just once more the same effort you have put into it and everything will be done and pitch in everything.
So before I will start myself, are there maybe some questions or comments or anything?
Okay, my turn.
We discussed conclusions for the past three lectures at the end.
We talked also about some more advanced architectures as we have experienced when you are when you were in
Plamentic VSCVH and Competition.
But now we will do some kind of a restart and we will start talking about some other way or some other ideas how we could construct neural networks.
So conclusions were good at doing some local processing where we had like pixels and there's
Roundings and we performed some kind of local processing and then we have built the whole set of architectures around it.
And now we'll put that aside and we will start talking about recurrent neural networks.
The recurrent neural networks are a different way of coming up with neural networks.
But this time it will be all centered around sequences.
So the idea is that our inputs have, well usually a temporal dimensions, which is like a sequence of elements.
You can think about, I don't know, stock prices going up and down.
You can think about speech in a sense that this amplitude of the sound which I'm making by my vocal cords gets into your ear.
Then you could record those.
And then you would also have a sequence.
Although a sentence is a sequence in a sense that it starts somewhere and ends at some point.
Either you can think about it as like a temporal thing that when I'm speaking, I say the words sequentially or you can think about the writing system.
We're very some kind of ordering generated by the writer.
So that's what we will do now.
We will assume now that we have gotten a sequence and we would like somehow to process.
Whatever we will come up with, should be able to handle sequences of arbitrary length.
Right? So it's not like we will get three elements on the input and that will be our sequence.
That's something we did in the uppercase competitions somehow, right?
We just said some number of characters before some number of characters after, but right now we will want to handle small, large sequences.
And we will do that by this recurrent kind of idea.
So the main approach here is that we will have some processing unit, usually called the processing cell, which will process the sequence elements one by one.
Right? So we will get one element of the sequence.
This cell will compute some output which I will discuss momentarily.
And then it will also generate some kind of a state.
What I mean by state is the information which we have learned from the sequence so far.
Not necessarily depending on this input element, but also on the ones which we have processed in the past.
And then this state will be used as input to this cell.
Whenever we are processing the second or the following sequence element.
This state it will represent the sequence from the beginning until until this point.
And then the question is what is this output, right?
So either the output could be the state.
So we could generate in the output like the digest, which we have learned from the whole sequence until now.
In some cases the output might be like a good representation of the input element, but conditioned on the elements which we have seen already.
So it is like a context to analyze representation, not just the element by itself, but in relation to the other sequence elements which we have seen so far.
So this cell can be thought of as a function which gets like a digest.
Like some state representation of the prefix of the sequence, then it gets another element.
And the goal is to produce a new digest, a new state, new representation of the sequence from the beginning until an including of an element being processed.
And then an output can be task specific or task dependent.
So it might include all the information about the sequence so far or not.
It might only generate representation of the input element mostly like focused on the input, but using the information from the prefix of the sequence as well.
Until now all our networks had to be a cyclic, right?
We need to have some order in which we will run the layers, which this image doesn't really seem to be.
So what I mean by this image is that when we get a sequence of multiple elements, we will process them one by one using the cell, which we will replicate the many times.
As as necessary. So this is the same cell same layers with the shared weights, which we just apply or use as many times as there are input elements, right?
So we got the sequence here.
We will produce a sequence from the output and also internally we will have a state where we got our processing the whole sequence.
In this setting, sometimes it's called unrolled cells.
We, there are no more cycles, right? So we'd like to see in the recurrent formulation like this recursive edge that's why it's called recurrent because we are just using the same thing over and over.
It's no longer there because when we expanded in this timex case, the other cycles go away.
So these inputs, there are vectors as usual, right?
So there are some representations of the sequence elements and it might even be something more than a vector, but mostly it will be a vector.
So when we say that we want to use such a cell, what we will do is we will specify the dimensionality of the output, right?
That's what we do with the full connected layers, we say, yeah, this layer will have 20 outputs.
Also what we did with convolutional layers, when we said we want the convolution to have 20 filters, so for each pixel or for each coordinate, we generated 20 values.
Here again, we'll say okay, let's create a cell with 20 outputs, for example.
So that's the output size. The input size can be arbitrary, it can be something, unrelated to the output size.
It will be somehow processed inside.
As it is with the full connected layer, right? We don't care.
A large of the input is, we will just produce as many elements as we will.
The size of the state that depends on the specific cell, which will be using, but mostly it will be of same or similar size.
To the output one, but the output one is the one, which we will define when we want to create a cell.
So in the first half of the lecture, today's lecture, we will discuss how the insights of these cells might look, so that the networks like this one will train well.
It is actually non-chivell, so we will see some new ideas or maybe slightly new ideas.
And then once we know how such cells can be constructed, we will start talking about how we can actually use them to solve whatever tasks we might want to solve.
So that's the better plan for today.
So regarding the insights of the cell, a very simple solution is just to use a single fully connected layer, right?
So what we get on the input is the element, sometimes denoted as xt, right? So we are in a times t, so xt is the current one.
And then we get the previous state, which is denoted here as h, n minus minus 1s.
Sometimes the states are s, sometimes they are h, like in the states.
And our goal is to produce an output in a state, so in the simplest case, both of them will be the same.
So whatever will be our output will also be, although be the state, the thing which we will send with an xtime step.
So we can take these two inputs, concatenate them, do a fully connected layer, and then get the output of the required size.
And then just use it both as the state and sdl.
Right, so that is like an image showing that when these two arrows merge, what I mean is we will concatenate the dimensions,
then there is a fully connected layer in some activation.
So usually, in this simple cell, the activation function, which we will use on the layer, is tonnage.
Now that's unusual, right? Because you are like in a tonnage, but not that's so great.
The values are the activations of choice. So what's happening here is that once we start training ourselves, one of the largest issues which we will deal with,
well, being to make sure that the size of the activations and size of the gradients are reasonably limited.
But that's the main issue which we want to deal with when training, right? We want the size of the gradients or the whole network to be roughly similar,
and we have designed initializations and regionalizations and many things around this, even in the dropout, we deal with the fact that when we decrease the volume, we need to increase it again.
So here we want to make sure that the same is true. And so the using tonnage as an activation has the nice side effect that the activations will not be unbounded,
because tonnage is always limited in the minus one to one range. So it will not happen by design that the activations will grow without any limit.
So that's why we are doing something unusual because we are, well, preventing the catastrophe where the activations would grow very large and then in the backward path, the gradients would grow very large and it would be bad.
So apart from the unusual activation, this is really just a fully connected layer when we concatenate these two things.
So either we can write it down as to independent fully connected layers, like one for the inputs and the other for the state, so we would two way it matrices and then we end them up, so we could even have two biases in this formulation I have only one bias.
But sometimes what we could do is we could take the state and the input element, concatenate them, I don't know, I will concatenate them, these two signs right.
And then take this concatenated vector and just multiply it by a single weight matrix.
Now this is actually the same, it doesn't really matter whether you will do the fully connected layer first and edit up or concatenate the vectors and then do the fully connected layers like the idea because the difference is just what are the weight matrices.
So in this on this image, what we call V here, right, that are the weights from the inputs to the outputs and what is called U here are the weights between the states and the output.
And in this formulation where we have concatenated, then this W just represent this overall matrix, which goes from all the inputs to the output, so this W is really just the concatenation of U and B.
So we will be seeing both of them, so don't mind them, they are the same.
So this kind of a cell is called simple RNN in Keras and it's just called RNN in Biterge.
And the simple RNN name says that something will be not great with this architecture because we will probably have some more advanced ones, so what the problem with such cells is.
First people have been using this for 30 years or something, so it's not like it's something brand new in deep learning, but actually people try to do this recurrent processing called already in the 90s.
But the problem is that training is simple cell suffers a lot from this imbalance, so that the gradients will either explode or vanish.
So we have make sure that they won't explode by bounding activations, but they still could vanish, vanish easily.
What I mean by vanish is that if you have multiple of these cells and there are some input and then there are states, let's say that this output here depends not just on the input element, but maybe also on this input element.
So this could be an old sentence where the inputs could be birds and VN people to say whether the birds have fired at the sentence as I positive meaning or a negative meaning.
So it will depend on some of the birds and some of them might be far away from the place where we compute the output and if this happens.
If this network is to train successfully, then it must somehow realize this connection, so the loss or the gradient of the loss from this place must travel through the repeated application of the cell until it gets to the input element which actually is useful whose information we need to get through through the recurrent cell.
And if in every step we decrease the gradient by 0.5 or by some constant, then the longer the dependency is, the longer path, we will need to push the information through the recurrents.
The less chance is that we will be actually able to do it, but it's needed for the network to realize that this kind of element is actually useful in predicting the output.
So this is sometimes called either just vanishing gradient or specifically in these settings, it's called the challenge of long term dependencies.
How to make sure that we can discover the dependencies which are far away effectively with just an SGD.
So why this is a problem here? Well, this is a problem generally when we train a very deep neural network, then it might be difficult to passing the gradient through multiple layers like pens or flares.
So here you could say this is just a single layer, but in fact because we are processing the element sequentially the depth of our network is quite large.
So it might not seem when we look at it like that, but what we are actually doing is that we have this one cell which we are repeatedly calling on the input of its previous instance and then we feed the sequence elements there.
If our sequence contains 200 elements, then we have a network depth of 200 which is challenging at the bottom and another problematic part is that the cell is the same in these many applications.
So this has not happened so far even if we did convolutions like many layer resonant network, then the layers were generated independently each was initialized randomly.
So everybody or every layer did something different. So even if one of those layers, for example like the creased, some some time mentions of the gradient and the other layer could have left them intact or maybe enlarge them again.
But what wouldn't happen is that we will systematically decrease some of the information. So what I mean by this hand waving, well let's say for simplicity that what the recurrent cell does, we will explain it using just a linear transformation.
There is a percentage there, but for the sake of the argument, let's say that roughly what the cell does is that it flies some linear transformation.
This linear transformation is the same so after p steps we will just compose it with itself t times so the input will be passed through u to t.
Now that's the problem because it's the same operation over and over. So what does a linear transformation do? So if it is a well behaved transformation, if it has an eigenvalue decomposition for example, then what it does is that it takes the input.
It does some kind of a rotation of the input and then it does some kind of a squashing so for each axis, it either enlarges it or make it shorter.
And then it rotates back to the original position. So that's what all the linear transformation is do, right? They do some orthogonal transformation scale things and scale back.
So if we do it t times, what happens is that we do the rotation, we do the scaling, we get back, we do the rotation, we do the scaling, we get back.
So the rotation, the internal ones that they cancel out, so the composition like applying it t times means that we will do the rotation, then scale t times and then get back.
So if this transformation wants to get rid of some of the dimensions after the scaling, this process will accumulate more and more, we will apply this the same transformation.
But that's different from the previous settings where if we generate each layer independently, then each would transform different dimensions of the space.
So composing many of them wouldn't be that problematic, it still wasn't great, but this is even worse.
So we make sure that we don't explode by limiting the activation so that we will somehow make sure that the eigenvalues will be larger than one hopefully.
Well, not always, but what it's a good proxy, but we need to handle the other case where some of these dimensions are strictly, eigenvalues are strictly smaller than one, which means we'll be losing the information.
And well, in the decade, which passed after people realized this is a problem, there were really dozens of architectures of how the cell could be constructed.
And nowadays there are two designs which have survived long enough and are implemented in most of the frameworks and these two cells are called LSTM and GRM.
So it's some names which for the time being might not be interesting for you or known, but don't worry, they are on our list, so that's what we will start with.
So let's start with the LSTM. The LSTM is a short abbreviation for long short term memory, it's like weird, it's long short, but short term memory, like a term like when we remember things in our operation memory that we can use and then combine it with other stimuli.
And the idea is to make sure that it's short term memory is as good, as long as possible, right? But I also think that the authors like the name which is like provoking.
Actually, it was suggested like a millennia ago from the view of deep learning in 97.
And the idea of what the people had was actually very much similar to what ResNet people did, right? It's like we need to push the gradient through many applications of the same cell.
So what we would like is if the gradient of that cell would be one, right? Because then when you have gradient of the loss, then in the back propagation would happen is that you multiply it by the derivative of the cell itself and you will do it multiple times.
Ideally, the gradients could flow through the cell as unharmed as possible. I'm not saying that it can be done exactly like that, but that's what they wanted, right?
And they call it constant error flow in the sense that the gradient just goes through.
That is actually like a thing which fulfills this is ResNet your connection, right? ResNet your connection, which is copies the information flow through has gradient of one, but it doesn't do anything else, right?
So we will need to also do something, but this is really like 20, I don't know, 70 years ago. So the idea was that there would be some kind of what they call it, the constant error color cell, like there is this many go around and whenever the information comes into the cell, it gets on the very go around and then it will be going there.
No, not being changed so that when some back propagation happens, it would be able to go back. But of course, we will not really have anything like that there, but in practice what it means is that we will have some value which we will copy through through the cell mostly unchanged.
So people said, okay, let's do it like this. We will have two components of the state. One will be the memory cell so that the C on top.
And the goal of the memory cell is really just to remember. So we will mostly just copy it through without changing it much.
And so that will be the part where the gradient will be able to go through and it will be a way of explicit remembering things.
And then we let another component of the state and that's called H. And when I say state, I mean this H while memory cell is C. And that will actually be the output of the previous step.
The output will only be H, but it will be like visible part. And then the cell has this internal memory C which it will somehow end.
But of course, it's not enough just to copy the memory through. We need to find a way how to remember things so how to add some information to the memory.
And we will also need a way of generating some output using the memory cell which we have. So what we want to do is somehow remember things. So add the information to the memory and then also to use it when generating the L.
And so what did the authors propose is to use so called gates for it or gating as a process of how to do it. Right. So we are in a situation that we have got the memory cell which goes let's say from top to bottom.
And then we want to add or remember something. And the network will need to decide what we are going to remember. So what you will do is we will compute like a value like a gating value range from zero to one.
Which we will tell us whether we either not want to remember it at all zero or we want to remember everything one. Right. And then whatever value which we want to remember will go through this gay which at that point will decide okay we want to remember this or it will say well we already know it or it's not important or something. It will be like the the really like gating where the information it will come.
Relay in front of the gate and then the gate will decide whether it will go through or not. It will not be just zero or one to be smooth. Right. So half of the information can get through as well. And we will use this on the input to decide what to remember.
We will also use it on the output. So whenever we are producing a value we will decide what information is currently relevant from the memory because the memory represents like everything interesting from the beginning of the sequence but the current output might not necessarily require all the information.
If you imagine that we are producing a sentence then the memory should probably contain all the words relevant first or all the meaning but when we are producing an output if your task is to decide I don't know the part of speech of the of the word which we are processing then we do not necessarily produce all the information about the sequence only the information relevant for the current element.
So that's why we will use gating there as well. Right. So technically how will the gating work? Well we will just do a fully connected layer with a sigmoid.
Right. But we have actually seen this already in the squeeze and excitation network if you remember where we also generated some kind of weight from zero to one and then we decided which kind of channels will go through or not. So that's the similar principle.
So what we have on the input will always be X the current element and then H. So the output from the previous step or the state from the previous step.
So here on this image what what we can see like the images just a graphical representation of these long formulas.
So first we need to decide what we would like to remember we will do this in exactly the same way as in the simple RNA right. So we will take the X we will take the H from the previous step we will do a fully connected layer from their concatenation.
So this is like the candidate memory candidate values something we have just computed which we could add to the memory but we might not necessarily want to add everything because then the gradients on the memory might be weird.
And we also don't just want to store every information in every time step it's like maybe you just want to store relevant information or some of them right so the gating is what will make sure that.
What will give the network a control to decide what to store or not right so this gating will be just an element twice multiplication of our candidate value with these weights which goes from 0 to 1.
And then the spark will be added to the memory stuff so this is like the memorization bar right which here adds some new information to the.
So the gate this contact is called an input gate because it decides what we decide to input into our memory is computed very similarly to how the candidate values computed right we take the X we take H from the previous times that we concatenate them we do a fully connected layer but then.
We just use a sigmoid activation the the weights are different independent right so there are actually be six wait matrices in all the formulas which we can see.
So that got us to this state where we have remembered.
Now there is a natural question of whether this this gating and computing the value couldn't be done like in the same step right right now the network does two things it computes.
The candidate value which we might want to remember and then independently on that it decides whether we actually want to do it or not.
When theory the network would just generate output and generate zeroes whenever it doesn't want to remember anything so like this.
Could be done as well but at least empirically it doesn't work that well because in such a configuration is actually quite difficult for the network to just say I don't want to remember this because then it needs to generate zeroes which needs to be composed.
From multiple inputs and weights so it's just really difficult for the network to produce zero it seems nice or easy to say zero right but.
How does this zero gets computed well because it's one it would be just one of the neurons on the hidden layer it means that after processing call input multiplying by by by all the way then by all the weights you need to get to zero which would be.
So to make this this process easier we do do independent outputs right and the the gating which uses sigma activation there it is much easier for the network to say I don't want to remember anything it's enough if it just produces a negative quantity like negative.
Logitech right if it produces minus 10 minus 20 minus 3 all these values when they get through sigma they'll be very close to zero so we don't have to hit one specific value just enough to fit.
I never in the in the negative range here before the sigma activation.
So you can see from my explanation that it's about it's a bit wobbly right so this works doing it both at the same time doesn't seem to work even if there is no hard reason which would say it's not possible in theory.
The may be the way it's could be said that even just a single layer could generate zero whenever we don't want to remember it but it's difficult at least in practice to to train such a model using things like as gd or Adam on the contrary when we hit two components that it's much simpler because just a simpler.
They're climbing is able to trivial say I don't want to produce anything or I want to produce everything.
We will actually see this gating idea many times in the in the future and generally at least empirically it is it is a good idea yes.
So exactly as you say the city in its process will get larger and larger probably well it could get smaller if we decide to produce like to learn a negative quantity.
But right now it is larger and larger we will deal with it quickly but that's actually one of the problems of this architecture is that if the sequences are larger and larger then the the sea might get built a bit too much.
So I will comment it on the next slide just after I will deal with the output but good observation.
So how to generate the output by taking the current memory cell.
The sea have just noted it might be very large so we will pass it through it on each activation so that it gets bounded reasonably and then we will decide what part of this bounded memory cell is actually relevant in the current times.
So the rest of the network might decide for us but here this kind of decision is built directly into the cell which might decide what part of the memory cell will actually be the the output in the current times them and we will do that by the output gate.
So again we will do another gate we will take x we will take h we will do a fully connected layer we will pass it through sigma we will get a collection of plates, zeros and once these are all vectors like so the gates works per dimension.
And we multiply the financial of the memory cell by the output gate and then the result will be the output of our esteem cell.
So this kind of works but it can get better and the main observation is exactly that the memory will get larger and larger like we need to add information to it.
We will not add anything to it you will just keep it the same and then we will not remember anything at all which wouldn't be interesting so we need to add things to it but.
What people did to improve the situation is to allow the network an ability to forget things right so next time you forget something you can you can say I am like the state of the art you know network I have the ability to forget which had to be added to these components so that they actually work.
So how this will work is we will add another gate which will be called a forgetting which will decide what kind of memory we will actually keep.
To the next time so the idea is let's say that I want to remember the last verb in a sentence or in the text which I am processing right so then it's it's easy to remember next verb like whenever you look on x and you decide that it's a verb.
You will just say okay the input gate will just pass through so it can get to the memory but then.
At this time you probably also want to forget the old last verb which you have got in the state so with the forget gate you will say I see a new verb so let's just drop whatever we had in the memory cell and then the new one will be will be edit momentarily after this forgetting has had.
So nowadays when we say LSTM we mean this cell.
Formally it was described well it's actually three years after the original cell so we had to stay in people three years to realize it's forgetting in case it's actually good.
And and this cell works also in the situations where the sequences are really unbounded if you imagine that you want to process elements without a break.
So you will need some way of actually forgetting all its information which are too far in the past for them to be relevant for example.
So this problem looks very weird if you are seeing it for the first time I was feeling the same way.
What is magical in some sense is that people describe this 24 years ago and even if there have been dozens of proposals and papers and computers searching for the inside of the cell.
Nobody was actually able to significantly surpass this thing which people generated in the dark ages of the deep learning or machine learning maybe could say area right.
For example for the convolution that's definitely not the case right people did convolution networks for five years and then the efficient net game or last net or some other computer generated networks which are totally dominated the field.
Well they tried it for the cells and some of the cells exist but none of them seems to surpass this thing significantly for specific tasks you can do things better people tried to make it smaller but it's not like there is like a better implementation which would just be good or better in in many dimensions.
So that's that's really I don't know unbelievable in some sense either the people who designed this were very genius or they just were lucky and stumbled into it is I in which worked like it's like an idiom like it is still working ten years after it was designed even if there have been hundreds of papers proposing new optimizers but still nobody was able to to get better but maybe this is just a this is just like a practical evidence.
And the random search just works like even if you're doing things on random you have not even a chance of stumbling up on a good solution anyway right so if you try many of them some of them will be fine.
Well I will stop with my rambles of how this is how this is interesting and one more technical thing and that's with the frigate game.
Training might actually work badly at the beginning and the problem is that the frigate game is now working against our original motivation our original motivation was to.
To make sure that the gradient will go true but if this frigate gate would be initialized to I don't know 50% of forgetting stuff then that would definitely hamper the gradient going backwards because whenever the gradient would go through the freshly initialized.
The frigate gate it would be half and that's exactly what's happening right because if you initialize this weight as usual right then these weights are initialized in a symmetrical way so on average.
But this this output will be zero.
Maybe some other more assumptions are needed but I don't think they are needed because if they are independent and zero centred then whatever excess are the the excess have a good chance of either being multiplied by positive or negative value with the same range so I believe the expectation will actually be zero for that.
So it depends on the bias usually the bias is also initialized to zero so at the beginning of the training this frigate gate is an expectation sigmoid of zero which is 0.5.
It can not make sense like what a freshly initialized network should do when it should generate values from zero to one well it would generate an average.
But that's bad so by introducing the frigate gate as is the training might be first but we can we can help it what we could do is we could say okay so maybe let's initialize the better so that at the beginning we are not forgetting.
Right the beginning we will remember and we will only forget when needed and we could do it by initializing this bias to something positive so let's say it will be one.
So if we will initialize the frigate gate bias one then at the beginning on expectation the frigate gate will be a sigmoid of one well it's not perfectly great but it's something like 0.73 so we could even say two or three or something larger but initializing this bias correctly helps us to specify what happens at the beginning of the training which might be important.
So this was actually described already in the original paper by girls but then people kind of forgot that and in many implementations this was not done.
And then when the GRU cell was proposed which we will go to what momentarily it was better on some of the on some of the data and people were like why is it better and then they realize that the most problematic part is the better initialization of the frigate gate so there was a paper in 2015 which did a lot of experiments and showed that.
So we definitely want to do that right so they showed that by initializing the frigate gate bias to something positive the training abilities of LSTMs get better in a sense that they can capture the long term dependencies better because the gradient can flow better at the beginning.
So when you say it like this the question is whether one is enough right it seems to me that using like three could be better because then the sigmoid of three is 0.95 which is like close to one.
So I haven't seen an experiment on that in the paper they actually say we said to something positive like one or two or three and then they do the experiment with just one and then just and then the conclusion then say it's a good idea to say it was something was it a for example one.
So well my guess is that some larger quantities could be better but anyway now maybe let's say I had a like some some more more or I'll give you some more on the thought before we will get to the GR you think in LSTM.
What you might wonder is why are we actually doing this gate like what we did resonate we didn't do it like in a resonate the residual connection it was just an identity right we just went over the over the computation.
And what we are doing now is that instead of just copying things we explicitly we have this this memory cell we explicitly forget things add something for some forget learn forget learn forget learn forget forget or forget memorize maybe forget memorize.
Why are these two things non symmetric so I don't have a definite answer but in these cases the situation is a bit different in resonate this dimension where the residual connection leads.
Right this is not the different parts of the input this is just a different feature spaces different set of features which we use to represent the image right so in the.
In the.
At the beginning we have a simple feature so the dimension means I don't know circles and lines and then they get more and more of strike then at some point we have got like heads and arms and then we have got like full people and animals or something.
So what these residual connections do is they they go over the different feature spaces and in this case we don't want to like.
Selectively copy or forget some of these because we want the features space to be the same for all the pixels we want the meaning of the dimensions.
To be the same in every portion of the image of course the values are different but when you look at one pixel then the first dimension should be I don't know whether it is fluffy or not.
But it would be the case for all the pixels right it's not like the second pixel on the first position should have whether you require a car battery to run it.
So that's why it makes sense in my sense in my view to just copy the information because we want to keep the the the meanings of the features the same even if the values are not of course.
So with the with the other lens we are in a different situation because what we process are different sequence elements so here it makes sense.
To like a depth or change the features for every step because it means we will really remember different values for the different elements of when you have like a sentence then some some some birds they are like.
You just fill it or thought at the end or something you're able to do much with the we don't remember them or something but some birds are very important like it's a verb or subject or something.
And then they will influence the processing more so for them you want to select people copy them to memory right so in my head the difference is that the x says over which these residual connections go.
are the individual parts of the input so it makes sense to be more selected because different parts of the input might influence the output different but all this is my like like take on it and you don't have to take it or you shouldn't even take it for granted because well.
Well it is definitely not true not all of it but but it's the way how I explain why this is not really symmetrical by the way when we.
or when we describe the experiments in the in the resin they actually try this setting so here what we have is the usual resin that plug the original one with a relative after the addition.
one of these experiments this third one see that's exactly the gating experiment right so what they did is they.
had added fully connected layer well in images it means one time one's convolution right then they applied sigmo to the output.
and then what they did is they used it for every channel as the like a way which will decide whether the information will flow from residual connection or from the the block which computed the updated that's right so we have taken the way multiplied.
this output and then we have taken the one minus that so the remaining part one and we multiplied the residual connection so you can think about as the weight of saying if the weight is 20% and 20% will go from the newly computed values and 80% will go from from the original residual.
so that's very similar and I think this is really inspired by gating because this is like 20 years after Palestinians right well maybe just 17.
and when you do it really like that with the usual initialization and the training fails.
but if you start playing with the bias there so if you start changing what is the initial value of this of this way then.
and as they formulated that they physically decrease the bias which increases the the weight which you apply on the residual connection and when you get to minus six.
the training converges but with versus results and without that but at least trains because at the beginning it will be like yeah so the default is very strongly to go through this way and then the network can learn to actually also replace the values by something newly computed.
but even with the optimum bias the results are actually worse so the this kind of mechanism doesn't seem to help in the situation and my idea why is the one which I which I explained there.
yes a question yes that's a very good observation squeeze an excitation block is exactly this this mechanism so here what we are doing is that we are computing weight for every channel from zero to one and then the use that as a gate to this side which channels will go through and it's.
just that the gate is computed how slightly especially to avoid the large memory and time requirements but yes it is.
so.
Apart from the my images which I have generated for you there is also a famous block which did a nice little stations of the LSDM cells and other cells so while also show you so you can see again how it works so first this is the.
simple rn kind of thing right so we have got a sequence on the input.
and then we generate the output we have got the states every cell is the same cell actually the same set of weights and in the simple case we just concatenate things do.
so let's go through the components right so this is the memory cell which is goes through and we just forget stuff and.
so the the part where we will memorize something you is being computed through to components first we have this candidate value.
right so we can activate it for a connected layer here you can see that what they do is they concatenate so what they mean by this is that they concatenate ht minus one and xt multiplied by this one matrix which is a concatenation of the two matrices which I have.
my formulation ad bias applied on edge so that's the part what to remember and then we have the input gate which decides whether we are going to remember it or not.
then we also decide what to forget so again pull a connected layer sigma and activate it and then we will do it right so we forget things right they use this weird operator and we denote it as a dot in the circle like element wise multiplication so that's the forgetting part and this is the memorizing part.
and we also need the output right so when computing the output we pass the memory cell through the tonnage and then multiplied by the output gate which is again just a fully connected layer of x and ht minus one with symbolic activation.
yes.
so the question is that when we are forgetting things it also depends very much on the capacity of our short term memory so definitely I think that when you increase the dimensionality it would not necessarily have to forget that much but it depends definitely on the task.
for many tasks forgetting might never be useful actually so just remembering everything should be a good idea even independently on the capacity you just might want to remember just all we have seen.
for example the task might be to I don't know you could get a sequence of numbers and then at any point you should say the closest number to the to the new element which we have seen so then whatever capacity you have you should just remember everything what you have seen but.
there are also tasks where it's sometimes obvious that you will not need any like any new information so either you could have a sequence is for every day and then there could be could be some kind of separator so whenever you see it was I just want to forget so yes and no.
that on some tasks the forgetting might you might not want to explicitly forget too much if you have larger capacity but sometimes it might be independent so you might not want to forget at all or sometimes it might be useful to forget even if your capacity is very large but hopefully if the network has the capacity to learn to forget it could decide for itself.
what will maximize the performance on the data so with the forget gate it has the possibility without it it just just couldn't but.
we need to initialize correctly so that by default it doesn't forget at the beginning because it's much easier to start forgetting instead of deciding that you will learn to remember because at the beginning there is no evidence which.
you in the direction that that remembering is good because if you don't remember anything and then you decide to to remember one element for one time stamp you will just not see any gains from it so you will not do it.
well at the beginning if you remember everything you might quickly discover the remembering is good and then you will not want to give it up from that moment on.
so.
GRM I said that there are some new great improvement over LSDM which in some sense is true but the GRL cell which was proposed in 2014.
it's kind of so it's like and the cheaper version of us here so the idea behind GRL because behind the design was to make a cell which would be simpler and also faster.
and that kind of work so the overall GRU has very similar performance to LSDM mostly but but it's cheaper so how how does it work.
so there are two main points which people follow when they design GRL first is that in the LSDM we have got this memory cell and this previous state which played a different role and we have like hard core divided them so the network has to use half of state for the memory cell and then half of the state from from the output from the previous time step so in the GRU.
if you say okay so maybe the network can can decide this for itself right so we will just have a single vector which will be the state and in every step the network will decide which portion which part of the state will be just copied through and which one of it will be which part will be updated.
so compared to SDM where this this division is explicit in GRU it's like a do whatever you want we will just give you the ability to easily copy the information through and in every time stamp you can decide how you will like do it how you will manage your state.
so that's one thing so from the previous time stamp we will only get h and this also means that the outputs and the internal states are now the same so in the LSDM they were different but here we only have one quantity which is both the output and the state.
and then the second part the part where we will actually save something is that the people said well in LSDM we can independently decide what to forget and what to learn but maybe it would have sense to to couple these things together whenever we are learning something new we should forget whatever was there in the dimensions which we want to learn.
right so what the GRU does is that instead of this forget gate and input gate maybe it only has one gate and the other is just one minus minus the first one right so whenever we decide to learn something we will need to forget what was there so that we have a place where we can put the new information.
so in the GRU cell we will have two gates instead of one right and one of them will act like the couple input forget gate and then there be another one.
so what I will do now is I will give you like a very brief overview of how it worked with before we will go through the formula.
so we have got the state ht minus 1 which is the state of the previous step and it never times that we will decide which part of it we are going to to replace where we are going to remember things but I am saying replace because whatever we will overwrite things which was there before so we can imagine that we have decide this part will be recomputed.
for that there we will use an update gate so the update gate will tell us what part of the state to update and the part which is not updated it just copied through so we can think about this as this distinction between what we copied through what is the memory for this time stem and then what is the part where we will learn.
and then when we copy the candidate values when we compute the candidate value when we try to come up with a number which we will put into this update region what we will do is we will take x and then we will take the state but during this computation maybe only part of the state is relevant to what we want to learn.
Similarly in LSTMs when we use these states they were the outputs of the previous step so they were not necessarily the whole memory cell they were just a portion which the network deemed to be relevant in the previous step so we can do it by deciding what part of the state we will actually take.
Once we consider when we are computing the new value like the LSTM did it in the previous time step but here we will just do it now so we want to take the relevant part of the state combine it with the input element and produce the value which we want to remember.
So this is called a set gate which I don't know the name is not very nice but now let's let's have a look at the formulas right so when we compute the candidate value either the new updated ones then it's a fully connected player with a x and previous state and then here this is the place where we keep just some part.
of the previous state part which is relevant in computing the the new value the candidate value to remember.
Afterwards we just get concatenated past to tonnage and this is our new candidate.
Once we have the candidate then here what we do is we either or we copy the information from the previous step and from the new candidate value and gated by the update gate so the update gate takes some volume from the previous state and the rest of the volume one minus will go from the candidate.
The gates are being computed exactly as before so a fully connected layer and a sigma activation.
So there is an image which I tried to try to do it's not as nice as before but here is the part where we will do the candidate value right so we have the reset gate which decides which part of the state to take.
And then we do the tonnage and then here is the update gate so this is the part which decides what to copy and what to update and then we have this previous state.
We have this candidate once right and before we will ever we wait the according to what the update gate will do that.
We will look at the color of our image from the guy from the blog but if the same thing is just written differently.
Yeah I like my method.
So now I have made it difficult I have given you two architectures if I have just given you one then it would be obvious which one to use right you use that one but now you have two which means you will have to decide another hyper parameter for your experiments.
So how do these two cells compare? Well the GRU is simpler because it only has two input gate so there are just six matrices instead of A so in some sense it is 25% faster and smaller than LSD.
That's it's advantage although the GRUs are sometimes easier to work with because the states and the outputs are the same.
That might not seem very interesting or relevant at this time being but the fact that the state of an LSDM is actually two vectors might be technical at some point.
If you want to pass the initial states from somewhere you need to pass not just a single tensor but two of the if you want to use it in some in some attention or something it's it's a bit weird that there is two of them so sometimes when you are.
I would say lazy I would say time bound to implement the technically great solution the GRUs might just be simpler because well yeah put in the state are the same.
This is actually my motivation in various places where I have just used GRUs because I did the LSDM and then I realized what everything I would have to change and then I was too lazy and then I went with the GRU for a bite itself.
So this is probably not the correct motivation but well it's place role in my decision making when I even decide what to use.
So you will see the practical examples of that later to worry on the and mostly the LSDM if you initialize the target gate correctly and the GRUs have very similar performance it's it's difficult to say which one is better.
Like there have been many papers doing evaluation and it's like it's usually very similar and on some data LSDM is better on some other data GRUs better.
Apart from the tasks where the GRU is considerably worse than LSDM so there are tasks where this is the case.
And this has been described both theoretically so if you think about the languages which the GRUs or LSDM can generate and in a sense of languages and grammars kind of kind of view like if you've been collectors.
You can think about what kind of sequences these these cells can accept or recognize.
And there are languages which are difficult to recognize with GRUs while it can be done much better with LSDM.
And the reason for that is exactly the coupling of the forget gate and the input gate the GRU whenever it's memorizing something new it has to forget.
So if you have sequences like 8 to N, 8 to N, 8 to N or even just 8 like some number of A's and the same number of B's right what you need to do is you need to remember all the A's without forgetting anything.
So the beginning you just want to learn without forgetting and then maybe you want to like subtract what you'll start the B's or something but these kinds of sequences are difficult for GRUs to learn because it just cannot do the incremental counter internally because that's difficult.
So it tries somehow it trains or short sequences but it fails much sooner than with LSDM which can more effectively just do plus one plus one plus one without forgetting things.
So there's some practical tasks as well so there's a paper for example by Timdoza on syntactic parsing in a sense that you get a sentence and you should say this is subject, this is subject, this is something.
And then the representations generated are actually much more successful if they are generated by LSDM's compare GRU.
Now I've given you many points without giving you like clear instructions right so the the simpler instructions are if you don't care much about anything just use LSDM's it will be larger it might be more technical in some cases but performance wise this is roughly what you can get.
If on the other hand you are performance bound use GRU in most cases it will be worse it might be even slightly better but make sure to to benchmark or to verify and if you are also working with the internal states and you hate the case where the LSDM's.
But have had two of them then just ignore LSDM and just use GRU right but these are all based on all these properties of these cells.
So now you know.
There is inside the LSDM's in GRU's but of course we are using them in the network you don't implement the internal you would just say I won't an LSDM layer and then you will get it.
So before we start talking about how how to regularize and use these recurrent neural networks we will do some few like brim marks on the side so the first one is that we already know that we need to be careful when initialize the biases but.
When initializing the cells it is also good to be careful whenever we are initializing the the weights which will be used for the state transformation right because we have got.
These various weights which gets a t minus one on the input and then produce something which will be roughly a charge will be used to construct a new state.
And that will be used like over and over in all these repeated applications like so which we have seen at the beginning and get the motivation.
So for these matrices and which one do I mean so in the in the GRU case that would be all the matrices which process age right so this one this one and this one because it gets.
Future space of age and the input and then it.
Produce something which will be transforming to another age so in some sense it will be used iteratively especially especially this one.
So what we will do is we will initialize in a way that it does not enlarge nor decrease any of the dimensions.
We will initialize the S or for goner matrices right and then the norms of weights and the columns are one which.
Should help us or could help us with the vanishing and exploding radians because well it will keep volume exactly right in the normal initialization we care about the scale so that the input and the output of several variants.
We are because we know that this kind of an transformation will be applied many times in in the unroll the less the ms or or g argues then initialize initializing them to to completely preserve the volume might be might be useful so that's.
What happens when you ask for a stem. This happens automatically there is an argument called called recurrent initializer so that the initializer for the matrices which process.
The state from the previous time stand and you will automatically get these forget.
Gayed bias of LST to be initialized to one.
So another remark which I wanted to make is that.
We have seen the recurrent connections in resonant but actually people were having these thoughts as you can see even much sooner because in the LSTM state they consider it even in the previous millennium.
So something like half a year before the resonant paper came out people also wondered how we could make deep network but not with convolution but with fully connected layers which could train effectively like even in the resonant and motivation was we are under fitting if we have got 50 conclusions.
This is also the case for fully connected layers right so what people did is they say okay let's let's think how to do that and let's use the gating mechanism we know that it works well for the LSTM and g argues so what we could do is we could say okay let's add when we have a fully connected layer we could have.
connection around it but but it could be gating right so what we could do is we could take the value compute the gating value right and then decide to either copy the new the newly computed value or copying the original value right and then something.
So this terrible image is just shows that we could be gating just simple output of a single single fully connected layer right so so with some math we could have this fully connected layer right and when we produce the output it will be either the newly computed value or it's input gated by whatever gate we can come up with.
And the what we would do to compute the gate will well it will just be fully connected layer of the input with a single activation right.
So this is very similar to a GRU cell with the only difference that there are no states in in the metal and when we are copying things we are copying x instead of h.
So you can think about this is like a concurrent research to the residual connections even if here we just do the explicit gating.
And also they are careful so they say they say well for this to work the the bias in the transform gate should be initial I saw that by default we are just copying the information through so we are not applying the transformation we just copy.
But through and then we will always start applying using the layers when it seems like a good idea.
What they do is they train at works for on some simple sad tasks so like an emnist.
And so here what you can see is training loss on emnist with 10, 20, 50 and 100 layers so you can see that the loss is really increasing.
So we are really underfitting when the number of layers increases which is what people mean anyway but just the here they make sure it's quantified.
And then if you use this this approach of what they describe is an highway network or there is this this this highway where the information can just go through while avoiding these.
So it looks slow and difficult parts of the connected layers like that's the highway network.
So with the highway networks the underfitting is not so severe so for the 10 layers 20, 50 and 100 the training losses are definitely definitely much better.
So they show that this might not be a better idea and they show that even for small number of layers like for something this could still be beneficial.
So there were a few follow up papers which used this kind of architecture when whenever they wanted to use them non trivial computation for which a single layer wouldn't wouldn't be enough.
And they also had this idea what we have used in ResNet and that's if we are doing this we are we are having this this highway connections or residual connections.
So that's the last name later there then actually the teacher space of what the networks compute is very similar so what they did is they tried removing the layers after training just to see whether it will still work.
While with the usual combination of all the connected layers when you remove one of them and just copy the information to where everything is lost.
Here they have shown that it's not so bad in the sense that if we look at the.
Lost or cross center of the error this is where you can get if you don't throw away any layer and these are the performances of what you would get if you would remove that one layer index index.
So removing the things at the beginning is the most harmful because well these layers probably do a lot and and they are most being used and the highway connections not so much.
Then at the end it is slightly harmful and then in the middle it's not harmful at all well probably because the M this M is this is this side not to use the the layers in the middle because well 50 layers for M this that definitely an overkill right.
So they do some illustrations which show that like in in the middle we mostly really just copy all the information through so one one could explain these things are but generally this is probably good enough.
On the larger it assets like see far 100 the model is actually using all the layers so removing any one of them is actually actually harmful.
But why am I showing this is not necessarily because you want to use highway layers in all your architectures just to connect this.
These procedural networks and the recurrent cells and the fact that the people were thinking about these spaces and whether you could remove them and how they change when you add these shortcuts to the network.
For the the resident people came around so after the course you might be like and the resident people they were so genius they just came over all these ideas well they implemented them successfully on a large scale.
But the main idea or like the approach to that has has been similar for for many years even even before that it's just that they were lucky to do it correctly and train very famous model.
So maybe now would be time to have a break so let's let's take a break and then we will continue by regularizing the recurrent unit looks.
Let's start by talking how to regularize recurrent unit works we have discussed that for the conclusions and it was on trivial we had the batch normalization there and then many kinds of dropouts and.
So how does it work with the recurrent neural networks so with the recurrent neural networks the dropout is being used law and using dropout on the inputs for the cell.
And on the outputs to the cell that's fine it's been used long however there is a complication or there is a place for the dropout doesn't work very well and that's the hidden state which is the past between the individual cells.
The problem is that if you imagine how does the state look like right so if in every time step you use dropout then but in all in the first step you might forget these three ones then maybe this one so at the beginning.
Right you would forget that some parts of the state then maybe you would we would forget some other parts of the state so after a number of steps.
Easily all from from the state could it be forgotten right because we generate them are the masks independently with the dropout so single one dimension the chance that it will get past k dropouts is just one over to the k right so after 10 layers of the dropout the chance that any video or the dimension will get unharmed.
And this goes against our goal for for capturing long term dependencies right if you want to realize that we need to consider elements which are 10 steps or dozens steps apart.
And then the dropout will well kind of forget it right because we had this short memory which we have constructed so painstakingly and now the dropout is forgetting things without our control or something it just throws away the information right so.
Applying about there will make the model under fit a lot and the problematic part is that we have got this one same feature space and we are doing a lot of independent dropouts on the.
So we will well be forgetting things right so the dropout us if you have like a lot of independent layers.
And then you do dropout between them it's not so bad in a sense that after every layer the information will be.
Criser of during in some other places so.
It's not so it's not so detrimental but here when we have exactly like they mentioned which we actually want to keep.
To pass them through the sequence then well dropout will just just interfere with that so the question is what to what to do.
So there like this is like the great case for the research is like how we have a problem we have a dropout it doesn't work on the cell so let's do something about it right so there are many papers describing how great.
The solution that the the the authors proposed and show done various assignments or tasks that it actually works.
So I will describe the most successful one but the bottom line is that mostly it's not worth.
Applying any of them and using dropout on on the hidden stay between the cells and the reason for that is mostly speed.
In a sense that we will have implementations of these recurring networks which will be specifically implemented for the GPU card which are fast but then.
If you want to add whatever regularization on these hidden states that means you have not to use them but some like explicitly unrolling implementations which are easily five.
Or in some case 10 times slower so it's usually not worth it.
Usually you can control the regularization in the other parts of the network well enough so that you don't have to bother.
Well anyway if you think that you are over fitting a lot you benefit from regularizing also the the memory of the of the R&M then what works quite well is the so called variational dropout there unfortunately many variational dropout it's a very overloaded name but the one which I mean which is from the paper mentioned here the idea is that.
Didn't like dropping different dimensions independently so maybe we could just generate a mask for dropout and then keep the same mask for dropping all the state in the sequence we could have a mask which not would be time dependent.
If you generate the ones and then the same mask of zeros and ones would be used for one whole sequence so this way.
Here when we when we process the states we would always.
Forgot some of these dimensions maybe the first but it would be the first two in every time step so these two would not contain any information but the other dimensions would so there would still be way for the network to remember things right on the dimensions which are currently not dropped.
This could in theory be also used on input elements and the output elements or that's why these colors here in the arrows are the same they they indicate the same mask is is being used.
By the way when you use dropout in in the keras then you can control the size of the masks which are being generated quite quite nicely because the dropout as the noise shape are human which tells you what mask should be generated so imagine that on the input we have with images so we are going to like batch size high with channels if you just say dropout.
Then by default it will generate the mask of this size so every value will be dropped independently but with a noise shape what you could say is you can say batch size one one.
For example right then when you say noise shape like this then what happens is that you will just generate a single value for every height and width so you will have the mask which will either keep all the use of a single channel.
When the multiplication is performed like the mask times this then the broadcasting will make sure that these ones will be expanded to all the positions but this way.
I want to drop whole channels that will happen and also if you implement the network with stochastic debt you can use a noise shape of one one one which means well I would just generate one value and either I will keep everything or nothing.
And with a batch size you would still do it independently for the batch size is but if you said one one one and then you state whenever this block and then you could have this dropout here.
Plus then either this block would be copied all of these values for all the examples on the batch or none of them.
So that's how you can control these kind of things and also this inspirational dropout on this on the recurrent.
For the recurrent cells and networks can be specified exactly by saying recurrent dropout is for example 0.5 and that means that the time independent mask will be generated and then apply on the states on whatever cells you have.
So that's the reasonably well performing dropout for also for the states but the disadvantages that it's considerably slower for some technical reasons and that's that the super fast implementations just do contain support for it.
So there have been some other ideas how the dropout could be used. So for example when you are copying a new value state in both in GRUs and LSTMs you compute this this candidate value right once it was denoted as HR with with a tilde and once see what a tilde what you could do is you could drop out the candidate value which you will remember.
Right you will keep the memory cell in tech but you could apply dropout just before you remember things so that you just remember some dimensions of the information that that kind of works as well.
And it's very specific to just LSTMs and GRUs so apart from the initial paper it's not very much.
Another idea what people had is that if I am having an RNN cell right and then if I will decide drop one of the dimensions here.
Then instead of really dropping it and setting to zero what we could say is we could use the the value which the state has before the previous times.
So it's like if you are listening to me about my voice and I was very tiring so you just like put your head on the desk and start sleeping right and then you wake up right so we have lost some kind of information but it's not like you wake up and we have like.
Zero knowledge in your head like you still have the things that you had before you and to sleep right so this was proposed as zone out which.
I actually like the idea but unfortunately there is also not very very great apart from the initial paper of course in every paper every proposed architecture works but but in reality it wasn't that great.
So apart from drop out for convolutions which normalization was crank right we did it it normalizes things and maybe here we could use it as well right because we have got this state which needs to be passed through.
and times times we even discuss that it could get larger or smaller so using which normalization might maybe a good idea so that seems nicely.
But at the beginning there were actually some negative results in a sense that people tried to use it for training.
But in the end actually people realized that it's not like it's not working at all it's just very difficult to set it up correctly.
And the reason for that is just the initialization for the best normalization because in the best normalization if you remember what it does by default that it generates distribution with a mean of zero and.
But this variance of one that means that the values are quite large so if you then pass them through a tonnage right what will happen is that well.
The tonnage looks like this right so the derivative of the tonnage looks like this.
So if you input very large numbers into tonnage then the derivative of the tonnage is kind of small it's nice in the middle when the values are small but for the larger values.
So if the batch normalization is there it will actually scale things up considerably because the outputs of the tonnage are bounded from minus one to one so they will have.
A smaller variance than just one so the batch norm initially will scale things up and by scaling them up the the gradient of the tonnage will go down so here there is a graph which shows.
If the input has the standard deviation of zero zero point one until one then this is the size of the gradient of the tonnage so at the beginning it's one and then in good small and smaller the farther you are on a bridge from zero.
But that means in the back propagation pass every tonnage decreased the the gradient was zero point six so the size of the gradient dropped.
Considerably right this is the one which goes down and down and down I don't have a green one right but you can image it going down and out so after many elements you just you just get nothing but we could improve it.
We could improve it by setting the initial variance or initial standard deviation to smaller number so what people will the data is they try to 0.9 a seven and so there are these less decreasing gradient magnitudes.
Until with zero point one the size of the gradient which went through was not changing and it started working.
So it's like one one character change like instead of one dot you need to initialize it by by one and this makes it work even if some other teams said that it won't.
But still this is still a difficult and tricky and difficult like problematic to work with so instead of which normalization people actually came up with another normalization layer which is called layer normalization which works definitely better in in this regards.
So in the batch normalization how does it work is that if you have got an image like an image as some number of channels and you actually multiple images in the batch then.
The specialization we take one of the channels and we normalize across all positions in the channel and we also normalize across all the images in the batch.
This works but it requires the batch size to be known trivial.
What we could do is we could do some some like a transpose version of it instead of normalizing across the batch size we could normalize across the channels.
So what does it mean is that if you have a fully connected layer then in batch normalization all of these neurons are independently normalized they don't consider each other in normalization is the other way around so we could normalize.
So the output of the layer like as this vector so that this vector as a whole has some prescribed mean and variance right so instead of normalizing across the batch you can think about it as a competition.
Inside inside the hidden layer where the largest activations will be the largest the small it will be the smallest but the statistics will be kept the same so whenever each of the mean and the variance will be as prescribed.
So this is called layer normalization we normalize the values in in a layer it's implemented very similar to the batch norm but things are.
Conceptually simple because we don't need to think about multiple batches we don't need to think about the different training and the inference regime because we would be tracking the the the mean and variance or something.
So we always do the same thing right whenever we get a vector of value so then an output of some layer.
We will just compute the mean across the values the variance across the values then we normalize so we subtract the mean divide by the square root of the variance.
And then we add this trainable statistics right so for each for each value in the in the layer we can actually say what is the mean and variance of this dimension approximately.
So with this kind of normalization.
The recurrent unit brings substantially better so if you imagine a cell where the.
layer norm is applied on the on the on the recurrent state then it trains quite good so this is a graph from a paper which introduced later normalization so surprise surprise it works the best in the paper.
So what is this this is some RNN architecture where we have got a text on the input it's like reading computation so then we have a query like a text can be.
The commenting player which gave goal in some match off of two teams and then the query could be and whose car the second goal or the the one whose car the second goal was and the goal is to find out which of these words on the input should be put there and what they do is they process the input with the recurrent.
recurrent network to get nice representations so not adding any normalization just using drop out but not on the on the state trains but it takes some time.
doing this very careful batch normalization although works but using Claire normalization that the blue and sign one doesn't require any tuning hope we are type of parameters we just use the default ones and it seems to train faster and get to the better results generally with RNN's layer norm might be used.
The disadvantage is speed though because this this super tuned implementations don't contain it so you have to consider or you have to think about that your implementation will be easily 5 or 10 times slower than the usual one so here it seems like this is this is long right but.
it would be 10 times slower means you would get to these points in these curves at the same wall speed so it's like.
Shady so my my personal my personal experience is that it doesn't help me in a sense that I was able to regularize the other parts of the network strongly another day didn't need or didn't benefit from the layer normalization on the states but the layer normalization will be useful in other architecture as well which is why I'm discussing it now even if it's not a clear when.
So now let's have a look of what the recurrent networks can offer us right so what we can do with recurrent unit for so first.
We can be in the situation where we get some input sequence and we would like to generate the output of the same size.
But the representation of every element should depend not just on the input element itself but also on all the elements which preceded.
So this way we get a representation which includes also the surroundings we can think about it slightly as a convolution layer because the convolution layer also would get some kind of sequence produce the output and every every output would depend on a fixed surroundings of the input depending on the kernel size right here.
We depend only on the previous elements but theoretically the length or the window is like unbounded because the information could go through the sequence of any length even effectively it will only be tens or small hundreds.
So that's one thing we could contextualize the representations of the simple what we could also do is we could use this but not taking all the outputs but taking just the last output of the last state and use this quantity as the representation of the whole sequence.
So we can use our elements to represent sequences of any length using a fixed size vector we will see in a few slides why that might be used.
The elements also offer us to generate sequences now that something what we could do right now we just did some fixed number of crossifications or regressions but imagine that you would like to produce sequence of like unbounded length.
So what we could do is we could say okay so let's get the representation of the sequence use it as the initial state and then we will generate the elements one by one.
Of course what it means is that during the generation we will do them and the thing is when we train it we just say okay we want to generate the i element depending or condition on on the previous ones.
And you want this because well you want to compute like the the whole sequence like x1 x2 x3 and so on and this can be written that we want to generate x1 then we want to generate x2 given x1 and then we want to generate x3 given x1 x2.
Right all these things so we want to generate an element condition on the ones which we have generated.
Now the thing is when we are generating this element we got this stay but we explicitly didn't see these elements but it might be important.
So in order to actually be also able to see what we have generated we will exactly get the input and that will be the element which we generated in the previous.
We get some idea of what we roughly generated from this state because the output was generated from it but if we are talking about words then from the state you can see it it probably meant it was something large but it was large gigantic enormous huge not small or something we might not know we know like general idea but not the exact decision.
So in many many cases it is actually useful so that's why we explicitly pass them the information there not just late in some some some late in states some continues idea but really the decision which the network did like for example in the language yes multiple ways are you can say something you can say I go I run I whatever.
And depending on what you have actually chosen like on the discrete choice you might want to decide what will happen like either you want to say I go or you want to say I am going and it might not be so this from the state but if you say I go then you need to continue with the sentence if you if you said I am then you need to pause a bit and say going because otherwise it would make no sense so.
It is like it is important to actually see what we actually generated in the previous step to to make things that in the case of some ambiguous sequences of multiple reasonable outputs we will choose a consistent one.
So we will discuss this in more detail in in lectures but that's definitely one of the important applications.
Paranets so yes the question.
Yes thank you very much I didn't really say so the question is when do we want to stop generating right so so what we do.
It's usually a specific symbol which means this is it finished we are at the end right so here it's called end of sequence and so we generate like during training we probably know how long the sequences are because they are in the training data.
The idea is that we are still doing it in a supervised way so we have to go data and what so what we will do is we will add this specific symbol at the end of the sequence which we have generated and then during prediction we just generate until we encounter this symbol which is stopped now we have generated everything and we are done.
So I'm not sure if I answered your question.
Yeah it just depends on the data so it can be audio it can be text it can be images it's like whatever you want to do it's it's like like this is just supervised right so during training you probably have the sequence which you want to generate.
So whatever is your in your training data the this model will try to predict.
Yeah or we can generate the image by generating sequential parts of the image that also works like like you could generate the image like I don't know some parts of the image row by row.
That's it's like possible to be like any image that you would just say okay I would just generate it as a sequence of nine like fetches which are there and then I will compose them into an image later or something.
So that's that can be done but like all the specific applications I will discuss when we get to the specific domains but conceptually it doesn't really matter it's just about what what sequences you have that you would like to generate.
Well at the beginning like whenever I'm going to use it right I will really just take whatever I predicted and then use it in the next step but at the beginning when I'm training.
Probably the net per dozen produce for the reasonable values at the beginning so at the beginning what we will do is we will give it or during training will give it the values which it should have predicted in the penalty on whether it did or did not predict those.
Because well it's probably not very good in it anyway but it's but but it should get there so we usually wanted to behave well if it gets correct input like if it would generate something very weird.
And we would then then use it and then we would try to to generate the correctly the second element it might be difficult because well even the first one was very bad so during training we usually use the growth sequence.
So on on the input hoping that after conversions the numerator will actually be able to predict these elements so then when we start actually using the predictions it will work well.
Well it's just different regime of the model right so this this top part is during training.
So there because we are not predicting good values probably initially we are using the goal once while when and for this of course we need to know what the help today so we need to have it in the goal data.
But during inference when we don't know the output should be the only thing which we can do is rely on ourselves.
So in the top part we know how the full audio should look like when we do some text to speech for example.
So we give it the the parts which should have been generated in the previous steps independent on whether they were generated or not.
And we hope that the network will get better so here the the predictions will get closer and closer to the real ones so that whenever we actually use it then is quantity which we will get here should ideally be the correct ones.
Yes yes yes well it's like a loss as usual so it's like we should have generated a zero we have some logits for it so we want to minimize the cross entropy between them so we use them as the goal target.
The goal targets here and we try to make sure that the network actually generates generates the correct output as described by the data and then hopefully we assume that it will be able in the end to do it so we put the same things also on the input.
Yeah I always want questions I'm happy if you ask.
So the in convolutions we are using multiple convolutional layers and that is useful also for the recurring neural networks so you can easily have multiple layers so this would be three layers right because it's like one layer whether cell is unrolled to process the whole input sequence the idea that this is the sequence.
And then there is the second layer here and third layer in the row it makes sense because the the second layer can get representations which are much much richer than the initial one because here the representation of the element depends also on the previous ones so adding can you layer might help.
But because now we can see more dependencies or more connections between the elements on the inputs or hopefully we will be able to do something more and well when we do it it makes sense.
Also add residual connections because the gradient can flow nicely in this direction that's how the cells are constructed but if you have multiple layers you also want the gradient to flow nicely in this direction.
To that end the same the standard residual connections are a good idea and usually they are not being used on the first layer right one thing is that this input dimensionality and this output dimensionality might not be the same.
The output dimensionality might be whatever we are processing and this is some quantity which we which we are like which we control right so in the convolutions we had this this trial layer and then transformation there to make sure that the dimensions are are the same but usually in these settings we just keep the first layer without the connection it also makes sense.
Basically because here you have some features like the dimensions mean something and after you will and the RNN you'll probably want the feature space to be different like originally there are informations about the individual element but now I need features which will consider whole sequence not just the individual element so it makes sense.
Not to force the network to keep the same feature space which is what the residue connection do so that's why it's not there on the other hand from the second layer of further it's parverts if we use the same dimensionality on the input and on the output it can easily be added there and then it can behave like a residual block which is compute the difference.
on whatever was computed by by the first layer so this is a fairly standard way so this helps with depth.
One thing which is however bad about RNN is that they consider the context only from one side.
I always set context twice with respect to the previous elements that makes sense for example with voice which is really forward kind of thing.
So it's fine for you just to know what I said before you don't need to know what I will say in a second so that you know what I am speaking about but when you are reading a sentence for example then seeing what will happen in the future might be useful.
If you are reading German then a lot of birth are at the end of the sentence actually so it might be a good idea to pick there and then go back.
So in order to handle this we can use to so go buy correctional RNNs and the bidirectional RNNs are well just two RNNs where one is the one which we have described already and the other is backward facing one so it will process the elements from the last one.
To the first one so that when you get a representation for example here it will be depending on the corresponding element and then also on the elements which follows it in the original sequence right or the output which we get are like the output from the forward RNN and the output from the backward RNN and we need to combine them somehow.
Usually this combination can be performed either by concatenating things so we have good like two sets of dimensions.
One is from the fake forward RNN and the others from the backward RNN or you might even want to sum some to map.
For example when you want to do the residual collections around it.
It is useful that the output has the same dimension on it is important and this addition what it does is that it gives the network to control what kind of features will be produced.
So for the forward and the backward one the network can decide how many features will come from the forward and the backward direction it will just manage it by itself.
So even if it seems weird to some things up it works well in practice because the neural network is just responsible for deciding which parts of the states will be most generated by the forward or the backward dimension.
Direction. Yes, I mean literally plus just add them together.
Okay so now before we will start working on assignments we will need to deal with one last thing and that will be how to represent first.
Right, we have done the uppercase assignment but then there we really just had letters and then that postate and we just mostly consider fixed sequences of them but let's say now that you would like to be able to represent first.
We could do is we could just use one hot encoding right so we could have a vocabulary the vocabulary could have I don't know 100,000 words for example and every word would be just very large vector with a lot of zeros and then one one.
Now this is nothing entirely bad and people have been using it for quite some time but there are some disadvantages one disadvantage is that this is large and therefore it can be can be inefficient.
And also from this point of view all the verse are completely independent like every verse is on dimension so when you learn something about the dog then and you also have a doggy in the or in the or data set then.
You you cannot transfer the knowledge in any way you just learn something about dimension 20 adult and then you have this dimension 30 where there is a doggy and they have their own set of weights in all the fully connected layers so they don't share any information even if the words might be very similar even if you learn that I don't know people like keeping keeping pets or they shouldn't keep pets into one specific part.
Then if you have learned that they you shouldn't keep the like like bring dogs to some specific part maybe you should also shouldn't keep snakes or or cats or some other kinds of animals to the dog but.
There is no way how to capture this with this representation so the only thing which you can do is just learn everything independently for every word which you have.
So instead what we will do is we will find some other way which could leverage the fact that some words are actually more similar to others.
Then then not so if you have got like a dog and care they are somehow similar you can say that they are different but they are closer together compared to a nuclear power plant or variation auto encoder or something like they are all animals are fluffy people like them so they they have some similarities and this is not really.
Not really captured by this one encoding so instead of doing one encoding you use the so-called distributed representation.
The idea of the distributed representation is we want to represent words by themselves like the identity is the word but we will represent concepts.
I feel we could have concepts like it's alive right it has for everybody likes likes to I don't know likes to watch it it needs nuclear fuel to run right you need to I don't know.
I've started up only when there is a blue outside so you could have these kind of like concepts or or factors that sometimes also call and then instead of saying well that the basic thing is that we have got like 20 and then let's learn for something forward 20 we will represent the word as the.
or as the collections of these concepts so then it will also dog is an animal it's flat a we like it it does not need nuclear fuel to run and then we will learn.
About these concepts instead of the words by themselves so then when we learn something about the dog we don't learn it for the dog per se but we learn it for for the concepts which we use to express this word so we learn it for for animals for fluffy objects not for things which needs gas to run and so on.
And this is called the distributed representation because we are using some kind of underlying factors or parts of meaning or something which.
which we can learn about and then the words will be expressed just in this kind of space or coordinates of these underlying factors so technically what does it mean well the words will be represented by vectors right so every word will get some number let's say d number of coordinates and the idea is that this d that's the number of the concepts or factors which we have.
And then we express by way of every every of these of these concepts like so we have the space where the coordinates are no longer words but they are these common vectors.
So the question is how we get such a representation right well we will just train it.
So at the beginning we will start with some random description of the words and then we will train it together with rest of the network on our supervised task so the initial representation will be like a matrix for each word we will.
have this d numbers it's concepts it's factors and whenever we have a word we will.
represented by a row from this matrix and then instead of the word or it's one other representation we will use that row that corresponding representation of the word itself.
And these representations of these words they are called beddings well just because that embedding is a map from some space into into some other space.
So it's just a way of saying the coordinates in this are to these space bedding.
We could actually implement it by the by the layers or or concepts which we already know we could have taken one other representation of the word itself and then do a fully connected layer without activation.
What would happen is that for the fully connected layer would be a matrix which would be of size w times d right and then when we multiply the vector by the matrix what we would get is well we would get just this one row corresponding to the position of one on the input.
So it would just give us the row index by the number of the word which is exactly how I describe how this embedding would work however this so called embedding layer is much faster than really doing one hot and then a fully connected layer because we don't need to construct a vector with a lot of zeroes and then multiply a gigantic matrix with this vector of a lot of zeroes and one one instead we can implement the embedding player by just indexing.
Right when we get an index 20 like we get to 20 we just return the 20th line of the embedding matrix is the same thing as one hot times matrix but much faster.
So there is a layer for that in the perous or in vitage and we need to specify the number of words input identity and the number of vectors in our representation the output edge.
So what happens is that you could say that if our embedding matrix is just one hot encoding plus a fully connected layer what's different to using this one hot representation everywhere.
So usually when we use the words or the inside the computation there is not just one place where we are using it but multiple ones for example in the RNNs there are eight matrices there in the input so the in the cell so every input will be multiplied by four different matrices right so if we use this one hot representation.
The each of these matrices would be extremely large and each would learn about the every every word independently so what we do in the embedding is we like take this one large part like it's embedding matrix which represents the individual words is being performed first and then all the flowing processing.
is then much smaller because instead of words it takes just the embedding on the input so we still have one place where things are expensive where we need to learn about the words but once we do it in all other places in all other uses of the word we can learn not about the word themselves but about the the factors.
So then when we learn something in this matrix about the dog we will effectively maybe learn it also for for paths and cats and people and other words people like and this matrix will tell us okay and these these dogs sorry these paths they are not just dog or the cats and snakes and whatever.
So this is all nice and peachy but there is one practical problem and the practical problem is what happens if your word doesn't appear in your training data well if you want to process a word which was not in the training data you don't know it's embedding you don't know it's underlying factors you don't know how similar it is to other words or how to represent it all we just have nothing you do.
nothing you don't know anything at all so we need or ideally we need to find a way how to represent words which we haven't seen during training and there are always perfect.
We have seen during training unless you have trained on all the internet and even then you can come up with new words like look with five ohs or or miscell words use use emojis lot of things to be done and to handle this case we will.
also use the fact that the words are usually composed of characters so we could represent the words not by it's like identity but by looking at it as a sequence of letters.
We have a sequence of generally arbitrary length and we would like a fixed length representation for that well the recurrent neural networks could help us hear as well right what we could use you could take the characters.
embed them but using like embedding for characters not for words and then pass them through recurrent neural network right and then take the output state so let's say that we have with the word cats right so we could embed.
that every character and then pass them this is a forward facing R and LSTM so here.
We get a fixed representation which contains the information about all the character which we have seen.
it's naturally more biased to the characters more at the end right like it can theoretically remember everything but things at the beginning we're seen in the path and then we have.
so probably this representation will be mostly biased to the last characters but that's not necessarily bad when you look at the at some unknown word that looking whether it's it ends at IO and like corporation or i and g and then it's a verb probably kind of makes sense.
anyway we will do it also in the opposite direction so we will also use the backward R and N and that will give us the representation of the words when we consider it as a sequence but we read it from the last character to the first one right and then we take these two directions combine them and we will get a fixed size representation.
of the word as itself like a dog is a verb 25 but as a sequence of characters how much similar is it to sequence of other characters and so on.
this probably will not handle well words which are very very weird or irregular or something on the other hand that's difficult if we haven't seen when you're in training but for many words if they are irregular they are are also frequent.
what language tends to do like the words which are irregular has to be prevent enough so that people actually know how they work if you are seeing a word which is very unknown is probably a name of something.
and then just guessing the structure by looking the first character is capital and then there is some subject of suffix might be enough right so we.
usually what we do is we combine the verb embeddings for the words which we have trained and with these character level embeddings for the words which we have which we might not have seen during training but at least some kind of similarities.
well we'll be useful so then I don't have it here so I have it in some other.
so then when we represent the words you put say the verb embedding then you compute the character level verb embedding combine them together and then continue.
okay thank you very much we will continue with this topic with the recurrent unit for rocks and.
Berger presentations on the practicals I wish you would like in in solving the competition assignment which is very difficult this time.
and I'm looking forward to meeting tomorrow.
