Hello everybody, it is my pleasure to welcome you to this online recording of the 13
lecture of our deep learning course.
This time taking this form of the critic audit, a recorded presentation, because there
is a sports day today, so I hope you have been enjoying, your possibility to do some
sports and regarding the lecture, we will continue with the generative modeling, so we
started discussing the image generative models by talking about the variation of auto encoders,
and we will discuss two more families of models, which can do image generation on this lecture.
In the generative models, we need to describe this probability distribution px where the
x is the domain of the data, so for example, images. So the so-called likely, who is likely
with based models, represent this density function, probability density function directly. So usually,
they generate some kind of logic, and then we can compute the distribution out of them by
doing the softmax kind of thing, so we do the expansion, and then we normalize. So this
denominator, which is also called a partition function, is there just to make sure that the
resulting object will be a distribution, so it is just integral or some of all these densities,
and while in softmax, let's straightforward, we just go over all the classes, like in softmax,
you would go over all the sets, and we would look at what the model gives us for Z and some
overall of them. Doing this for continuous domains, like for all the images, might be very difficult
or even intractable. But the generative models somehow need to deal with this, so there are
various ways how this can be simplified. So first we could restrict the model architecture,
so that we wouldn't need to compute this normalization constant. So for example, we can do
sequence modeling and compute the probability, so the image by computing the probability,
so the individual pixels and that helps, or we could use some invertible models like if we have
the encoder and the decoder, like in the variational encoder kind of formalism, so if
they are complete inverses of each other, then this restriction of the architecture also helps us
to compute the exact normalization factor here. But that generally this limits the architecture,
or we might just approximate this constant, so that's what the variational auto encoders do.
They cannot give you an exact estimate of the image, but you can do
some estimation of it, if you use the encoder, you will get some part of the lighting variable
set, which are useful to generate the image, so by looking at the probability density of these
lighting variables, you can get some estimate of what the density of the resulting image would be.
Or we can use models which will not be able to compute the lighting explicitly,
but there will be so-called implicit generative models, so they will not store the lighting,
but they will allow efficient sampling. So that's what the generative adversarial networks do
and the first topic of this lecture. So in the variational encoder, we had to components,
we had an encoder, which encoded the input image into a lighting space, and then a decoder,
which decoded the back, the lighting space, back to the two images. With generative adversarial
networks, what we will do is we will have a generator, so the generator that analogues
to a decoder in the VA, so the generator will get some lightened noise and the generator will
produce the images. So these lightens, we will assume they come from some prior distributions
of the distribution of the lightens, but we don't want to know anything more, just what is the
place or space where we can sample from the noise which we will pass into the generator.
But instead of, for example, having an encoder in VA, we will have a discriminator, so the discriminator
gets an image and it tries to say whether the input image comes from real data or whether it's
synthetic, so whether it was generated by the generator, right, so we have a generator which
from latent point generates an image, and then a discriminator which for a given image should say
whether the input image is either real or synthetic. And we will train these two models
together, we will imagine that they are playing a game, a game where each of the models is trying
to fool the other, right, so the goal of the discriminator or maybe outplay the other, the goal
of the discriminator is to successfully distinguish whether the input to the discriminator is either
real or synthetic. On the other hand, the generator which will get the latent noise, so on the
input and generates a synthetic example, we will try to generate such a quality synthetic outputs
that the discriminator would not be able to distinguish the real and the synthetic images, so we can
imagine these directions as a two-player game where the discriminator tries to be as good as
predicting the reals or synthetic, as possible, so when given the data it will try to maximize
or predict the largest probability for them being real, and if we can generate the images which
we do by sampling the latent noise, so here running the generator on it and passing it to the
discriminator and then computing the output, then the discriminator will try to say that it is fake or synthetic,
so from this objective we will look at the probability of it saying that it's synthetic and
the discriminator tries to maximize it. On the other hand, the generator tries to
do the opposite, so it tries to convince the discriminator that there are these outputs of the
generator are real, so this is like a two-player game with a zero sum, so that we have some
theoretical results about such a class of games, and ideally what we would hope is we would like
for the training to work like this. At the beginning, let's say that we have some real data,
let's say it was traded by this like distribution here, and the generator is generating some kind
of data, at the beginning it's probably something very random, and then the discriminator tries
to distinguish between them, so after training for a while, the discriminator should become good
and distinguishing between these inputs, by so in this very toy example, on this left part,
it would say, yeah, this is real data, right because most of the real data are here. On the other
hand here, it will say that the data is synthetic, because well, that's what most data here are.
And this information, like this knowledge about what is synthetic and what is real data,
can then help the generator, so that's the main point of discriminator, it's just to act
as a trainable loss for the generator, because when the discriminator can tell,
what makes the synthetic image synthetic, then when the generator tries to go against that,
it will be fixing the output, so it will be making the outputs closer to the real data.
Right, so what we need for it to work is for the discriminator generator, for the discriminator
to be differentiable, right, so whenever the discriminator is reasonably well trained,
then if we have a synthetic data, then we can back propagate through the discriminator
and the generator, which would allow the generator to change the behavior, so that generates
images which are closer to the real images, right? So this is how well, that's what the main goal
of the discriminator is just to be able to provide us with the information, like with the loss
of how the generator should improve. So in the VAE, what we did is we had this gold in the
image X, and we tried to generate it, so we started the image and we tried the decoder to
actually produce it, but it was difficult to find out what latent loss we should use to do that,
so we also had the encoder, which came with this latent variable Z, and then we tried to decode it
back here in GANS. We don't start with some specific training data. We just take a data
set of two images, then the data set of specific images. We train the discriminator, and then
when the generator generates something, we don't really compare it to some of the training data
explicitly or something, which has had the discriminator, and we use it to give us the information
of what we should do to improve the quality of the data. So what we will do during training
is that we will have these two objectives, so one of the discriminator which tries to maximize this,
and the one of the generator which tries to minimize this, the discriminator ignores this,
sorry, the generator ignores this part, because the generator plays no role in it, so it only
can do something here. So when we will alternate between optimizing the discriminator and the
regenerate. For the discriminator that's kind of well behaved, because we are trying to maximize this,
so if we would switch to the minimization kind of approach, then this would be a negative
flock likelihood, this would be a negative likelihood, the objective of the discriminator is like a
standard MLE generated by no recursive laws. However for the generator, it is a bit different,
because we are trying to minimize something, and it's like a positive flock likelihood, so this is not
the usual MLE laws, and it is that it's really impractical, because the gradient, so that laws
might saturate at the beginning, so when we look at how the, how the laws look like, so
imaging, how the logarithm of 1 minus x looks like, then it looks like this, and close to 0,
the gradient is kind of small, so if you imagine that the discriminator is actually
using a sigmoid to do the discrimination, then in the web propagation, we will take the gradient
of this logarithm and then might apply to the gradient of a sigmoid, and when the discriminator is
very powerful at the beginning, this gradient will saturate, so we will not get anything, it will be
very difficult to see what we could do to improve the performance of or the quality of the
images, because they are so bad that small improvements would not make the discriminator
happier, because of the technical formulation, which we have done, so what we would do is
it would like to come up with a different laws, which wouldn't saturate at the beginning of the
training where the discriminator is very sure, and what we could do is we will change the laws
to instead look like this, so instead of minimizing logarithm of 1 minus x, we will minimize
the logarithm of minus x, so that's the red laws, it is the same same objective or the same
minimum, but at the beginning of the training, where the discriminator is very sure, the value of
this law is very large, so it will provide some gradients using for training, so even if the original
to player formulation would suggest that the generator should minimize this, we will
estimate minimize this objective and we will, and we hope that it should converge to the same
solution, although this is what you would done automatically probably, when I would tell you
that the generator tries to make the discriminator to say that the end book is real,
because this is the standard negative lock like little blocks, so by changing these losses,
we have gotten into something very, very strict over it, so the overall algorithm looks like this,
we alternate in training the discriminator and the generator in the discriminator training, we generate
a mini-batch of noises, we generate a mini-batch of real examples, the noises are passed through
the discriminator, and then we try to minimize the negative lock likelihood of the predictions
of the discriminator, a new original paper they said that, well, we, for the whole training to
work, the discriminator should be of a reasonable quality, so maybe it would make sense to
to train longer for the discriminator and for the generator, right, because if the discriminator is
like 50, 50, you receive that the gradients which come into the the generator are of very low quality,
so they wouldn't help us to actually improve, so ideally the discriminator would have like
70 to 90 or something performance, so it is good, but not entirely perfect, and then it would
give that usually gives the generator the best idea, the best direction of what it could do
to generate the most real-looking reasons, and then after saying the discriminator, we will
train the generator, so for that we will generate a mini-batch of noise, samples, sometimes they are
the same samples as here, sometimes they are not, and we try to minimize the negative lock likelihood
of the discriminator saying that the generator outputs are real, so let's have a look at some
examples, these are the examples of the original paper, the original paper list is from 2014,
you can see that the samples are actually quite of a low quality like for the MS, it's kind of
us also, then there is these faces, data sets, and then these are C4, C410, data sets, which
were the samples are of really bad quality, I would say, but despite these, not very convincing
result, the idea was so nice, and people were impressed by it, that the idea took off very quickly,
and many people started to improve this idea in the architecture, and very quickly, a new workshops
were created, were just the generative, for example, training was presented, so here on the
MEC the samples, the images on this right side, we also show the authors, also show the closest
example from the training data, so for the generative models, it's always a question whether they
can just replicate the training data perfectly, or whether they can actually generate novel samples,
so if we show the nearest closest sample from the training data, we can visually assess whether
they're the model, or created something new, it wasn't present there or not, so for MEC the
samples looks kind of similar to something else from the training data, but in the other data
sets you can see that it's not the case, well mostly because the model is not strong enough to
actually replicate the samples from the chain set, but at least we can see that it just didn't end up
and memorizing the training set, but usually the models cannot just memorise the training set because
they need to generate an output for any latent noise, so it's likely that they will generate
also novel things because the latent space is very large continuously, infinitely usually,
so as I said, the incremental improvements of this idea was coming quickly, so a few months later
there was a paper or they show how to make the gains conditional, sometimes you might want to say
that I don't want to generate a handwritten 9, not just any digit with the model bit until now,
so if we have data which have also labels, so the outdated now are not just excess or some
labels, why we can do model for conditional generation of the images or data and it can be
done kind of straightforwardly when we sample the input and the label from the data, we need to
pass it on the input to the generator apart from the latent sample, but that's not all because this
way you would say to generate a I1 to generate a 3 and then the generator would give you an x-quiz
it 8, for example, then the discriminator would say yeah this looks like a great digit but it's not
3, right, so the discriminator also means to be conditioned on the label and then the discriminator
can actually assess whether the data are like real looking and also corresponding to the label
asked for, so regarding the visual quality in the original paper they just use the feet forward
layers of some number of linear layers both in the discriminator and the generator,
which is definitely some optimal, so if we want to produce higher quality samples we should
need convolutional networks, so for the discriminator that is kind of straightforward because well
we can just use a standard classification method like on image net but we are just predicting
two classes instead of 1,000 classes on image net. For the generator we need to start with some
latent sample and then generate a high resolution image so to that end we can just use the
like part of the unit architecture if you remember in the unit architecture we started with image
then we generated some high quality low resolution abstract features and then we generate
the image again so it's like an autoencoder but we will actually also like sometimes you
be also yeah I'm sorry so it's like kind of variational autoencoder thing or autoencoder thing
where we start with the next then do this abstract blatant representations with the features and
then we could generate it again so this this generator part well that's really just
very analogous to what we did for example in kex segmentation so we start with the latent space we
do some linear transformation and then reshape it into low resolution large number of features
so for example here they propose to generate an image four times four times one thousand channels
and then we will be doing a lot of convolutions and once upon a time we will do a transpose convolution
of strike two which will make the image larger and we will decrease a half the number of channels
so after some time we will end up with this high resolution image which will be the output of the
generator so to look at the samples here are the samples of bedrooms generated by the models so
you can see that there are much more convincing than the samples from the original paper
this is here and half later after the original paper when we have generated what
or we might again look at what the latent space look like right even games came up with some kind
of a latent space because well any samples that from the latent space is able to generate x so
imagine that we will sample two points in the latent space x z 1 and z 2 so it
this time you'll sometimes interesting to look at how the model behaves when we try to generate
samples for the latent variables which interpolate between these these two points so that's
what this image or the sample looks like here on the left side we have generated some number of
samples each for randomly selected point in latent space then we have done it also right
so we have independently generated a different latent space there and we can look okay I'm sorry
I said that they are independent but they are not so this point is actually this one and this
point is this one so we have generated the latent variables here and then what we do is we make the
model to interpolate between these generated between these generated images so that that happens
in the direction right so here we see the interpolations gradually going from the latent
code of the first image to the latent code of the second one and you can see so ideally the
all the outputs I should always be the real looking generated images but they will probably smoothly
transform change from the first one to the second one so sometimes that's kind of interesting because
you can see that here we have some orange I don't know part maybe a pillow on the bed or something
and so here it's not so you can see that gradually it's getting smaller and smaller and then
at some point it stops to be there or here you can see there is window there so when looking here
it gets gradually smaller and smaller and at some point it becomes a lamp or something something like that
but generally the interpolations look kind of nice so apart from the linear interpolations we could
also do the similar thing which for example were to like date in the in the in the in the
bedding space so first know that the original papers for generating images were kind of like like
required a lot of courage to look at because the samples were not really very realistic looking
so they are more like horror looking samples especially when we are generating people or animals
but apart from that what we could try is we could try looking whether of the latent space
also somehow encodes meaning so we could try doing this linear algebra thing which we did
in in the bar two eggs so let's say that we get a latent representation a latent vector
which is able to generate a smiling smiling woman and the latent space which would generate
a neutral woman right so here we have got like the woman neutral here we have like a woman
smiling and then here we have got like a man neutral so we would like to we would hold that
like subtracting the representation of these would just mean change from neutral to smiling
independently on whether it's a man or woman so then if you take this vector and edit
to the representation of the neutral man we could hope for forgetting smiling man which
with some fantasy the model was actually able to do and of course doing this in pixel space
wouldn't work like so doing this directly on pixels would produce very the red looking
results while doing it in the latent space works better you apart from the low quality of the
samples note that filed finding the latent code the z in the latent space which is able to
generate smiling women is not not simple with guns because this is not something what the
gain can do the can the gain can take the latent space generated into the image but ideally here
we would like to do some kind of an opposite thing and we don't have this kind of component in
guns so how did it work is that somebody sampled a large number of latent samples from the
prior like you can imagine hundreds and then somebody looked the images that manually chosen
the ones which are smiling women neutral women and neutral men and then because these were generated
from the latent codes and we can keep latent codes so we get the latent codes of all these images
and then these were averaged thus obtaining a simple single point in the latent space
of who we then associated with smiling women this was then repeated again with glasses and on glasses
so we have taken the representation of men with glasses without glasses attracting their latent
representation edit the presentation of women without glasses and we have obtained a lot of women
with sometimes very dark glasses we can also look at these interpolations and the
others started with the faces from their right side and then try to interpolate a different
face shown from the left side and they try to show that the model also encodes this information
of how the face is rotated in the input so and this is like generated automatically without
any labels or something's real like I'm supervised kind of training or the model just forms
the latent space so that these for us like abstract or high level features are actually encoded
in the layout of the of the latent space so guns are actually difficult to train
while it might seem nice and easy in the papers so when you try it you will see that the training
is difficult and make it converge is kind of tricky even in the very very good papers
they sometimes say we've trained the model for a few days and then the training crashed we don't
know how to prevent the crash but anyway like diverge when I say crash so the loss became very large
and then the more you stop generating anything interesting but if you take the model before
this divergence and if you sometimes generate a very very good samples so for for comparison
this is what I could get when training on this bedroom beta set so when you look at it kind of
looks like bedrooms but some of them are really crazy color and so this is like the result of
I don't know a few days work trying to re-implement an algorithm from some paper so
it is a bit tricky and it is also tricky not just practically but but theoretically the thing is
the way how we train guns it's not even guaranteed that it would converge and the problem is that
if we try to finding some kind of optimum for this many max games so when we have an object
they even some player wants to maximize it and the other ones to minimize it then trying to reach
the optimum by alternating the steps which and independent minimize and maximize the
objective from the point of the individual player so this is not guaranteed to reach any
optimum like normally as gd when we just minimize one single objective it can reach the local
optimum with some assumptions but here it's not the case so consider the following very simple
objective like for example game we're able to players and the first one chose his eggs the other
chose his y and the the payoff would they get is well one of them gets x times y and the other
gets minus x times y so you can look at it as that one wants to maximize this product and the
other wants to minimize it so when you look at it like that then the optimum like a Nash
equilibrium in this case is for both players to generate zeros because if you will generate zero
then the other player cannot take any advantage right on the other hand if x just decided that it
would generate one then the y can take advantage by generating so here we say that we want to maximize
so why being very large number right and that allowed there is how to be very large and the y
would be happy and the larger the y the number y would select the better it would be while if the
x chooses zero then the y cannot just take any advantage of it so that's what we would like this
to convert to so now consider what happens if the as gd is being used to alternatively maximize x
and y so for the x is what we get is we get x and and then we want to minimize this so we take the
current value and then we'll do the minus times the learning grade times the gradient of the
objective so the objective is x times y so the gradient of it with that with respect to x that would
be just y and right so the next x and I can be committed like this and the next y and plus one
can be computed by taking the current value of y and then you want to maximize this we are
maximizing so I will say plus instead learning grade times and again the gradient of x times y
with respect y and then this is just x and so we can write this in a matrix form where we would say
that the x and plus one is one times x and minus alpha times y and and the y and plus one is
alpha times x and with the power and then plus one times y and that's that's this part so
the sgd what it does is it takes the original pair of x and y's and generates a new vector of it
by multiplying it with this matrix so when you look at this matrix it actually looks kind of similar
to rotation matrix the rotation matrix in 2D looks like this it is going to cosine on the diagonal
and the sine on the opposite elements and the sine have an alternating sine so this is the same
structure but the numbers are larger here right because here the cosine squared plus sine squared
get 1 so if we take these numbers and divide it by the square root of 1 plus a squared then
this will really become rotation matrix times this vector this multiplication vector and it's
multiplication vector is larger than 1 so the course of the training looks like this we will start
at some point and then we will be rotating and we will get further and further away because each
application of this rotation matrix will get us further further from from the optimal from 0 0 right
so this will diverge like every step of the training actually gets farther away from from the
optimal in in some sense right so generally like when you look at it like this when you are x
player right so you are like increasing your value to adapt for the fact that y is doing something
and then you will see that it's not advantages for you so we will start decreasing it but then when
starts being useful to you the y will react as well so you will need to react again so the
training will never get to this 0 0 but instead it will be more and more chaotic where you will
try to generate larger smaller larger smaller numbers and so on so that's that's bad like even
theoretically the procedure which we are doing does not really work so in practice the this
manifests manifests into an issue which is called the mode collapse and the problem is that
usually gains can generate nice quality samples but they don't cover all the possibilities available
in the train set so for example what could happen is that in the and for the M is data sets the
generator might do well at generating three and fours but nothing else for example charge these two
digits would be the only thing which it would be capable of generating and that's not what you want
right you would want the generator also generates 0s 1s 2s and so on and so on so this is the
called the mode collapse because instead of generating like all images from all the regions with
large density we would only generate samples from some of the regions but but not all of them
so here that's also visualized so let's say that we would like to generate images
and our data points and these are the densities to their eight kind of quite separated clusters
of data and what happens frequently with gains is that we would generate just just one of these
or just a subset of these clusters so during training after some time the discriminator realizes that
right so if you imagine the generator to generate only three for example then after some time the
discriminator will see I am seeing so many threes that the threes are just synthetic right
it's just so often there much more than it should be and it will say I don't like it and when this
happens the generator will react so maybe the generator will start generating fours instead of three
so at the beginning the discriminator will be happy because well it can see a nice four but after
some time it realizes that the fours are two free went then then they should be probably a
minute we'll say I don't want you to generate four but what could happen is that they could like
chase each other so the discriminator could so the generator could be generating only one of these
digits and the discriminator is always like forcing it to generate another one after some time but
it could easily happen that instead of generating all the digits at the same time the generator
will only digit a generator subset of them at each at each interval so there have been multiple
ideas or proposed improvements how to handle this mode collapse one single
or simple way how to improve the situation is to allow the discriminator to consider the whole
batch not just the individual images but the whole batch because if the generator is generating
threes only so if you are generating data from like very similar images of the same kind then
if the discriminator can see the whole batch then it can immediately see that that the very similar image
is appearing multiple times in the batch so it can get much quicker feedback of the
images being very similar from the generator and even just a batch normalization helps a lot
with it is because the batch is normalized across or a batch normalization normalizes across the whole
batch so if you have a lot of very similar elements in the batch then it will influence normalization
in a way that this can be observed by by the network itself so in this convolutional samples
the author actually got a big boost from using the convolutional transfers because that
automatically meant that they started using batch normalization and that meant that it
helped the discriminator to actually detect this situation where the images generated by the generator
might be too similar to each other the other thing which you could do is you could do this so
called historical averageings or what you could do is you could consider not just a single discriminator
at the new one but you could consider a whole sequence of historical versions of the discriminator
and when training you could randomly sample one of them right so you would hope that this way
you would prevent the model to collapsing at any time because when we have this large collection
group of discriminators then if there is this behavior where the generator is only generating some
from some region of the images then if you have a lot of discriminators then each of them
hopefully would vanish punish the model for generating too many images from some given region
of the latent space so together they should provide a nice signal for training so that's what
happens in this in this first image that when the discriminator is being sampled randomly
then that makes the model to try to really generate the data from all high probability
of the image space yeah and the labels moving can also also help quite a lot
this labels moving is applied only on positive samples with the target for the real images is set
for example to 0.9 instead of 1.0 and the idea is to avoid the discriminator to be overconfident
so to to overfit on the data because then the generator would try to fool it by
by generating or relying on these overfitted features or with the generator every discriminator
will know the training data by some peculiarities which will just allow to fit them perfectly
then the generator replicating these might not be that useful so to to counter this overfitting
on the training data we might use smaller target value and hopefully that would make
the features which the discriminator rely on to be more more generic more transferable
and therefore the images generated by the biogeneator should look a little bit
so now we have got the variation on the genders and gans so to compare them together
the variation auto encoders are I found this price number in a paper and I really like it very much
are theoretically pleasing so that means we have a nice loss we have a nice size theory they
they don't suffer from the mode collapse they also have an encoder for example in contrast
again when you have an input image you can easily easily find the point in the latent space
which can be used to generate it so that can be used for unsupervised feature instruction and
actually the VAEs are being used in many modeling architectures to get you nice and supervised
features of the inputs but that is advantage of the VAEs is that the generated samples
tend to be blurry we have seen that already on the on the previous lecture for example here when
you look at the samples you can see that they are really like blurry looking blurry looking images
and this is caused by the combination of the loss and the way how the VAEs work because
if you imagine like here are the images so like this one x2 x3 now these are mapped into the
latent space right somewhere so maybe I will do these parts of the latent space in different
color right and so during the reconstruction what we do is we sample a point somewhere from this
distribution generated by the encoder so maybe that's it will be this one right and from this point
this can be sampled from all of these spaces so this can be this z can be sampled for both
images so the goal during training for this for this point might be to generate both x1 x2 and
also x3 and the simple way for the model to do it is to produce the average of those right so
because we don't give like an generated an inclusive exclusive latent space for a point for an
input image but we generate whole subsets right then every latent code might be used to generate
not just one but that's a number of images and if we just measure our error which is at the loss
then the optimum of that is just to generate an average of these similar looking or close or average
of the images how whose latent codes are close to each other in the latent space so people have
overcome that later by using a more suitable loss so that if the blurry image was generated
the loss was bad all the time instead of of being kind of an avoid on the other hand
gains offer very high sample quality and but they are difficult to train both theoretically
and practically and they suffer from this mode collapse so even if you train them they
previously generate just the images from a small or some part of the image space but anyway
the gains have saw a big development from their proposal and for some time they were like
the best approach of generating generating image samples so there have been a large number
of papers about them and for example here you can look at the images sample from the so-called
big gain which is from 2018 so the samples are definitely much better looking than the one from the
original paper but apart from the good images the more the gains still suffered in various domains
so that people are very weird and the musical instruments are are very useful but at some point
the quality of the samples generated by the various alternatives also increased since let's say
2019 or 2020 they dealt with blurriness and with the way how the latent looks like so they have been
used for generation 2 and they allowed generating or they they allowed avoiding the mode collapse
so for example the original dirty model that you want used the VAE to generate the images so all
the images which you can see here have been generated using among others a variational autoencoder
but of course people have tried also other approaches not just gains and VAE and
in 2021 one of these competing other architectures became prominent and ended up to being currently
probably the best approach of generating images and that's diffusion models so in the past years
I would say they have become the defector like best or the chosen architecture for generating
images the one being used by the stable diffusion and mid-chernet and Sora is using this kind of
approach so nowadays I would say that they are like the architecture so these diffusion models
will be the topic of the rest of the today's lecture apart from being called diffusion models
the resulting architectures of the models are very similar to another class models which are so
called score based genetic models so they were developed independently the score based in
native models and the diffusion models but at some point the groups realize that they are describing
very similar way with just different terms or different views so they converged to each other and they
like used the usual view either of the score based ones or the diffusion ones depending on
what's most suitable for the current problem which you want to face so when you see something
about score based generative models then that's from the practical reason very much similar
to what I will be talking about on the practicals I will also like directly talk about these
score based generative models to give you an idea of how they look like and how they relate
to the diffusion models which we will discuss today so the diffusion models from the high level
overview will work by describing our two processes or by utilizing processes the so-called diffusion
process we can also call it a noise process starts with the original images and then adds a lot of
noise so that the result is completely noise and then this is simple to describe we will just somehow
describe how noise is added to images and what we will do is we will try to train the opposite
model so we will try to train denoising process which given a noise sample tries to generate
the original image we generated it and then this denoising process will be the one used for for actual
generation so to look at some practical example here we can imagine that we start with the images
on the right side so in the nosing process we gradually add noise until we get real full
really full noise and then the generation will work by starting with a complete noise and then
gradually getting great of the noise and finally generating the output image so this is from the
template which you will be working on in the assignments so to describe this slightly more mathematically
let's say that we are given a real data point so it's a clean image x0 which comes from the real
data data generating distribution and we will describe or define this diffusion process or
noising process as a T step process so some number of steps it's also being called forward
process so that we just go forward and then gradually we'll add the Gaussian noise to the input
image so to generate the sequence of noise images from the original input in every time
step we will take the image which we have so at the beginning we will take x1 and we will generate
sorry x0 when we will generate x1 by adding some small number of the Gaussian noise and then
we will take x1 you'll add some more Gaussian noise and we'll get x2 and we will do this until
until we perform all these T steps finally arriving as this XT which could be like fully
full noise image our goal will be to reverse this process so instead of this forward
process we will try to learn how to do the reverse process how to do the opposite and then
what we can do is we can generate the image by starting with the pure noise so we will sample
a pure noise and then we will perform these inversions of forward process so we will be performing
the reverse process and this reverse process will be approximated by a neural network so
we'll have a network to be able to slightly denote the given image so to show
you some more example for example let's say that we have got this some kind of a data right so here
it only pixels but you can really think of all this as objects on a 2D in 2D space and they have
some kind of density so they should be on this on this curve and so in the forward process
or the noisy process or the diffusion process the noise is being edit so every point is being
moved randomly by the Gaussian noise so after more and more steps the points will get spread out
and then after full all the T steps we should get something very very noisy without reminding
the original date and then in the reverse process we will go the other way around so we will start with
this complete noise and in every step we will try to reduce the noise a bit so that would that mean
is that we would try to move the points to get closer to the regions with high density right
so they should start getting closer to where they should be so after the full reverse process
executed we should hopefully end it up with the data in this thing wishable from from the original
or like belonging to the image space right so when I say get away or denoise it then we would have for
example here some idea of what is like the direction where if we push the data they will get to
a more dense dense regions so we will make them more urban so these denoising models
so generating xd minus 1 from xd it's usually performed by unit architecture right so an architecture
which starts with the image then it performs sort of allusions generate abstract representations
with a smaller resolution right in a large number of channels and then we do like the opposite
and generate larger larger images with smaller number of output channels finally
obtaining the denoised image and we will have some residual connections being edited
over here is a illustration from the paper let me start with the convolution then there are
this dumb scaling blocks here the resolution it is small is and there will be some up scaling blocks
here are the residual connections right in the end we will generate a larger idea you're in training
what you will do is we will train this model by training it to be the inverse of the noising step
like so we will randomly sample the time we will randomly sample the noise image xd minus 1 we will
start with x0 and then noisy it enough to get xd minus 1 right then we will noisy it again using
the forward process to generate more noisy version and we will train our network to start with this
image and generate this out of it instead so that's how you will train it and then to use it
generate the images we will start with xd and then we will do many steps p steps and in every step we will
invert the addition of the noise slightly so hopefully after the p steps we will arrive
at the x0 hopefully mean and newly generated image so that's the overall overall idea that
we will start describing the formalism which will allow us to do this training and sampling
in the our first initial attempt this sampling will really require large number of steps
then we will see how we can how we can decrease it to something more modern
we need to start with some like reminder we will be using a normal motion distribution a lot
in this in this lecture so the normal distribution is the distribution over a continuous
random variable and it's being parametrized by a mean so the mean at the place where the density
is the largest and is the mean and mode and the median everything of that of the distribution
and and some variance where the variance does you have wide this this bell shaped curve
it's we also know how the how the probability density function looks like even if it
will be we won't be needing the specifics anyway and by the way this distribution is
this maximum entropy and look the one however we will be generating vectors so we will be
interested in the d-dimensional version of it so let's call it a multi-variate Gaussian distribution
well the the generate multi-variate Gaussian distribution is actually complex because
while the mean is just the vector of means these variances are in the d-dimensional
pace parameterized by the so-called covariance matrix so this covariance matrix is of size d times
d which might be complicated when these large right so instead of just the numbers it's
really this squared and that's a symmetric positive symmetric positive definite matrix
which tells us how the variance is look like not just for the individual dimension but how
the different dimensions correlate with each other so the this multi-variate Gaussian
distribution when you look at the the pdf for example in the 2d then the one way or the
simple ones are just just like circles right so if we imagine the the the distribution coming
like up then this is this would be the the high point of the high density and then it would be
good down gradually and the curves with the same density density would be circles and if you
want them to for example generate ellipses with different that's not good ellipses and ellipses
with different rotations then you need to be able to say that the other dimensions are somehow
connected correlated to each other which is what will allow you to rotate this this kind of thing
so I don't really want to get that much into it because apart from being in practical
we will not be using it at all so in our case we will only consider the so-called isotropic
distribution so the covariance matrix will be just diagonal matrix with the sigma square
everywhere on the diagonal so even if we are using the d-dimensional version of it in this lecture
you can think about it as a lot of independent one-dimensional Gaussian distribution so every
dimension we'll have its own independent distribution which will have some parameterized mean and
we will have some fixed variance shared by all the dimensions so when we have this normally
distributed random variable we can either say that x has got distribution or we can use the
repair parameterization trait which we used for the variation auto encoders and write this
random variable as standard normal random variable so to one without any parameters
which will then be multiplied by the standard deviation so the square root of the variance
and then it will be shift or of set it by the mean and we will use this repair
parameterization or parameterized representations quite a lot in the formulas
we will also need to know the fact that when we sum two normally distributed random variables
so add them together then the result so x1 plus x2 is also normally distributed
and the means are just added and also the the variance is added so it's like a nice operation that
for for independent normally distributed random variables when we look at it from the point
of the repair parameterized representation it doesn't look as size because we don't have
variances in this repair parameterized representations but a custom the deviations if you imagine that
we have for we have got like two standard normal random variables with some some standard deviation
sigma 1 and sigma 2 then this this sum has a variance which is the variance of this first one
plus the variance of the second one so the standard deviation of that is the square root of that
so we will be able to sum these independent Gaussian random variables and we also have the formula
how the mean and the variance of the sum will look a big so we will start by describing the
ddpm so that's the model for for diffusion models denoizing diffusion probabilistic models
one which has become the the default terms standard one and then we will improve it later
to allow more efficient sampling so in this ddpm model we start by describing the forward
so the noisy process so we assume that we start with the clean image and then in every time step
we add some noise and then this addition of the noise will be described by the distribution of
how the noise image should look like given the input one and this this noisy will be
performed by the normal distribution so we also say okay we the the resulting noisy image comes
from a normal distribution where the mean will be suitably scaled original image and the noise
will be generated independently for each for each dimension with some given
variance right or here the beta t is the variance on this of this noise we can write this down
also using the representation random parameters through the parameterized way so that this
noise image is the original image plus noise of some suitable range it might seem surprising
that we are scaling this image down like what would say that maybe you should have taken the
original image and just add noise the problematic part is that then the if you imagine for example
pixel intensity then they will get large and larger because even in the input image they you might
have like a full red and then adding more and more noise couldn't be actually performed so what
we want to do is we want to keep the variance of the sample images like the same ideally so that's
why we are scaling also the image down and we do it by a suitable factor so that the variance of this
result is still one right so we know that like this this image and this noise are independent
random variables so by using this formula for adding them together we would get that the result
would have like if we consider this xd to be random image with the standard parameters like
but if we if we do that then the result would have a variance which would be which would be this
squared plus this squared which would be one yep so now what we will do is we will show how we can
compose a multiple of these forward operations into one right so what we do is we gradually
add more noise by in every times then but the thing is adding the noise because it's always
Gaussian then we can actually express analytically how what result we should get if we for example
want to directly get x4 right so we'll be able to generate x4 directly instead of performing
for noise steps so we will by having defined how this one step noise looks like our goal is to look
like how or this they derive how the teeth image will look with respect to the original one
so that's the current goal of this slide the result will benefit from defining more variables so
we know that the beta t are the variances of the noise they are like sequences so we will use different
beta t's for for different noisy steps at the beginning we'll add very small number of noise and then
we will gradually add more and more so we will define the opposite as alpha t because well
then this will be alpha t and it will be useful and we will also define the product of all
alpha from 1 to t as alpha t bar and you will see that it will be useful so we start by writing down
how x t looks like well by definition that's xt minus 1 scaled down suitably plus some some
standard noise so here I'm using alpha instead of beta so here I'm square root of alpha t and
here is 1 minus alpha t in square root of that what we will do is now we will recursively
define how xt minus 1 was generated where it was generated by taking xt minus 2 scaling
down suitably and then adding a noise and now we will rearrange this so this xt minus 1
gets two factors in front of it so the square root of alpha t and alpha t minus 1 and regarding the noise
we have got these two standard Gaussian normal random variables so we will use this this
formula here for getting them together and the result here is the square root of the first
coefficient squared so here we have got alpha t times 1 minus alpha t square root of the square ring
will make the square root to go away and then this part squared so 1 minus alpha n t and then the
square root of that so it's complex but we can make it simpler because here we have alpha t here we have
but minus alpha t so that in the way so the so the resulting variance looks like one minus
alpha t minus 1 and now we will continue so we can expand this xt minus 2 by definition
we will get xt minus 3 and so what will happen is that here in your factor will appear in the
same way how a new factor appears here and when looking at the noise then here what happened is that
this alpha t minus 1 appeared here so alpha t minus 2 would appear there as well so it's not
obviously the probability should happen but that's what happened here so if you try and
replicate the steps well that's that's what you would get and so we can continue with this until
we will get to x0 right so when we get to x0 here we will accommodate the product of all alpha
from 1 to t and we have a notation for that so we can say that this is alpha t bar and again here
we will have 1 minus and the product for alpha 1 to alpha t so we can say that this is the square root
of 1 minus alpha t bar so this tells us how this noise from x0 to xd looks like well we will
still scale down the image and add some noise but the only different thing is the
strength of that right so what is the amount of noise which will appear and how we have to
scale down the image so the variance of the output is unchanged so our overall goal is to end up with
a completely noise image so here we can see that if this alpha t bar would converge to 0 right
then the original image would really disappear and instead we would just get the noise so that's
how what we will want to do is we will want to use such such beta so that the corresponding
overall noise 1 minus alpha t bar how will we go to what so regarding the these values
originally in the original paper the proposition was to start with a very small noise and then
gradually increase it until 0.0 for and it was selected so that when looking at the alpha t bar
we will end up at the full noise images but when we look at how these alpha look like during the
course of the forward process we can see that the blue line here and it's like the signal
of the images falls down quite quickly so after half of the diffusion forward process we are nearly
and fully noise tempo so large part of the noiseing process was not very useful at the end here
so people later came with different sequences of how these alpha t are my look like so that the whole
process is actually useful and they propose to use a cosine schedule for this right so this is the
amount of like the amount of the original image which I will be keeping in the original in the
in the input so which will survive the initial number of steps in the forward process and this
way it's like nicely nicely starts at once with the beginning we have full image and then we
credually add more and more and more and more noise so at the end mostly only the noise or only the
noise is being kept there so this can be written down by some formula so here we are describing
how this alpha t bar looks like and from that we can look at how these individual
data look like so we could from this compute what is the amount of noise the individual steps
in the process would bring so in practice we want to avoid the edges like 0s and 1s because
there at the derivative it's also 0 so that the bitas are well like ill defined there so usually
we keep this this out fast in some epsilon to 1 minus epsilon range to get
great of the problems at the edges so what we will do is we will assume that the original image is
have a 0 mean in the unit variance and we will not just assume in but we will achieve it by
actually normalizing them using the statistics from the train set and then but that will mean that
every of these noise images will also have a 0 mean and and unit variance is exactly because the
variance of this sum is the square of these two vectors and that's always one and also they
they have mean of 0 all of them so the sum will also have a mean of 0 now these vectors like the square
of the square root of the alpha t bar in the square root of 1 minus alpha t bar they can be
considered to be signal rate and the noise rate so like the amount of the signal clean image
and the noise which are there present in the noise image x t and for the more if we want to
keep the the variance intact so if our noise process would always keep the variance of 1 then
that means that whatever these crates we use there the sum of their squares would need to be 1
so whatever kind of schedule you would use if we want to keep the variance
of these signal rates and noise rates would also always form a circle arc right it will always
look like this if we want to keep the variance so and then when we decide of like how
these these sequence will look like then we can say well let's just move uniformly on this circular
arc and if you do it you will get exactly the the cosine schedule which was proposed on this
on this previous slide right because the cosine schedule is exactly what goes
gradually through through this arc right so you can do some interpretation of when we do this
this is this is move of what is the the signal rate and the noise rate during the
the whole schedule so this was the forward process we know how we are noisy the images
and we know that we are doing it in a way that after that so when we start with x 0 and go to
x t then the x t is fully fully noise then a sense that it corresponds to the 0 and 0 1 standard
normal distribution now let's discuss the reverse process so let's talk about to be the
actually model with our network so the idea is to invert the noise in process so here we have
a forward process and we will train a network to do the opposite operation to be the
approximate of these reverse and the the important fact is that if we add a very small number of
noise then the reverse is nearly Gaussian as well and in the case we will represent the
inverse also s a normal distribution so this is not true for very large steps but if our goal is just
to remove a slight amount of noise then saying that the result corresponds to a normal distribution
is actually fine so our model will predict the mean of the slightly denoise image and we will
use some fixed sequence of of variances so this sequence will be specified later and in the end
we will get rid of it all together but right now we need to generate the distribution so we will
not allow the network to say or the variance of the tool be but you just use some fixed
sequence which will somehow correspond to this beta t which we are using in the forward process
the network good and theory also predict this part and some of those do the bug for simplicity
we will just come up with a good constants which will correspond to the amount of noise which we
use in the forward process so the whole reverse process the sampling process then starts with
sampling xd from the standard on distribution and then gradually removing the noise until
we will arrive at x0 so hopefully clean new generated image so what we will now want to do is
derive the loss so we want to say how the loss is how the loss should look like when we
would this reverse process to be the inverse of the forward process and the good thing is
that we can actually explicitly compute what the reverse of the noisy process looks like
if we can condition on the original image so if we would get the information how the original
image looks like then we for a given noise image we can explicitly analytically describe
how the distribution of the possible sources of the possible xd minus once look like it will also
be a normal distribution and it will have some specific mean and some specific variance so this
mean and variance is this so it's why terrible looking formula right but it tells you that if we
have this noise image xd and the clean image x0 then the xd minus 1 so the slightly denoist image
is a suitable combination of the x0 and xd and of course we will need to come up with this
distance somehow and of course also the variance of the noise can be completely so these are
very wild looking formulas but they are not very important like the overall shape the
important fact is that we can explicitly copy the inverse of the noisy process if we can condition
on the original image so there is a proof here on why this holds but we won't go through it
is there just for you if you are interested so you can look at the computation and at the end
you should be able to arrive with a result in formula what we will be interested in though
is in looking at how the lot or deriving how the loss should look like so the overall loss
which we care about is to maximize the the lot likelihood of the data right still
maximum likelihood estimation kind of loss so imagine that we will sample the initial image from
the real data distribution and we will want to maximize the lot likelihood so we want to minimize
the negative lot likelihood of the model predicting this image so how is this model works well
it generates this x0 by sampling xt and then denoising it gradually until it arrives at x0 so this
this part of generating the probability of x0 can be written using this expectation and now
if you just consider any xt right and any denoising sequence then the chance of the
generating directly x0 that's very small like imagine that you have got some real real image like
so there is a nice image which we have and we want to sample an initial random noise and then
this denoising steps so that we will arrive here whether chance of doing it by random is very
small because if we just take the input image by chance input noise by chance and then apply the
denoising step that we will probably arrive at something completely different right then then
what we wanted so what he will do is instead of sampling over these completely random samples
we would use the forward process and we will say so instead of these let's go over xs which are
the noise version of x0 right so let's start with x0 and then noise it gradually using the forward
however when we have an expectation and we change the distribution we also need to it
changes the value of noise right so but what we did is already in the variation auto encoder
when we were deriving the loss there right so it is actually not difficult to change the
distribution here because here this expectation means it's it was the integral where there was
the probability times this this value right so what I mean is that this something what I will
denote as a blue square right and then this part here what it is is that's the integral again
but instead of the beta we have the cube here so instead of the reverse process we have the forward
process there and again this is being quantified by the same quantity right so maybe let's
make this green so that we can distinguish between these cases and so when I want
these two terms to be the same then what I will do is I will divide this by this cube
and I will multiply it by this p so now both of these terms are the same so that means that here
we need to include this this term so we have p of x0 here and now we have good from x1 to xt so
in the denominator we have x from x0 xt and in the denominator we have got the noise samples
conditioned on the on the original image now we will use the the instance inequality
we discussed that on the last lecture and that allows us to move this
locally inside this this expectation and when we do so the result is not equal but it's larger
or equal to the original value but that will be enough for us because we will want to we will want
to minimize the negative look like so instead we have got an upper bound so we will minimize
this quality instead and that will also minimize the original loss which we care about so we
move it out then these two expectations are these two sequences merge together so here we have
got from x0 xt and here we have got the logarithm of of this this ratio and we will get
rate of this minus by swapping the order of the denominator and the denominator
now what we will do is we have whole sequence of images here right from 0 to x xt so first we will
take away the xt from or what we will do is we will start by starting with x0 right so here we will
take x1 the 0 and here we will take x0 in x1 we will keep them separate because they are a bit
different from each other then we will consider all the other one to all the other individual steps
here we have got the forward process which takes xt minus 1 and then noise it into xt and here
with reverse process so the one where we take xt and try to denote it slightly into xt minus 1 and when
we do it there is one more element in the denominator then in the denominator so the xt the last
way one will be left there so we will pull it here so now what we will do is we know from the previous
slides from this one that we can actually invert this this noiseing process perfectly analytically
but we need to condition it on x0 right so what we will do is instead of this noiseing process or
we will express this noiseing process using the base theorem by the the probability where these things
are swapped I have added conditioning on on x0 which we can which we can do and the base theorem
will need to like this swapping we need to pay a price for it and that is the density of the individual
two random variables so now to the next step what we will do is well this logarithm can act on
this product so we will get the sum of the logarithms but the interesting thing is that when we look
here then this will tell us call because in one step we have got xt minus 1 in the denominator
and in other step we will have it in the denominator male opposite signs because it's a logarithm
on that so all of that will cancel apart from the xt the denominator and then the last one x1
in the denominator so now we are nearly done these two will cancel again so the overall result
will contain this part so the negative lock likelihood of generating x1 of x0 from x1
then we have got the t steps of in gen like the reverse process where we denoise the image
and here in denominator we do it like perfectly with the knowledge of x0 and here that's what
our model is trying to do and then we have got some the last elements will here we generate xt
given x0 and here we sample the xt from the initial noise distribution so these are the elements
of the losses which we will try to optimize we can express all of them actually are the these two
the first ones as as a care divergence right because the expectation of the logarithm of
ratio distributions can be written down as a care divergence divergence so here it's the difference
between the generating xt in the forward sample generating it in the reverse process here is the
difference between the ideal inversion of the forward process and what our model does and here
is just a probability of generating x0 from x1 so we will pull couple these components
in turn so the first one that sometimes covalete out of the LD for the times the t
then that tells us that ideally we should minimize the difference between the noise image so
what the forward process generates and the noise which the reverse process starts with that make sense
we want these to be the same but this this doesn't depend on the parameters right because
we generate this by sampling from the normal distributions so this just means that we need to
choose the parameters of the noise being so that the noise image will correspond to the
prior which we have selected in the in the reverse process but an optimising then at work we can
just ignore it then we have got the most interesting part so the difference between the ideal
inversion and the inversion performed by the network out here the care divergence between the two
gossians and that can be computed explicitly we can analytically compute the integral and
the result looks like this so it means where error between the mean of the ideally the noise
image and the mean predicted by by our model and then there is some ratio here which depends
on the variance of the distributions so this is in some sense very natural right just when I said
that we can convert this step this step ideally analytically then it's kind of natural to say okay
so we will want this to predict the mean exactly the one which this ideal inversion gets so
the resulting clause is not really it's not really surprising but that's really like a natural
way of doing it including the means for error but we've seen that we have come to that not
budget saying that's natural but really by optimising negative or by deriving the maximum likelihood
estimation loss then there is the far last component of generating x0 from x1 that's most
the technical and that can be used to generate this create image from the continuous x1 the thing is
the images usually are represented by pixels and the pixel as a pixel density from 0 to 255
and they are discrete these values so we can model that more carefully then just generating
a real value and then the rounding it to the nearest pixel values so if we want we can do it in this
place and it is usually done but in the slides we will not talk about it anymore we'll just ignore it
for simplicity so we can imagine that we will just denote x1 you get x0 as a real number and then just
coercing it into the nearest discrete intensity so now we are nearly done we nearly have the
loss which we want to do but one of the issues of this model is that if we were to predict the
denoist image it might be difficult kind of for the model because well the scale of the
noise is differ a lot depending on the T and this way the model would need to
really include this information in the predictions so we will show that we will be able to do
to better and what we will do is instead of predicting the denoist image we will aim at predicting
just the noise itself right so we will instead of this we will just try to describe what the
noise itself is so first let's look at how we could or how it would look like if we would try to
reparametize this ideal inversion into the settings so when we take the formula for the mean then
what we can do is we can we know that the XT image after the noise and the T steps
can be expressed by taking the original image and denoist and then scaling them with the
proportional or appropriate ratios from that we can get that the original image
can be expressed as the difference between the noise image and the noise and then the noise is
scaled suitably by the noise ratio and the result needs to be scaled again by the
inverse of the image of the signal ratio so using this formulation can put it
here and reparametize how this mean would look like when we would consider not just not XT
at X0 but XT and the noise which generated from from X0 so we put it there and then after doing
the math I want to go to the formula sub but what you can do it it's not difficult and then
we'll see that the result is again a combination of XT and the noise ET not applied by this
suitable suitable factor so here you have a signal ratio you use a noise ratio and here is a noise
ratio square so the ideal inversion of the noise process actually corresponds to taking the
image predicting the full standard noise which generated and then combining them suitably
so taking the reasonable scale of the noise subtract it's all removed from the image and then
multiply the image suitably so we'll make our model to predict just the standard noise so that's one
of the advantages of this that the noise which the predicts will always have the same variants
in dependency on the time step and the loss which we will try to optimize will look like this instead
of the means we will rewrite them by considering XT and the ET and so here it is like the real underlying
noise and this is the prediction of the noise and the Xs they will they will cancel so the only thing
which will remain there is the noise and our estimation of the noise that means square error of
that and then all the constants which will be there which depends on the signal ratio and the noise
ratio and this XT right what it is well it is just the original image X0 which is noise using this
noise ET with a suitable noise and signal ratio so this is the loss which we will which will
use we will try to predict the noise when processing the input image plus the suitable scale
of the noise on input now regarding these constants these constants what they do is they wait
these losses differently for different time steps so when looking at these loss we will concentrate
on a different time steps with different strength but in practice it turns out that this is not ideal
because then the network trains for different times steps with different strength so instead the
authors propose to train differently and that just means without these coefficients we will train
by randomly sampling a timestamp time step randomly sampling the input image randomly sampling
the noise and minimize the means for error between the randomly sample noise and the noise predicted
by the network where on the input it will get the noise image plus the times that p so it knows
how strongly the input image was actually noise now without these constants
they these solutions they converge to the same optimum because the optimum is to minimize this loss
for all the time steps right so what we have just changed is like the speed in which we will be
reaching that optimum for different time steps if if the network is in our capacity and we have
done it to be uniform so the network will train with the same speed for all time steps well
which in practice performance so we can take all of this and come up or describe the training
algorithm so after all this math we will end up with a short training algorithm that that's good
so the training will start by sampling the the clean image sampling the current times that
sampling the noise and then taking the gradient step of the means square error between the
generated noise and predicted noise by the network we can also describe sampling algorithm
right we know how this forward or how this reverse process looks like we have it described here
so we say that the process which removes the noise will generate the distribution where the
mean is the one predicted by the network but with the variance which comes from this normal distribution
standard noise scaled by this sigma so this mutable suitable scale of the noise which corresponds
to the strength of the noise in the forward process right so to do that we will start by sampling
xt from a standard normal distribution and then in every step we will compute this this new
this predicted means so it predicted mean that's this right so we will take the xt will predict the noise
we will subtract it so here we get the slightly the noise image and then we'll add the noise so it
might seem a bit counterintuitive to add the noise here but this this the noise thing it's not
perfect right it's just goes to the mean so this this noise there makes sense in some way because
hopefully even if we did some some error here we are fine because we will be moving
to some random sample of that the noise image anyway and another way we can look at it as we are
sampling we are not necessarily trying to generate the perfect output but sampling the image
and that's when we do this once the reconstruction we shouldn't arrive just one point but maybe
at the whole collection of possible little images and this part just chooses one of them so so some
notes in practice we do not necessarily need to use the discrete times that we did it like that
by the speed steps but when we look at the math this T if we consider it to be like from 0 to 1 we
are good this noisy process which starts in 0 and then ends at 1 then we could just use any continuous
point in time right we started like normally we started at 0 and at capital T so we can imagine
that we start in 0 and at 1 and instead of just considering this discrete steps we can just
consider any any real numbers there although this sampling is slow it's sampling is slow because
for the inversion process to be able to invert the noisy process the changes need to be small otherwise
modeling the reverse process as a normal distribution wouldn't work well so we will really need to
use something like 1,000 steps to get high quality samples and it's terrible regarding this
this sigma that is being chosen to be either beta T or this beta T if you recall that is the the
noise variance of the ideal inversion process right so yeah what this forward noisy process which
is the variance of beta T and then it's ideal inversion has the variance of beta T till done that's
really nearly beta T so that's the use the like straightforward amount of noise which which we
should add and but we will after we describe the architecture we will describe one improvement and
that will get rid of both of the large number of steps and although the need to choose
what kind of variance we'll be using in the sampling algorithm so regarding the architecture
we will predict the noise using this unit architecture with a pre-activated or resonant blocks
so apart from processing the noise we need to process the kind of time step so real number
from 0 to 1 you normalize it so instead of capital D we just go from 0 to 1 and we will do that by
using the transformer sinusoidal embeddings like I just input the value as a one number
to the network doesn't work well because the neural networks prefer features which are
encoded in whole inner combination so instead of just producing one number with T it's kind of
better to generate a whole vector and then it that will contain the signs and cosines of of this T
at various frequencies and we can edit in various places so here for example or in the DDPB
DDPM they do it by adding it in the middle of the every residual block so after the first
convolution we will be adding it and by adding I really mean taking this embedding passing it through
them's linear layer and then summing or adding it after after the convolution layer in that
in the residual block. Furthermore to maybe architecture's stronger we allow the model to
consider also the larger context apart from just convolutional methods and we will do that by adding
the self-attention blocks probably because normally the convolutional they only only consider
this local neighborhoods so what we will do is especially in the low resolution part of the architecture
we will allow every current feature or every current feature vector of the image to attend
to possibly all other feature vectors in the input image they are not in the input interval in the
current in the current feature vector so what you will do is you will generalize the self-attention
block to two-dimensional images normally we apply it only on sequences so here we will take the
two-dimensional feature features vectors and in order to do the attention while we will just compute
again the queries keys and the values instead of a linear well we will still do it by applying
a linear layer on each of these feature maps which corresponds to running one time one convolution
on all these feature maps we will combine all pairs of queries and keys so all pairs of these feature
vectors multiplied into their applying softmax getting the attention and then multiplying
it together so it's really straightforward self-attention and the only difference is that the
elements are not one dimension number they come from like a two-dimensional collection of features
so we get an output where every output every like position in the output could attend to any
position on the input representation so it could look for example like this overall
we do convolution and some downscaling blocks and upscaling blocks and finally some output layer
the individual individual blocks will be for example pre-activated
resident like blocks we consider these group normalization because the batch sizes are not very large
so we're seeing group normalization works better so group term activation convolution
they even include the one-time one convolution on the residual connection because they say that the
results are better and then the downscaling blocks it performs this this convolution
and then some number of these residual blocks and we might include the conditional embedding
so the time can time maybe some some text embeddings and we will discuss it on the next slide
how to how to deal with it but if we have some additional conditions like for example a prompt
for generating the image then once upon a time we can use a self-attention to attend
into the tokens of that and upscaling blocks well it gets the skip connection from the downscaling
blocks again we might introduce the embeddings of the conditioning which we want to use
add some number of residual blocks maybe adding the self-attention and and then we'll just
iterate it many times there can be of course many ways how it could look like this is concretely
from the image and architecture which was at some point the state of the architecture
for generating images using diffusion based models so conditions in many cases we want them
not to be conditional so that it can generate classes of images or maybe you want to be
able to give a prompt and and get the resultic image so to do that we might consider a
various ways how on what we could condition on for example an image so we could condition on
a very lower resolution image if you want to do super resolution so to do this conditioning
course we could do it is usually done is we upscaled the image and just concatenated with the input
noise right so when we do the the prediction we just don't get the noise image but we also get
low resolution conditioning upscaled to match the resolution of the output image if you want to
condition on a prompt on text then the usual way is to encode the text with some pre-training
coders so pre-trained transformer which will get you embeddings of the tokens in the input prompt
and then do an image text attention layer right so we are when doing this self-attention we can
also allow each feature vector so each position in the in the current image can be allowed to attend
to any token or to all tokens of the of the prompt using the attention where the queries come
from the image and the keys sorry yeah and the keys and the values come come from the text
and usually the encoder is already pre-trained at the point and do not point in
you anyway any more also in some cases the the model tends to or might want to ignore the
the conditioning like if you want to say I want to generate and then you say something specific like a
moon sorry I want to generate a tiger with a rhinoceros on its back
brightening on roller skates and so the more and more crazy your prompt will be the
model might choose to ignore what you want to generate something which looks like an image so
to make the effect of the prompt more pronounced we could do something which is called
classified for guidance so we might want to make the effect of the prompt from stronger using a
single simple trick so what we will do is we will train the noise prediction to be conditional
but sometimes we will train it without the other conditioning so we'll have a model which can
predict like a general image using the current noise image and then the one which follows some given
prompt and then when we do the sampling we will try to make the effect of the prompt more
pronounced by well looking at the difference of what would happen if we look at the noise predicted
including the conditioning we prompt and without it right so the idea is that without it this is
like the general image which would result from the current noise version and including the
conditioning this like a specific version design for the specific label and so the difference
between these two representations might be just in the direction of of the prompt itself like
the difference of what the prompt actually changes on the input being generated and then we can make it
more pronounced by scaling it up with some CUTEBEL, a CUTEBEL wait to make this this work
correctly we also need to add the original or original noise there and in the models like in
stable diffusion this this weight of how how how how much this this effect should be pronounced
is actually quite large values like some 2.5 root 0.5s are actually being used so the last topic
which we'll cover today is to show how we can make the sampling faster and that's was proposed
in the day noise diffusion implicit models and how we will do it is we will describe a different
forward process so we will change the way how we look at the noiseing process and we will want
to allow for faster sampling but we will do it in a way that the overall noiseing or the effect
of this forward process should still be the same right so it's whatever how we how we look at it
whatever how we look at it it should still be the case that after these steps of the forward process
the distribution of the noise images will look like this so it will be the original image
scale down what is useable signal ratio and there will be a noise introduced with the suitable
noise ratio so we can use the same loss we can use the same models we will just it will just allow
us to come up with a better algorithm or efficient algorithm for sampling. I know that we describe
all my very special case obviously the I am in the slides but it's the one being actually used so
we are not actually losing anything so that's the this thing which is specific is this zero here
so I will not explain what it means we will just just describe this version if you are interested
you can look in the video so we will describe the or view the forward process in this way
originally the noiseings started like zero right and then we gradually noise until we
write xt and now we'll do it differently now we imagine that we will first even in the noiseing process
generate the full noise image xt and then we will describe how we could also generate
the gradually noise images but we will still do it so in the way that the xt given x0 as the
suitable distribution so how this noiseing will look like well first to get the fully noise
image xt we will just take x0 and we will really scale down with this alpha t bars with this
final ratio and we will introduce the right amount of noise so we will really use the the margin
of the distribution which we wanted and then what we will do is when we have this xt and x0
then we will generate this xt minus 1 but the interesting thing is we will do it in a deterministic way
and how we will do it is well when we have x0 and xt we have the noise which generate today so
we will just scale the noise down suitably right so we will just directly consider this
direction and we will go slightly there so that we end up at the correctly noise image so
in order to do that the mean of the image will be the combination of x0 and then the noise
which generated actually xt from x0 and we can write it down this noise by well
using the fact that xt is of t bar x0 plus 1 minus of t bar square root of that of the noise
and now when we do it like that then the marginals will hold well that's to be expected
because that will be constructed so by induction we can prove it at the beginning it's true by
construction and then when it holds for a times t when we look at xt minus 1 then we have defined it
as the suitable combination of x0 and the noise and so this xt at that point by induction we
know that we can write it as x0 and the suitable noise and when we do it then everything will
will cancel and we will just get that the output is actually what we think it will be so
the combination of xx0 and the noise which generated xt so this will allow us also with a
sampling algorithm so we can sample xt minus 1 by following this process right so by starting with
x0 and adding suitable noise and this x0 how we get it is well we start with the xt and try to
do noise it completely into x0 which can be done by subtracting the suitable amount of noise and
then scaling right so the process does the following we are in some noise image xt we
guess what the original x0 would look like and then we will just go there to see suitable distance
and the good thing is that when we look at it like that not that we are like just generating
a distribution of slightly denoist images but we can really like see the full path to the
denoist image and so what we can do is we can do the step of any length it's not like
that when we estimate what the x0 looks like from xt that we need to do just one step corresponding
to xt minus 1 we can actually using this formulation do larger step which corresponds to the
inversion of several steps right so we don't need to go just to the next times them but we can
do a larger large jump and when we do it then well we just use a suitable signal to noise
ratio when we generate a new sample right so that allows us to use the multi step sampling algorithm
which will work by considering not just all steps from from one to capital D but just only the
subset of them and in every step we will do possible multi step in the reverse process to
like perform or invert some number of the forward steps so we will start the initial noise
and then in every step we will consider the current the current time step to estimate estimated noise
we will use the current time step to take this estimated noise and estimate how the x0 would look like
right so we have got this xt i x0 and then we decide how far we are going to go
on this from from xt i to x0 so that this amount would correspond to the times that of where we want
to sample next right so there is xt minus t i minus 1 in these two places
yes so this is just written there what what I just said and we can have a look at the sample so here
you can see the sampling so this out of the samples when we do 10 samples so this top line corresponds
to what we just described the DDIM this line here corresponds to the original DDPM so you can see
the samples after 10 steps are blurry and not very good while the ones generated by the DIMR
definitely better for 100 steps both of these look reasonably or reasonable quality even if
here it's still well not really so after 100 steps better like in a sense that these samples
are definitely better for the original DDPM still very nice like this is not obvious what it
is and this is very blurry and even with these faces they don't really look very well while
after 100 steps using the DDPIM that includes sampling it is quite a reasonable and even
10 steps are quite fine now when we do the larger resolution you can see that sampling 10 steps
for the DUDDIM still not perfect so the results are more blurry and then one would like but
if we do 100 samples then the quality of the samples is kind of nice so usually one goes or one
uses something like 50 samples when using this DDPIM so we can have a look at the samples which
you will be able to obtain using this implementation of the DDPIM sampling yourself hopefully
so here you can see various random generated samples and in this left column you can see the
reverse process this is actually how the image generates this image when generating using 50 steps
we will also try in a conditional model so here you can see what our model will do when
it should generate an output conditioned on this input image so when we you get this weird
violent blob then the model will generate a flower which corresponds or whose lower resolution
should actually be be that one so it's specifically interesting for for the faces so if you
imagine the model then model gets this lower resolution image these are three different
random samples on the model this is the true image the real image which was used to generate this
and these predictions so when you see them then these these up images were not part of the
train set of course right so this is how the model like generalizes and it's actually kind of
surprising how similar these predictions are to the true values even if you could imagine that
with this lower resolution version there could be many possibilities but the network kind of
quite well describes the output but it's definitely not perfect like here the guy with a green
hair and some kind of a frog in his hair in his reconstruction and for example here
whether it's a microphone and a hand and then the model struggles in deciding what this thing should
it truly be so generates there is objects but none of them are really really the perfect
well anyway that's all I have prepared for the today's lecture so thank you very much for your
attention and I'm looking forward to seeing you on on the practical tomorrow or maybe on the last
lecture on Tuesday have a nice week
