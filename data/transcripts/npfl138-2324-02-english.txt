Hello, good afternoon, everybody. I'm enthusiastic. I can welcome you on the second lecture
of our deep learning course. This time, dedicated to actually training the new
or networks. But before, I will start this review thing by knowledge.
However, any questions or comments or something would like to make us, or something,
a new timer.
Well, then it's my turn, but of course, in any moment, please don't hesitate
and try to ask if you have any questions or something would not be clear.
So the goal of the today's lecture will be to come up with an algorithm
to start training a neural network. We will not cover everything needed,
so some additional parts, regularization will be moved to the next lecture. But at the
end, you should actually be able to train a neural network and it will be one of the
assignments to actually do it. So we discussed how or describe neural networks briefly
on the previous lecture. So from our point of view, the neural networks are computational
model, just like a formalism, which when it gets an input produces an output, the input
is usually vector of values, like several real numbers. But in the future, they cannot
be a vector or a tensor or something higher dimensional, like the neural networks process images,
for example, so then the input is like a three-dimensional matrix, like height, width,
and a number of channels, for example. And the output are distributions,
most commonly the one corresponding to either the classification task, or we want to choose one
of the k classes, in which case the output is either a Bernoulli distribution or categorical distribution,
or when we are doing the regression, we will see today that we can interpret the model output
as normal distribution and these are all three, which we discussed on the last lecture,
what are going to happen. And the model works by drawing an image of the network, which contains
nodes, and the nodes are connected with an 8-cyclic graph, because it's not obvious how we
would evaluate if there would be a cycle. And the edges have weights. So when we want to compute
the value of the node, we sum up all the input values, the input values are the value of the
predecessor node multiplied by the weight on the edges. We sum them up and then pass them
through some activation function, every node can have an activation function.
Usually we do not speak about the individual nodes too much, because there are too many of them,
and usually the pattern how they are composed is somehow regular, to be usually talked about layers
of neural networks, but that just means like group of neurons which we designate with some
common name. So machine learning, like when we want to train on some data set in the supervised
settings, which means we have got input data and also the world outputs, we will usually proceed
by having a training set. So just a name for a data set containing input and output, which we
will use to train on. And we usually assume that these examples come from something which we will
call a data generating distribution. So that's like a magical process, which gives us the data.
You can imagine it as, I don't know, being like specific, I don't know, place or a building,
and you can come up there with a truck, and then you say, yeah, I want to get 200 more examples.
So these examples fall down from this magical data generating distribution. We have got more data.
Usually you don't exactly have it, or you cannot have an explicit description, although data
generating distribution is, it's just like images for classifying cats and dogs, like it's
something which gives you the data for the task which you expect. But you do not explicitly have
it in a sense that you do not have all possible images, which would exist in the domain,
but usually you can sample from it in a sense that you can go to some social media site and download
1,000 images with either cat or dog in the label, then you have generated some kind of
data. But we will need, like, formal way of saying these data belong to these tasks,
so we will say these data are generated by this data generating distribution, some kind of
magical process, which does this. And so when we will talk about machine learning,
it's very tightly connected to optimization. So the goal of optimization is to come up with a
model, which will minimize the error on the training set, so it means we will try to match
the training set as much as we can. We can imagine if you have got a number of points, if you
want to fit a curve, which goes through them in the best play possible, that will be optimization.
But that's not exactly the goal of machine learning. The goal of machine learning is not just
to match the training set or learn it or something, but to come up with a model, which will
then be usable on some new unseen data, something which we have not encountered during training,
but something similar. And by similar what we mean is that we assume that the data comes from the
same data generating distribution, so the data for the same task, but still still unseen.
To measure our ability to process unseen examples, we will usually have training set, sorry,
a test set, we will have a part from the training set, we will also have a test set.
And we will use it only to evaluate our model on unseen data. And the test error or generalization error
is how well we do not have the training data back on these kinds of a test data. And these
two things like this thing which optimization and machine learning, so while machine learning
uses optimization to minimize the training error, that's not our ultimate goal, our ultimate goal
is to have a model that the user can throw the data on from the data generating distribution.
And then hopefully the model will do something reasonable. Now, there are many ways how this
training can happen. And in some sense, there exists no perfect training algorithm
formalized by the no pre-language theorem, which says that if you would consider all data generating
distribution, so like all tasks with all possible data in them, then on average across all these
huge set of data generating distribution. So when you would consider all possible tasks,
you can think about then no algorithm does better than the other. So on average, if some algorithm
works for classifying photos, it will very badly for distinguishing weird noises from each other.
So this huge average is the same, even if your algorithm for classification would be always
the turn zero, or your algorithm would be always return the least probable output,
then they do the same. But this is some kind of a fatalistic view where you would consider all
possible tasks. But if you consider all of them, then well, there is nothing interesting in that.
So in practice, we want one of those, which work on some specific reasonable world level data.
So it still makes sense to talk about which algorithm works better in practice.
But sometimes you see it is no free lunch with theorem mentioned somewhere. So with good idea
to have an idea of what it actually means, but I would say that it is a little practical consequence for us.
So when we will be learning something from the data, we will, there will be two bad cases,
which we will try to avoid. Yes, a question.
Yeah, so the question is how the machine learning is different from the optimization,
in sense that they both generate a function, try to come up with a function which grows from
the input to the output. So the difference is that the evaluation or we want the machine learning
function to work well on some unseen data, something which we did not have in the trainings.
For the optimization, we just want to take all the data which we have and for them, we want to
get the exact output and that's it, just for the data which we have seen during training.
For machine learning, we don't really care much about what the model does on the technique
example, but we wanted to perform well on some unseen data, which the model has not
an access to during training, which nevertheless come from the same data generating these
distributions, so we have a chance of doing it. So it's just the way how we evaluate the model,
we're in the optimization, the training error is everything we need, while machine learning,
the best set error is what we are aiming to minimize.
Yeah, yeah, I would definitely agree. So the goal is to minimize some error in both cases,
but the optimization is the training error which we aim to minimize while in the machine learning
is the testing error which we aim to minimize, but we don't have the testing data during the training,
so it makes the task a bit harder because we cannot directly optimize the testing error because
it should be unseen data so we cannot use it during optimization. So we kind of wanted to
work for all of the data from the data generating distribution and just during the evaluation,
we just randomly sample some of them in the form of the test set.
Yeah, so the question is what kind of data comes from the data generating distribution,
so it depends on the task. Right now we are going to do the supervised learning for a lot,
so for us, the rate of generating distribution, all the generated labels. So generates pairs,
inputs, and outputs, right? So if you would go download images from the social media,
then the rate of the time will come to involve the images and then maybe the labels,
mentioning either cats and dogs, and they would say it's a cat, if a cat is mentioned,
and it's a dog, if a dog is mentioned, for example. Yeah, it's a journalist division,
generates pairs, inputs, and outputs. Good job, everybody. So there will be two bad situations,
which we will try to avoid during your machine learning. The first one is we will call it
underfitting, right? So in underfitting, the optimization process didn't work out well,
so even our training error will be, will be too large. So you can imagine that we'll have
a model about the model is too weak in order to even learn the training data, right? This is
also a case of problem in optimization where, well, we just couldn't train the model,
and usually that's because it's too weak. So we could say that it's capacity, it's not too large,
so there is a, not the illustrative example, or if we would try to learn a curve from the given
set of points, and we would consider only straight lines, then whatever we do, we just cannot
learn a good straight line, which would match all these points. So in the sense, we are underfitting,
because our model is too weak in a sense that what it can model is just not rich enough to be able
to actually describe the data. But in machine learning, we will also encounter another problem,
which is would not be the case in the optimization, and that's the overfitting. So overfitting
is a little more tricky to explain, but overfitting is a situation where our training error is
small, so our model is strong enough to describe the training data reasonably well. But our performance
on unseen data suffers a lot, so there is a big difference in what we can do on the training
data, and how well we do on the unseen data. So here an example of that would be, if we say,
okay, so let's consider polynomial so far as degree. So if we have some number of points,
we can measure them perfectly, right? If there are, I don't know, 10 points there,
and we consider a polynomial of degree nine, there exists exactly one polynomial,
which goes exactly through those points. And the polynomial can, for example, look like this,
that it does so we are thinking here when it goes down. So if we then use this to predict things
for the unseen data, right? So I don't know, for these points, then the prediction of the model
for these exercises is very bad because it will say something tragically negative for them.
While it is much more reasonable to expect that the values to be somehow in between
these training points, right? And these things can happen, we will discuss sit on several lectures,
how it happens, but intuitively what can happen is that the model can come up with some
some rules, which apply perfectly to the training data, but they don't transfer to any other input.
So consider tasks where I get an image of a cat or dog, and I should say cat or dog.
If the model is strong enough, what it could do is, for example, it could memorize the first line
of every image. And then what the model could do when it's given an image, it could look at the
top line, find out which of these training examples is this, and then answer on that, right?
If it's not, okay, it's the 372nd example, then that was always a dog in the training
and get us so I will say I was a dog. And now when new and scene data comes,
then this strategy will fail totally, right? Because we'll look at the first line,
we will try to find the closest image in the training data with the play. Well, first there is
no image, which probably has this line exactly, but even if we find the most similar image,
there is no guarantee that there will be the dog there as well, because well, we just look at
the first line and nothing more. On the other hand, for the optimization process this works
well, because well, we achieve perfect performance, we can achieve perfect performance on the training data.
And the way how to balance balance those things is what we will try to do, like we will try to
come up with model, which is strong enough so that it can describe the training data well,
but the rules, which it comes on, comes with, not just for the training set, but also for some
other unseen examples, and we will say in this context that they generalize, or also to some other
other inputs. So another, another example, I imagine that we are doing the classification, we have
the blue points and the red points, and if we would want to find a straight line to distinguish
the blue and red points, the model would underfit, because well, no straight line will do that nicely.
On the other hand, if we are very detailed and we will we would really learn how some of these
points are randomly like above or below or on the two different sides of the black line,
then that would probably not hurt well, because if we learn from the training data that there is
this weird part where there are a lot of red points, it could happen that there are also
vital arts number of blue ones, but because we concentrate it on this one specific situation in
the training data too large, it will not transfer nicely to predicting unseen examples. And one
way, how we can improve the underfitting or overfitting is by changing the model capacity. By
capacity, I mean just general quality of how expressive functions can a model learn. So if the model
is very weak and it is underfitting, we can make it stronger and stronger and stronger, and that
will should make the underfitting go away. So if we would have a graph where here we would talk about
capacity, then if the model is very weak, the training error the blue one will be very large and
then it will gradually decrease and it will decrease more and more assumingly or it should when
we make the model stronger and stronger. On the other hand, the generalization performance,
how good the model does on the unseen data, usually show this kind of behavior that from some
point onward, the performance on the best data can naturally decrease because the rules which the
model is coming up with cannot be applied on other data than the one in the train set.
So this is something which we will be seeing quite a lot. It's sometimes called the U shaped curve
for an obvious reason because it looks like U, right? And if it does, then what we will try to do is
we will try to come up with a sweet spot, right? The model which is strong enough but not so strong enough
that the generated rules would not apply to the unseen data. In our classified example,
cats and dogs, if we say the model or let's say that we want to distinguish bananas and cucumbers
on the photos, right? And we say the model you can create five rules and nothing more,
then what it will do is it will probably say if there is a weird yellow shape in the image
that is banana and if it is a weird green blob, say say cucumber. And the more and more capacity
we give to it, the more and more it can learn the individual examples, the individual exceptions
in the trading data to the point where it might stop looking at the yellow part and the green part
and then instead try to concentrate on things in the background or something which is not
either related to the underlying concept but still works on the training data. But if one of the
possibilities are to deal with the problem is to limit the capacity of the model so it cannot
either learn all training examples. Instead it needs to come up with rules which will describe
whole groups of examples from the train set and hopefully such rules will be able to transfer
to also unseen examples. Yeah so the question is in how do we quantify the capacity so we don't
right now it's like an abstract thing and I want to keep it that way for a bit because it's difficult
to quantify it exactly because the capacity has a lot of factors which which influence the
ability of the model to actually learn or remember something so put a time being it's still
some abstract concept that I'm talking about rules but the in fact the model learns just ways so
like it's kind of more abstract than then actually concrete but I don't know of anybody who would
actually do it there are some definitions at the end of the slides which talk about the capacity
but they are all abstract in a sense of how many training examples can the model actually learn
so that it works somehow. Yeah yeah in this example we assume that the training set is fixed and
that's a good idea that would be remarked because one thing how to avoid overfitting is to get
more data at the problem with overfitting is that you have or like one way what you can do is if you
have more and more data then usually the overfitting goes away so that is an example here that we have
15 data points the green are the underlying data but when we get the example there is some noise
so the trained model doesn't really follow the underlying green truth but it's influenced by
the noise in the example like that's the overfitting part that we are including the specific
of the individual examples into the trained model but if we had more data then all these specific
of the individual examples would somehow cancel out so in some sense having more data makes the
capacity of the model actually decrease in a sense that if you increase the size of the data you have
then the model can remember less and less of these examples so this is like a different view of saying
we have like decreased the capacity of the model by throwing much more data at it but usually this
is not what we can do at usually generating data cost money effort or something so our goal will
be to make the best of the data which we have but of course if you can throw more data to
model that universally helps is just a question of off money entry sources but we will not
bring the talk about it much on the lecture because well that's something you can always do but we
will try to learn how to like really use whatever data we have and in order to do this to improve
the generalization error we will come up with several approaches they are specific to machine learning
because optimization doesn't care and any approach method algorithm which decreases the generalization
error so which makes the model to work better on an example we will call it regularization
the regularization will often mess up the training error so usually increase it but we don't
really care whether we cannot describe the train set perfectly we only care about the generalization
performance by the way decreasing capacity also usually decreases the training error sorry increases
the training error right so it makes the model not to work as well on the training set while it can
decrease the generalization error so if we are I don't know in this situation and you will decrease
the capacity then the training error will actually increase but it can help us with the generalization
error so from some point of view setting a good capacity would be one of the generalization methods
we will not talk about the regularization methods on the today's lecture we will have a block
about regularization in next week so so to just give you a slide example of what we could do
is the L2 regularization that's a very old but still sometimes used for method of regularization
models and that's okay okay there are many models we actually prefer the ones which have smaller
weights right so during training you can say yeah training model but please please please make sure
that the weights are not very large now there have been many thoughts which try to explain why
this might be with the idea or not while smaller weights should generalize better but one way
how you can look at it is that if we have a model right which has got some input and I don't
want to say a single output then the if you change the input slightly then the model will
although change the output but the size of the change of the output is somehow bounded by the size
of the weights which there are in them right if the weights in the models are thousands then
the small change should probably generate thousands times the larger change on the output
if the weights are far smaller then the change on the input will generate just a small change
on the output I really just hand waving it to give you an idea so what this means is that if the
models would be smaller then the the model would behave more smoothly contrary to generating
really large changes longer and usually this chaotic changes in the output are something which
is bad for generalization you probably want the output to be kind of smooth in a sense that they
don't change too much when the input is changed I'm not saying that it's always true but it's
one of the ideas of why this might be a great idea but we will do the math around it about
it or something on the next lecture is just sort of an idea of what we could mean by regularization
so now let's say that we want to trade so we will have a training set and afterwards we will
evaluate the model on the test now during training the algorithms will change something for example
the weights and the biases in our neural network and we will call all these all these
values fitted with by the training algorithm as parameters right so the training algorithm
has a goal of producing best parameters so that the model works well but there will be some choices
which are not actually parameters there will be some additional hyper parameters which we
need to set and these won't be setable or trainable by by the training algorithm themselves but
we need to use them in order just to start so such a hyper parameter could be what new on a
third car we're going to use what what training algorithm are we going to use we need to set these
things in advance in order to be even able to to start the training process so the machine learning
itself will not solve all our problems it will help us learn something but some things still need
to be predetermined before the training itself it's started so all these additional choices
which need to be made by us will be called hyper parameters to distinguish from the trainable
parameters and the question is how we are going to set it like what new network we should use
how we will optimize these things need to be set in advance so the easiest and very commonly used
approach is well just use some kind of a brute force to try several hyper parameters
and then choosing the set of hyper parameters which work the best right we'll try three different
neural neural networks and the one which is the best well the two of you the one which will use
the question is how to evaluate that it's the best we could use the test set to do the evaluation
but then once we have used the test set for choosing the best model we have actually used it to
do training so we cannot then evaluate our generalization performance by again looking at the
test set because we already tried to minimize that error so for that reason we will need another
third data split or part portion of the data and this is called either development
sorry the validation set so formally we also use it during training but not for running the
machine learning algorithm itself but for evaluating the models for evaluating the generalization
performance so that we can choose for example among multiple models and then the test set we will
use it only at the end once we have selected the model to see how well we did so we will be doing
some competition for example and we will follow this this split but you will you will get the
train set and the diff set with the labels and you also get the test set but without the annotations
right so that you won't be actually able to train on the test set you'll be just able to do the
predictions and then give it to me and it will be evaluated somehow in record X right so that's why
you cannot actually use the test set during training and this is usual setting if you want to train
a model which you want to deploy in produce in your practice right because the test set the
ideas that are the users were going to use whatever you have come up with afterwards you have finished
right so so that's why you don't never you never can use the test set during training yes
what the names are there written as well so now we have an idea about the machine learning algorithm
itself will do we will try to do the optimization so we will try at the beginning to minimize
the error on the training examples so we will try to learn as much as we can about the training
examples and we will measure the error on the training example using some function which we will
call a loss a loss for example could be a mean squared error loss you may have seen it already
so we assume that we have a data set which has a vector of values on input it's volt and one
a real value on output we are doing regression and then we have a model we will write it like that so the
model is called as you can imagine a neural network the x are all the inputs which the model uses it
produces why on the output and then it has because these parameters this weights and biases
which we will denote as theta because that's how it's done in most papers so the loss could be
the mean square error and the mean square error is real just a good description so what we will
do is for each example we will compute the prediction of our model for the example compute the square
distance to the real real value which we should have predicted and then compute the average
or the mean of those our goal be for this to be zero because then the prediction of our model
would exactly be the training labels which is the best what we can hope for during training itself
although this does not necessarily mean that the generalization performance will work with kind
of tends to work and then we will discuss on many courses how to make sure that even the generalization
performance works well so I've written here one loss because I assume that we have probably
seen it even on the algebra courses people are doing this minimum square
regression or something or the method of the least squares and things like that but when looking
at the loss I was always wondering like why is the two the error why isn't it three why isn't
the error an absolute value or like why does it look like that right well luckily we will not
really construct the losses by you know me giving them to you or like like reading a paper and
then finding a great loss which somebody had an idea to come up with 20 years ago instead
we will actually describe how the loss like a suitable loss can be generated so mostly the
losses which you will use will actually be pre determined by the data which we have specifically
by the output distribution which the model generates because in some sense there is a best loss
which we can use for for every output distribution and we will describe how these losses can be
constructed and the the way or the method which can do it is called maximum likelihood principle
or maximum likelihood estimation right estimation because it will come up with the parameters
during training so how does the maximum likelihood estimation works
assume that we have got a training set for the time being I will not bother with labels so this will
be just in port it's weird but don't worry I will add the labels in a few slides there but for
the time being let's just say that the model gets the data which are just the inputs I'm supervised
learning let's say that these data are generated independently from the data generating distribution
like we don't have the explicit description description but we have generated a set
so if we don't have the data generating distribution we can actually come up with an approximation
of that which we will call the empirical data generating distribution it speed data with a head
and that just describes the training data right even if it seems menacingly it's just way
how to describe how the training data looks like so the data generating distribution will give
a probability of one over n to every training example so it will be like a discrete support
distribution if some example is like repeated multiple types in the train set of course
to get two over n three over n for something and this is actually like an approximation of the
real distribution but it's another way of how we can look at our trains and let's say that we have
like a neural network but the neural network can compute multiple outputs depending on the
weight so formally we all said that we have a family of models which differ when we change the
parameters if we change the weights the model would compute something different so we can consider
of having like an usually infinite set of our possible models by specifying the parameters we choose
one of the models to use and do the prediction and here we will assume that the the model
actually generates a distribution right so right now for each data it will say how much they
it actually likes it in the supervised learning when we have playblows it will say how much it likes
label for a given for a given input now when we have this this family of distributions it has
two inputs like XS and the parameters so if you imagine that the parameters are fixed I like you
kind of make them go away then this is a probability distribution right because we have already fixed
the parameters of a generated distribution but we can also look at it the other way around we can
try to ignore the inputs and adjust consider a function of the parameters and the the function
of the parameters what it would do is that it would compute the probability of the whole training
set right so we will ignore the XS the inputs by using the train set there so we can look
of what the model would think about the probability of the training set well because the examples
are independent we can compute the probability of the individual examples by multiplying the
probability of the individual training exam and this function in some sense it's a probability
because it's a number between 0 and 1 but it's not really a probability because probability
comes from probability distribution but this function L is not a distribution it's not like if
you try all possible theta it would sum or integrate to one or something so because it's not
really a probability it's not called probability it's called likelihood right so the likelihood tells
you how probable the the training data are for a given set of parameters yep and so the maximum
likelihood estimation says that if we want to come up with good parameters we yes a question
now right now the condition probability will appear later so so right now it's not but but
if you want you can imagine you know that this is a conditional probability it can be formulated
also in this setting that the model just says how much it likes the image let's say that we are
training a model which should recognize you know some some arises of photo so it gets
an image it says how much or how much this image is actually a sunrise and you could compute
this over very large number number of images so it's a weird setting but usually it's formalized
in this way and we will add the conditional probabilities in in three slides there
yeah so so the the P model that's like our neural network right but what it does is that the
parameters are kind of fixed so we assume that if we have the parameters the model can give us
the probability of the training example so it doesn't talk about the parameters the parameters are
there as something which we use to compute the probability right so we can image in a neural network
which gets an image here and then produce a probability on the output and the fade us are hidden
somewhere here as the weights and biases and some constants which we use when we compute the
final output so the model completes the probability of the training example but not of the
theta they need to be given in order for the model to be able to actually compute something
but for a different parameters the model might either like the train set or not like the train set
yeah so so the question is like from this definition it seems that the likelihood is the probability
in a sense that it's just a multiplication of several probabilities so a wife not the probability
in a sense it is a probability because the generated number is between 0 and 1 on the other hand
when you talk about probability usually it's like you have got some said of possible outcomes like
sayx if you are rolling dice and then like the probability is one of these outcomes coming true
and in that case you have the the whole distribution and you know that if you would consider
all the possible outcomes there the some of the probabilities would be one but that's one of the
reasons which we just didn't work here right it's like if you would consider it probability
it would need to be the probability of the parameters out of the space of all possible
parameters or something and that's that's not the case because if you just consider all possible
theta's they don't do a distribution altogether so because the L is not really a probability
distribution we don't even call it the probability and we call it a likelihood but you are right
that the one of the results themselves like this one number is a probability but it's not the
probability of the parameters is the probability of the train set given the parameters so so that's why
we have a different name for it yes yes at this point when we compute the likelihood of
the training that is fixed and during the training it will be the case all the time here we're
working yeah so that's one of the observations how how how one of the ways how you can
consider it is you can consider that our model computes the general distribution
overall possible possible data so either you can consider the externally input generating
probability or you can imagine a neural network which generates the distribution overall
possible Xs and then if you want to find out what the probability of X1 is you will just find
place there and it will tell you how how probable the X1 the model think it is I agree that
like if we will go slightly forward we will get to the situation where we will really model
the condition not it also works there as well and usually that was formulated at times of the
things haven't proven so so I'm going the usual way but I agree that it can be my surprising
who are where the labels are so in the end we will end up with the situation where our model
will generate the conditional probability of some labels given given the Xs right so here we have
logarithm so don't care about logarithm yet but we'll have the the probability of the model
which it assigns to some label Y given the input X nevertheless I will still go back yes in some sense
yeah I think about it as a function of our neural network like the the P model is the neural network
right it's in my head it still gets an X and it produces it's it's probability
so in the conditional case it will get an X and produce a distribution overall the labels
in this description in my head it gets an X and just produces one number which is the probability
of that one specific X so the whole goal of this setting is to say okay let's prefer the parameters
where the model will assign the largest probability to the whole training set
right to the idea okay so this is the the model maximizing likelihood so it's the one which
says that these data are the best in a sense that this model can say okay I like this train set
as much as I can so let's have a look where this will lead us so we will say okay so
these set of parameters generated or predicted described by the maximum likelihood estimation
I said that we want such parameters which will maximize the probability of the model to
maximize likelihood like these maximize the likelihood and once we have it we can say that
we are maximizing the probability of of the training set this arg max what it means is that okay we
want to find a maximum of this but not really the value of that but we want to find the argument
theta parameter of that which will make the value to be the largest right so that's exactly what
I said we want to find parameters which maximize this quantity and we can write this quantity again
as a multiplication or of the individual probabilities of the training sorry examples
which are independent so this is fine so now we will simplify this a bit one thing which we want
to get trade is the the the product is difficult to work with product so we are in a
situation where we have some function right and we want to find theta which maximize the function
in this case what we can do is we can easily apply some like increasing mapping to these values right
if we are maximizing this I don't know fx we can also maximize any transformation which is monotone
we can maximize fx crap five for example like the values will change but the the place where
the maximum is will not really change because still the largest value will be the largest
even if the different value and what we can also do is we can maximize also the logarithm
of fx right because logarithm is a monotonically increasing mapping the the values which we have
there are larger than zero hopefully so this will change the the functions somehow but still
the place where the maximum value is pain is still the same right because the maximum value is
still maximum after logarithm even even if the value is different now we can do this with any
function it doesn't in a change of the output so this is not a choice which would influence the
result it's really just for for our comfort so imagine that we do it so we add log here
and this logarithm will change this this product into some and another thing which we can do
is that right now the the values which we have are negative right because well the logarithm
goes from minus infinity to zero on the range of the values which we have so what we will do is
we will flip the values over right so that the values which we have here are our positive
or at least not negative but then instead of searching for a maximum we will search for a minimum
it's fine if you change the signs then maximum becomes a minimum and vice versa so this is just
transformation which I could have come up with many more transformations because they don't change the
arg minimum more maximum it of course changed the value of the minimum is the maximum but not the
place where it's a cheap now when we look at it it starts to look especially close to something which
we talked about on the previous lecture and that was entropy so let's let's make it into entropy
so first here we have a sum over all the training examples we have the x y and x 2 and so on so
let's instead of going the individual examples from the train set let's go from all the unique
elements of the empirical data distribution they are the same data right the only difference is that
if one example was three times in the train set it will be just like once the output of the data
generating distribution but if you look at the probability of three over n so you will still compute
it three times it just here the same examples really get iterated by the sum while this expectation
like if I would run it I would say it's over x from the P data generating distribution so these are
unique x's but because we have also the probability here right and this
include the number of times that specific example was in the training date so yeah so if I would have
written times n then the values would be completely the same of the above and lower part it's just
a different way of writing it and we don't need to have like we can ignore the times n because again
it will not change the place where the maximum is achieved and now when you look at it this
is something which we already know and that's just the cross entropy between
the data generating distribution and the distribution predicted by our model and so we have achieved
this or we have arrived here without saying entropy is great let's do entropy right instead
we said okay we will just come up we will just use the theta which maximizes the likelihood
and that go to us to the case where the quantity which we want to minimize is actually
cross entropy and now we know something about it from the previous lecture but it's not because
we think that entropy is great it was just like consequence of the maximum likelihood estimation so
at this point it would still like to know whether maximum likelihood estimation is a good thing
of attaining the parameters or not but if it is then well cross entropy is just a result of the
which which is there now we also know that the cross entropy can be written as the entropy
plus the KL divergence right tail divergence was defined as the cross entropy minus entropy
we can rearrange and do this and when we look at it the entropy of the data generating distribution
is this thing this is not influenced by the parameters by the theta this is just a number
of the time so again we can ignore it when we compute the the theta because again the
theta which will maximize the quantity will be the same so the minimizing the cross
entropy between the data generating distribution and the model distribution is the same thing
is minimizing the KL divergence and so this gives us closer to the losses which we use and now
we will switch to the condition okay so now everything will be the same just instead of
predicting the distribution over x is we'll be predicting distributions over y's given x right so
the model will get x and generate a distribution over the possible labels for this to work
we would need to define a conditional cross entropy and a conditional entropy it can be done
but that's one of the reasons why it's usually formulated without the y's because then the
entropy makes sense and here you need to define them for the conditional case it's not not very different
and this loss function which we have arrived at is called either negative look likelihood and L L which
is just the description of negative lock likelihood or it's called cross entropy loss or KL divergence loss
but the extent that these names are really names of those functions in the frameworks so we
will train our model by saying we want to use categorical cross entropy loss for example so all of
these math from the previous lecture in today's lecture is just to make you understand to
what are the names of the things which we will be using in the framework to train our neural networks
so right now our goal is to when my goal is to convince you that the maximum likelihood estimation
is a good idea right now it came out of the blue by just let's do it like that it's kind of
into it like we want to find a model which will say that the labels have the largest possible
probability for the given access let's probably we won't like ideally we want the training
model to say 100% to each label in the train set so it is kind of into it but still maybe there
would be other ways right so in order to do that we will need some some more terminology so
let's say that we have got some some random variable and for example I would like to know what
the average height of a student is in the class I could measure it exactly by taking everybody
to the board measuring them and computing the average it would take some time but it's doable
but if I wanted to get a result quicker I could come up with a process which would estimate
the quantity so any such a process can be caught an estimator right so something which will try to
guess some value which we are searching for so imagine in this case that we are searching for
for the average height there so the estimator which I propose is to randomly choose one of you
taking them to the board measuring them and then say okay so this height that's my guess
of whatever height of all the people here is that's an estimator it doesn't work all the time
obviously if I choose somebody very small or very large it will be off so if you have an estimator
the question is how good it is an estimator might contain like a systematic error right if I say
height of everybody here on average zero then that's a bad estimator because well there's a huge
mistake between the prediction and the reality unless you are like infinitely height or something
so when you consider what the process gives you an average when you consider what's the
expected output of the estimator then ideally we would want the average of the estimator at
the expected value to be exactly the quantity which we which we aim for right so I would like
my estimator on average to give the correct answer if this is true then the estimator is
called unbiased because on average it's fine and if it's not the case then the difference between the
expected output of the estimator so if I would repeat the process many times and look at the results
and it would be off from what I want and the difference would be the bias of the estimator
is the same name as in the neural networks we had with the bias for the neural neurons there but
but it's not really connected like in a sense that it's always some kind of offset but
but generally they are kind of independent but we were mostly care about the world unbiased
so the interesting thing is that the the thing which I proposed to get the average height by
measuring a random student is an unbiased estimator because well what is the average output
right well the average output is the average of the height of all the students which I have
taken to the board in all these experiments and well that's by definition is the average height
here right so this makes no systematic error but of course the individual examples can
can vary a lot so generally if we have samples from independent and identically distributed
random variables then estimating mean by computing the average or just like like computing the average
of any number of samples including one sample is an unbiased estimator we will use the thing
but surprisingly it doesn't work like that all in all the times because if I would like to estimate
the variance of the the data points which we had then the natural way how to do it is to
well compute the sample variance like compute x minus the estimated mean I said that this is an
unbiased estimator do the square do the average and the thing is this way of estimating the
other variance is actually a biased estimator right if you have taken some statistics class
then you know that here you need to have n minus 1 for the result to be unbiased if you haven't
heard that don't worry I will not return to it on other slides so just give me 30 seconds to finish
it's not really important I'm just trying to show you that some things are unbiased estimates and the
other which look might look very similar might not be unbiased estimates so the difference between
this and the unbiased estimate is a factor of n over and minus 1 however it's still useful
estimate because the larger the n is if you had more and more samples then the error like the bias
of the estimate would go to 0 so it's still useful in a sense that more and more data means less
systematic error but not really unbiased and so now I need these words the estimator biased
to just say how good Emily is that that's why I need them and to be using them later as well
so in order to describe Emily imagine that we have the model of distribution the family
of distributions and we know that there is one magical set of data so there exists
the parameters which describe exactly the data generating distribution right so we know that there
exists parameters which which will do what we want so it will say the probability of our data
is exactly what we want them to be and so Emily can be considered an estimator right it comes
up with the set of parameters hopefully the ones which exactly describe the data generating
distribution and the randomization there is in choosing the training examples right because if you have
a different train set generated from the data generating distribution the Emily would give you a different
set of parameters so you can consider it to be an estimator and the estimator is consistent
in a sense that even it might be biased the bias will vanish more and more examples we have so it
converges to the to the correct solution if you have enough data so formally you could say okay
so for any error which we consider the chance that the expected difference between the predicted
parameters and the real parameters so this probability that the probability that this is larger
than a given epsilon will converge to zero more and more data we have so it will our networks
or model with converged to the correct solution more and more data we have yes
I mean the number of training examples yeah right because that's that's how it works like if
we have one training example you can never match the underlying data distribution but the idea is to
like perfectly describe the task at hand with more and more data we have now consistency that's
something what probably every training algorithm should have right you should be able to perfectly
describe the whole data generating distribution more and more data we have and the thing is out of
all possible consistent estimators so if you consider all algorithms which will converge
to the to the of the solution more and more data they have then Emily is by some sense most efficient
of them and this some sense is that if you measure the square of the differences between the
prediction of the Emily or whatever estimators sorry and the real set of arguments then this
quantitative like mean square error is the smallest for Emily compared to all other consistent estimators
so whatever rule you came up with a different than Emily in this specific measure it will be
verse so the average error it will make during prediction of the parameters will be on average larger
or the same SML I don't know the proofs they are very old I just believe them and
but for these reasons Emily is like the metat training algorithm of choice and I don't
really know of people who would try to say that they will do it somehow differently so in some
sense or at least in practice like the accepted way of generating loss functions because that's
what it does in the end right the process gives you loss function and the loss function is such that
you will converge to your conclusion and in this setting like in a specific metric you cannot
do any better with a consistent estimator yes the question yes my idea is that my understanding is
they exactly work in this in this very generality it's interesting but but lucky for us
okay so now that we have that I've tried to to present the arguments why Emily is great
let's see what Emily has to say to the means square error right means square error is being
used sometimes so we will show that the means square error is actually a loss also predicted by
the Emily so it's fine to use it and you'll also see where the two comes from so let's say that we
are performing a regression our model gets an input let's say X and it will produce one very wide
in order to use Emily we need the model to generate the whole distribution of values but if the
model just gives you a number it's not really distribution like it's just a number so
where to get this distribution well what we can do is we can assume that the model predicts
the mean of the distribution but it might make some kind of an error right so the real quantity might
be larger or smaller and probably like like but the on average it should be whatever the
model predicted but there there might be an error and we will assume that this error is like the
magnitude of the error is same everywhere so that's what I mean when I say fixed variance
that is the case in many examples like here in this data like it seems that the average error
between the blue and the red points is the same independently on the X so the error is the same
everywhere so in that case we are in a situation that we have a mean of the distribution we have
some some variance of the distribution but that's all we know so what kind of distribution we could
use to describe this situation well if you remember the last lecture we could use the maximum
entropy principle and that's okay okay okay if you have these two conditions mean and variance then
there is one most general distribution to describe this situation and it's exactly the normal distribution
right so we can imagine okay so the the model actually like predict this this the head kind of
function by explicitly just saying where the the the highest point of the head will be and we will
assume some fixed with error right if you have I don't know some kind of measure my
end and you know that that is all this plus minus one one millimeter plus minus one meter if you
are measuring something very large so that's that the situation which we are in and this
allowed us to come up with a distribution so let's say that we have a newer model
actually a newer network which produces just a number and we can define the distribution on
the output by saying it will be a normal distribution with a mean predicted by the model and then
some fixed variance I haven't said what it is any point matter in the end we will see that
it will all kinds of allow so the the specific value does not matter what matters is that is
variant is the same for all the existing the same everywhere so now let's do some math let's plug
this into maximum likelihood estimation with the normal distribution right so we want to compute the
likelihood here so we already know that it's negative likelihood some of the negative logarithms
and now we will expand the P with a density of the normal distribution I don't seem that you
remember it but it's this quantity right so there is some normalization constant and then E to
minus the exponent so here what's happening this thing contains no theta so we can just ignore it
right it's not there now we have logarithms and the E so we can imagine as two nights going
against each other with these weird sharp things and they will just cancel themselves out right so
the only thing which will remain is this minus and then then this exponent also this sum sorry
so that's what we have here right these two minuses well they will conveniently cancel
right so what we get is just a sum where we have this this exponent here from the normal distribution
and now we are nearly done so the sigma like the variance is here but again we don't care about the
specific value of the likelihood we just care about the place where the maximum is achieved and for
that sigma place overall so we can just ignore it we can also ignore it too if we won't we can
edge one over n there in a multiplicative sense and if you look at the result well that is
exactly the mean square error so the mean square error is just a consequence of saying okay
we assume that our model does the same error everywhere and we model the output as the normal
distribution and you can see also where the two comes from right because the two comes directly
from the formula of the normal distribution density which in turn comes either from the central
limit theorem like the people lying in front of the pub or you can see it as the result of the
maximum entropy principle so it actually comes from the definition of what the entropy looks like
so the the goal of this is not to make you to memorize a lot of weird formulas the whole goal
of this is to say that the usual losses between use usually come out of somewhere the mean square
error is a good loss in some sense the best for regression with the one specific assumption
and that's the model does the same error everywhere that might not be the case sometimes the model
might generate the error like relative error like plus minus 20 percent so you might imagine that
the model will do larger errors for larger inputs in these cases this is not the best possible loss
and instead you would instead want to interpret the output of your model to be a distribution
where the variance can increase for example and that in turn would come up with a different formula
but it's the assumptions which is important and now you know the ones for the mean square error
never mind the formulas they are just a tool to the end but they allow us to interpret when
means where error is actually a good loss so now we have a loss this loss comes directly from
the distribution which we want to predict which comes directly from the data we have so we will
not be choosing losses they'll be induced by the data now let's try to try the model so what
you will do we see we'll try to come up with the data with parameters which will minimize the loss
at hand so let's say our loss looks like this we can compute it for a single example
and so sometimes it's called a per example loss like a function which gets one example and it
tells you how good or bad you are doing and our overall loss is just an expectation over the whole
data set like I'm denoting this L and E but I'm not trying to say that there is some difference
between loss and error function or something there are just need two labels one which is for example
what's in the other which is over all the data set so this either you can also in your head think
about it as an expectation of the per example losses over the whole data and so in order to minimize
this we will use the I don't know stupidest way possible of doing that I can imagine that
you are in your garden and you would like to visit the highest place in your neighborhood you fear
for a good hike and you want to go peak kilometers to the largest hill you see so if you have
a map you can see here there is a great hill like three kilometers there you will go there but
if you wouldn't you would adopt this this extremely simple algorithm and that's to be look
under your feet right and then just decide which direction leads uphill maybe this one right so you
could go and this way you could go and then I don't know climb through the mole hill or something
there is small then to get on top and then you would say okay no more way to go even higher so I
finished my trip after doing three steps right because well I cannot really go any higher so this is
actually the algorithm which we will be using and which is still being used to train most
neural networks instead of going up we will go down so the algorithm is called gradient descent
and the idea is that if you have a function and you want to find its minimum then in each step
you will decide what is the direction which leads downwards as much as possible and you will just
do step in the direction so the algorithm it will like to go down here and then it will say oh
I found the minimum so formally what we need to do is we need to find the direction
where the function decreases the most this is one place where we will use calculus and specifically
derivatives right if you have a derivative or partial derivative of the entire neural settings then
we have got this loss function lot of parameters so what a partial derivative does well for one of the
theta 1 right you can image in that you try increasing increasing it and the derivative would say oh
what do you okay if you increase this theta 1 slightly then the loss will increase by the same amount
times the derivative of the derivative tells you how changing the theta 1 will affect it will
have on the final loss so it tells you whether the function increases or decreases and with
which speed if you fill with theta 1 so that's what you get when you compute the
partial derivative of the loss with respect to theta 1 and you can do this for all the theta
which you have which we have right and this whole vector it's called the gradient
mentioned it on the previous lecture is denoted like that is where triangle is the gradient
the theta there in the subscript mean with respect to these arguments it's a vector of length
of the number of parameters and this this vector is actually the direction where the function
increases the most right if one of these quantities is large it means if you change theta 1 the
corresponding theta the loss will increase dramatically and the larger mean that in this direction
it will grow even even like more more strictly or more more more quickly so the direction of the
gradient between the derivative as a vector is really the direction of the high high largest increase
this is not obvious just from the fact that these are partial derivatives needs to be proven
but in some sense it is into it because the derivatives tells you how large a function increases
if you would go in the direction so it makes sense but some I don't know two slides of math
would mean to be done to show that that is actually the direction of the largest increase but it is
if you are interested ask me on the consultations or practicals I will be happy to show you but I
will not do it now so if we want the direction of the largest decrease look at the gradient
and then we will go backwards right because well if it increases the mode that way then it decreases
the most in the other in the other direction so our gradient descent what it will do is that
it will come into gradient and then when you are in this lost function space you have one set of
theta and you will just go in the opposite direction where the gradient is pointing to and you
need to choose the length of the steps which are going to make right because we have just the
direction but you need to say okay I would just like a steps of this length or something if you
imagine this this situation then if the steps are too large something wrong can happen in a sense
that you might not hit the sweet spot in the end because you are making large steps but if the steps
are too small it might take you a long time to actually get to the correct place so choosing
the size of the step will be some choice which means it's another hyper parameter right and this
is called learning create so the learning create is this factor like the ratio of the gradient
which we really use when when we perform step so this sounds pretty simple but surprisingly strong
so even the largest models like like I don't know to gpd or something are being trained by some
small variant of this algorithm actually so we need to compute the gradient that the largest
problem right once we have the gradient then performing this will be a piece of cake so how
to get the gradient so first we know that our loss is an expectation over the training data
yes question sorry if it is yeah okay so the question is what happens if the function is not
different so I haven't said it explicitly but this wouldn't work so for this to work we require
our loss function to be differentiable and it's a very very good remark hopefully our loss is
which will use will be differentiable but for cases where the loss functions are not different
iable this is just going to be used there are some other algorithms what to do in these cases
and they are all less efficient in a sense that they either require a large compute or or more
data to work well so whenever we'll train our neural networks our losses will always be
differentiable but the good thing is that the MLE tends to generate losses which are differentiable
so first let's get the rate of the expectation over the training data so one thing what we can do
is when we compute the loss we will actually go over all the training examples compute the
gradients of the individual per example or says and then I bridge it them out this which this
works fine the only problem is the computational requirements if you are training set contains
a million of images like in the image and data set then you will spend an hour computing the
gradients over all trains set and then you'll say I have the gradient then you'll do another bit
okay 100,000 more to go right so the problem with that is that it is not efficient so what we could
do is instead of computing the gradient exactly we could estimate it right you could come with an
estimator which will give us some some estimator approximation of the gradient and because the
loss is just an expectation over something we have seen an unbiased estimator right you know that we
could have taken one changing example compute the the gradient on it it would tell us how we should
change the data so that this one example would be better predicted by the network and this is an
unbiased estimate of the whole gradient because we're on average if you would consider all the
training examples that's exactly the gradient of the whole trains of course there is some noise in
it but we will be doing thousands tens of thousands or hundreds of thousands of updates so
if one of the steps in the in the gradient descent has some kind of an error it doesn't matter
too much because it will cancel out because we are doing so many so this version of a gradient
it is called sgd the s-force to castake sometimes called online gradient descent and we just approximate
this estimated by computing gradient of a single example so now we have got two approaches one take one
example compute the other gradient or take all the training examples so there's a huge of ranges
of possibilities in between and in practice that actually the best what we can do so when we will
compute the gradient we will estimate it but not by single training example but by some small number
of training examples we can imagine twenty sixty four these are the ranges which we will use and this
is called a batch of examples it's in this case imagine that if I wanted to estimate the height
if I didn't choose just one student but I would choose five students and then take their average
the channel that I will be off will get smaller because the channel that I will choose five
with large students or five small students would diminish when when the alternative is just taking
one large or one small student so this will make the estimate better so formally the variance
of the estimate would be lower but it is still reasonably efficient because even in the training
set contains an element of examples I would just use a small fixed number of them thirty two for
example to to get the estimate so it would work reasonably quickly yes yes yes exactly so when we do this
one step of the gradient descent we use one or some number of examples to get the estimate of that
and once we have done that step we will choose another set of examples to give us another gradient
for another set of examples it will be there on on the slides later after we we we we decide how to
compute these per loss gradients yes so the question is how to choose the batch size right
should we use I don't know ten percent of the thing example to always three or something so
there is no like definite answer to that but when you start it usually works by by starting
with something like 64 examples 1208 something like large tens usually works for all the data
generally choosing the batch size will be more technically motivated by the fact how long it takes to
perform the computation or how much memory it needs so generally we will use the largest batch size
which will fit in our computation requirements because better larger batch size means better
estimates but at some point it will be ineffective to spend more time doing the computation so most
later will be a technical choice which will influence the performance kind of a little and we will see
it on the example sometimes when it's too small then the optimization might fail or might not
work that well because the noise in every example is just too large so generally we will want
large which size is but not that we either cannot compute it or it would take too long but the
influence of the batch size on the resulting model is not very larger is some but it's not too large
and we will discuss it all the one the next lecture yeah so the question is how do you sample the
examples to the batch well I don't think there is too much research on it but generally
like if we sample each batch independently on the others it will work in a sense that that is
exactly the the unbiased estimator but there it has some kind of practical
consequences and that's if we have generated each millibaget independently then some examples in
the train set would not make it into a batch for some number of times so the defecto standard way
is to make sure that we process all the training examples same number of times right so when we
have the training examples we usually do a random permutation of them and then just split them into
chunks of size M of very quiet batch sizes and then say these are the batches which I will use
in the first pass through the training date then this pass through the training data is called an
effort and we usually measure the the training time in the number of efforts to number of times
where each example has gotten once in a batch right so here we have the algorithm of of the
ESGD right and so well until we are happy until we have more money in the AWS and we can still
run the computation or something we will generate the batch and mostly by really using the
epoch kind of way so taking all the examples random initiative of them going to go through it
and then we will do the update so the update computes the gradient somehow we don't know how we
it and then we will do this one step of DHGD. We usually want to generate this random permutation
differently for each epoch if there would be little data there are cases where the performance
of the model is not as good if you don't randomize the batches because well the batches
includes some kind of an error depending on which examples you have chosen so if you always have the
same batches it can have some repercussions if you have a small training set if you have a largely
say it doesn't matter but generally pro which is just to take a random permutation all the time
and that that is the practice which you do because nobody knows how to do it better and there are
cases if you do a different play that the harder the performance yeah so if we sample the batches
independently and some example would be left out it's not obvious whether it's a good or bad example
it might be an important example which shows a dog in some specific
specific situations so if we do it we usually hurt the performance on the on the test
right because it's like a different view on the same object so if you imagine that some
examples wouldn't be used by the algorithm it would be the same as having less training data
and usually having less training data is what you want so from this point of view you would like
lose the variety and the number of training data you have so to usually you kind of want to do
of course some of the examples might be bad but that's the shouldn't be determined by
randomly taking choosing them from the train set but maybe identifying examples with errors or
something so usually we want to use all the training data which we have yes
you're learning that very well as we have described it the learning creates and the batches don't
really influence each other because the the the gradient should have some like fixed scale
so increasing the batch size will only change the variance of the gradient like the amount of error
but it will not influence the length so increasing the batch size usually just makes this
computation to take longer because you have more examples to process but this update should be
roughly the same length because the gradients on average have some kind of fixed length so
what you are probably referring to is that sometimes when we increase the batch size we also increase
the learning crave there are some reasons for doing that and that that makes like physically
connect these two quantities but as described the the learning crate and the batch size are
kind of independent so let's go back to the last thing which we need to sort out
before we will I don't I cannot use it so the large thing we need to sort out
before we will be able to actually implement the the gradient descent
and in order also the last thing which we need to do is we need to be able to compute
the the gradient of the very example loss so the partial derivatives of the loss with respect
to all the parameters to all the traders luckily for neural networks it can be done
quite effectively I'm not saying that there will be formulas there but from the runtime kind of you
there is an efficient algorithm which will give us the the gradient so let me
complete the neural network we start the inputs and we compute the outputs and that's
that's sometimes called a forward pass so we have a neural network so let's assume that
the nodes of the neural network are you one to you and and we have already numbered them in topological
order so we know that we can evaluate them in this specific numbering because all the predecessors
will always be evaluated right so here I will formalize the how the neural network is computed
like the forward propagation is just algorithm of what we have set or a few times so let's say
that when we have this neural network like where there are these nodes some of them are input nodes
the some of them are output nodes and each node has some set of pre-disassers
the values of the neurons which it uses which are the node is A and then each neuron has
some activation function which it uses when it sums up all its inputs
so in order to compute the output of such a neural network well we will go through the nodes
one by one and for each node we will call get the values of all the predecessors they have been
already computed because they have smaller index we will apply the activation function and we will
get all the values so let's say that the output of the neural network is the last node so we
will return it so this is like a forward propagation in a sense that it computes the network
by going through the nodes once and now we would like to compute the derivatives of the loss
with respect to all parameters of theta which there are and we will be able to do it by using
the so-called of chain rule of derivatives so if you are from an ff or we're going on to some
other course for calculus you may have seen what the derivative of the composed function is
the derivative of fgx is f prime gx times g prime x so this is some thank you to which you might
have seen instead of this formulation we will use the formulation with the partial derivatives
which is good in the sense that it explicitly tell us which we are computing the derivative with
respect for so to rewrite this I could say the derivative of fgx with respect to x can be computed
by computing the derivative of fgx with respect to gx right so we can imagine that I would say okay
so this is some y and I am computing the derivative of f with respect to the y and then times
the derivative of gx with respect to x this is something I won't be proving here but it's something
what the calculus classes do at the beginning when they talk about derivatives and this is actually
the only thing which we need to be able to compute all our partial derivatives so how are we going
to apply it in our case so imagine a node which has got some number of inputs and it produces
then output by applying some function f to all these inputs and then let's say that we are computing
the derivatives and somebody already tells us what the derivative of the loss with respect to this
y to this value is and what we want to do or what we will be able to do is we will be able to use it
to compute the derivative of the loss with respect to these inputs to this node right and well
how it is well the chain rule of the derivative is very helpful here because if we want to compute
the loss derivative of the loss with respect to one of the inputs let's say this one
then we can do it by computing the derivative of the loss with respect to y the output of the node f
and then by computing the derivative of the output of the node with respect to the input of the node
right so when we have a node we know that we can compute the derivative of the node in a sense that
well if one of the inputs changes what effectively will have to be output of the node there are
several of these partial derivatives right differently probably for for each input because each
input can play play a different role so when we want to compute the derivatives
we will go in the opposite direction we will start from the end of the neural network we will go
backwards and whenever we go through a node to one of these inputs what we will do is we will take the
old value and multiply it by the derivative of that specific nodes of the derivative of the
output of the node with respect to the corresponding input so in the forward pass we just apply the
activation function in the backward pass we multiply the incoming derivative by the partial derivative
of the nodes itself that's simple to compute and actually does exactly what we need it to do
so we will do this in the so-called backward pass right in the forward we went through the network
in the backward we will go the opposite way and whenever we arrive at the node we will
multiply by the partial derivatives and we continue and this way we will arrive at the partial derivatives
of the loss with respect to all the nodes which there are in our neural network actually one of the
examples is somehow somehow missing in a sense that here we always had one output so what we don't
know yet is what would happen if some quantity would be used multiple times in our neural network
well but that's simple if we have the derivatives of the loss with respect to y1 and y2
what will these two quantities mean they mean if you change x the loss will change somehow through
y1 and then the loss will change somehow differently through y2 so the overall effect will be the
sum of those derivatives it will change somehow through this y1 somehow through this y2 so we can just
add them up all together and now we have got enough gadgets like these two images are enough to
cover all neural networks you can come up with and so they they give you an algorithm of how
you can actually compute the partial derivatives so let's write it down as an algorithm
we will use the same notation as before so we have got nodes which goes from u1 to un the un the last
output that will be the loss because we want to compute the derivative of the loss so we will
add the neural the loss computation to the neural network during training right or the
neural network will produce it output but we will also make it to compute the loss function
and so our goal will be to compute the derivative of the last output of the loss with respect to all
nodes which are there in the network some of these nodes are going to be the parameters
the theta which we want some of them are just intermediate activations which we don't
need per say but the algorithm needs them to to actually finish so how does the algorithm works
well we started the end so first our goal is to compute the partial derivative of the end node with
respect to itself well that's simple just what and now we will go backwards whenever we are at
no UI we will consider all its descendants right so all uj's which use this node and for each
we will use the chain rules of derivative so we will say okay through this path the derivative
of the un with respect to this ui can be computed as the derivative of the un with respect to uj
that's gj times the derivative of this quantity with respect to this one so this is the
the derivative of the uj node itself I so that's this part and that's it we will just sum them up
and we will get the derivative of un with respect to this ui it's done and then we will go
backwards backwards until we arrive at the end and we can just return all the derivatives which we have
computed and we are done well I wouldn't say recursive I would say iterative in a sense that
like there is just one forecycle but it's recursive in a sense that we are using the quantities which we
computed in the previous steps but we are not formally like calling ourselves again so I would just say
iterative probably so in practice these nodes they are usually not just like notes which would
produce real numbers in practice for effective in effective it they will usually get the whole
vector of inputs and produce a whole vector of outputs so you can imagine as like being like super
nodes which contains lots of the individual small nodes you can you can think of them as layers right
but the algorithm is still the same is just that when you compute the derivatives they will no longer
be just real numbers but they will be larger quantities so if we have a function with a vector on
the input and the vector on the output then there is a whole matrix of derivatives like every output
with respect to every input but this this doesn't change anything on the algorithm is just that
these these quantities which you have here can easily be vectors and matrices but other than that
the algorithms still stands so SGD is great what does the SGD gives us well first thing is
the gradient is probably converges if we have used the correct gradient but we are just using the
estimates of the gradient but well luckily we will also get to optimize our model we will also get
to the minimum of the loss even if the estimates are just noisy so to say this formally let's
say that we have a function which is convex and continues by convex I mean it looks like this this
bowl where you can do pancakes or something the convex function is interesting in a sense that
if you find a local minimum the one which you cannot grow anywhere to decrease the loss function
it's the global minimum of the whole function because well it only has one minimum at the bottom
right so here it's easy to say something about the algorithm so if we have a function we have
noisy estimates of the real gradients and the loss is so continuous and convex then repeating this process
we'll converge to the unique optimum almost surely if the sequence of the learning crates
are reasonable so here what we will need to do is we need the learning crates to to change in time
because well if the the learning crates would have some constant value we could always end up
stepping over the minimum right so in order to actually get to the minimum we will need the
the length of the steps to decrease so the formal requirement which we had for the SGD to converge
is first you don't know how large you will need to go in the last one scape in order to find
the minimum right there can be examples where the distance which you need to travel will be
further and further away so in order to be able to travel like any distance in the loss landscape
we need the sum of the learning crates to be unbounded it it would be bounded it would somehow limit
the distance which we could travel and if we start in a bad place we couldn't finish the optimization
on the other hand we won the learning crates to decrease but formally what we will want is we will
want the sum of the squares of the learning crates to be bounded this implies the fact that they
are going to to zero immediately because they didn't this wouldn't be bounded but this is slightly
stronger but generally apart from the requirement on the length of the steps we will converge for
a convex function which is woohoo win for our algorithm even if it's very trivial for convex functions
it works well what about the non convex functions well for non convex functions we cannot say too much
just that we will find a local minimum so if you imagine or select that the algorithm can easily
end up here and stop that's what it does we cannot really hope to get to the global minimum
because finding the global minimum of just the booty in function is at least np hard
if we imagine a function which would tell you I don't know for each dimensionality whether the
clause is fulfilled or not some some kind of logical formula then the global minimum would tell you whether
some some formulas solve a satisfiable or not and it's definitely difficult so finding a general
minimum of any function that's like super strong so we cannot really hope for an algorithm which
will always get you a global minimum so the one which gives you the local minimum for the non
convex function that's in some sense the best what we can practically hope for so now we could have
formulated the SGD algorithm because we know everything now you can implement all the steps here
and on very well on the only assignments to make sure that you actually understand all the necessary
steps that will be the only time when we will be doing it manually and then we would just keep
the frameworks or use the frameworks to do both other by themselves but still at this point it might
seem so abstract that seeing it at least once what exactly is happening there is definitely the
useful thing so we will spend the rest of the lecture by describing slightly improved versions
of the SGD algorithm I said that the SGD algorithm is being used and it's true we will just
do some minor adjustments to it to make it better so what are the problems of the SGD algorithm
I just said that it converges like with the right sequence of learning crazy all the time
so that's true but sometimes it might take too long right sometimes it might this approach might
not be the most efficient and to consider one specific example consider a specific loss function
which looks like like grand canyon like so it's like a valley which is which is very narrow
sorry which is very narrow when it's very steep wall so we can imagine that there is like a
canyon going down here to the stairs which goes really steeply up and then slowly it decreases in
this direction so the local minimum is somewhere here so if you imagine me doing the SGD so let's say
that I'm here so how how the gradient looks like well there is some small change in
that direction but if I'm standing on the on the steep wall there is an extremely large change
in that direction to the gradient is pointing that direction so what where the direction where I
want to go and SGD is here so a lot of the distance in this direction and very small amount of
distance in direction which I actually care about because that's where the minimum actually is
right so the SGD would be converging it would be doing this and most of the time I would be going
in this dimension which doesn't really help me but is the direction of the steepest this and
right so it would be ranging right and it would take me a long time to get a right so I will
speed it up so there is a nice nice link here which I haven't opened but I will do it quickly
which allows us to fiddle okay yeah but we'll fiddle afterwards with it so one very engineering
way how we can improve this situation is to say okay so the problem here was that the gradient was
very large in the direction which we didn't care about but we could find what this direction is
because we were oscillating very much right it was plus minus plus minus back for back part there so
if we consider not just one gradient but some some history of the gradient I don't know 10
past gradients we could identify the issue right because in one of these changes
directed directions and one of the fedas we would be back for back for large small
large more quantities so what we will do is we can add some kind of memory or like physical
momentum to our algorithm so the the conceptual idea is instead of using one gradient we could consider
the whole I don't know history of I don't know 100 steps and we could consider the average
of the gradients from the last 100 steps right that will nicely smooth out or ignore this
direction right where it was large and small so the average would cancel out while this the
contribution of this direction which goes to the to the minimum would stay strong well weak but consistent
in all the gradient so so it would it would stay there however computing this thing like the
average of the last 100 gradient that's not really effective because well we would need to keep
these 100 gradients and when a new gradient comes we would forget subtract probably the first one
right and then continue but we would always need to keep these 100 gradients so that we know what to
we don't want to think about them for like not to forget anything because if we wouldn't
forget anything then that the gradients from very far past would be very off like the idea is that from
the just few steps in the past they are probably kind of right they are still relevant to the
direction we're going to go but but we want to be able to forget so if not
if not the arithmetic average we can use some other way of keeping the past gradients where
we will forget and we will do this exactly by by using this kind of momentum so what we will with
you will have the V which will be some kind of a history of where the gradient was pointing in the
few past steps and we will update it in this way well in every step we will keep let's say 90% of
the gradient from the past and then add the the current gradient which we want to consider right
so this way in every time that we kind of forget some portion of the history but we keep
but say 90% so this way each gradient this is being used for some number of steps each
one more step which we will make the influence of that will get smaller and smaller on the final
output so we are forgetting things which are further and further in the past so this also have
a physical interpretation imagine that I have got like a ball which I will put here and I will just
go and it will go down and the gradient it will act as an acceleration but it will be like the
direction where the gravitational ball will make it go faster in a direction but it has a momentum
from the previous timestamps right so the acceleration only changes the direction where all
the changes the velocity where the ball is going and if we have some some friction then then this
velocity gets decreased in every time step by by being breaked by by the ground or something so
this is like the physical analog of what this is doing and this is why it's called the momentum
also in a sense that instead of considering the gradients to be the velocities we can consider it
to be accelerations which will influence the velocity which we have accumulated in the few past steps
so this kind of solution kind of helps especially in the situations where we have got a loss
function which is like steep in one direction and not in the other so this is the same similar
example as what I was trying to show you here so let's say that we start here that is the minimum
here and so if we fill with a with a learning crate and if it's small we don't make much
progress and if we do more and more steps it looks nicely we are getting closer but because the
step is larger we are doing large and large jumps at the beginning to the point where it will
diverge because the steps will be so bad that it will just yeah go away right but if we instead
add the momentum right it will decrease the oscillations in this one specific direction because
they will cancel out in the momentum while on the other hand it will increase the the progress which
we make in the in the direction that that we care about right and if we set it correctly we can
converge kind of nicely here you can see that we are still oscillating but we are at the same time
making a lot of progress in the direction which is consistently pointing in one in one direction
so this is like an engine yearing improvement but in practice it helps with these functions
where in some dimensions the function changes rapidly and in some other dimensions it changes
just very likely however it only helps when we have gotten to the situation where we are oscillating
in that in the dimension right it could happen that we have a loss which goes very quickly in one
dimension and there is slowly in the other but if it's decreasing in both this will not help you in
the sense that you will still going faster in the dimension where the the decrease is steeper
because the momentum does something interesting only once you will hit the bottom and so the
pluses and minuses will cancel out right until that you will go faster and and fast it's like
with the ball kind of makes sense like you you are going quicker and quicker in the direction
where my function is increasing something interesting will happen only when you hit the bottom
where the opposite force is well well cancel out although when we are computing this
this momentum there are actually two ways how we can compute the gradient like the straight forward
way is just to compute gradient update the momentum and do the update but we can do it slightly
better because when we are in some point we know that we will travel some distance just by using
the momentum right so we will get here anyway so when we are computing the gradient instead of
computing it in the originals of parameters because we will be moving here anyway it would be
better to actually compute the gradient here because here it might be different and it was there
and what we do is when we compose them is we compute the gradient here and then we combine it
with this so we kind of also assume that the gradient will stay the same even after this
momentum step which is not necessarily the case so we can easily rearrange the computation to
update the theta already with the change it will happen do the momentum step then compute the
gradient which will be more relevant to these two to set of parameters and then update the theta
only by the amount which has not yet happened now this seems like a very minor change in the
taste in some of you but in practice it works better and there are also some theoretical results
which says that is actually somehow isn't particularly better in some cases than the previous case
so if you are doing it you won't do it in this kind of way and this is called an ester of momentum
just because of the guy who described it for the first time this is really just a minor detail so
we can forget about it just saying it that if you say an ester of momentum is just a slightly
better way of computing the moment so two more algorithms and we will be done now we have gotten
out of the Middle Ages of the plurneying of these 90s and this is the first time we are doing
something more more recent so we are already in 2011 and in 2011 somebody came up with an
algorithm which ended up to be very interesting and influential and the core idea of it is still being
used I'm definitely not saying they were in other algorithms there were dozens of algorithms which
people proposed before that and after that I'm just mentioning it because it's the one where the idea
was so good that it is prevailing until now and so the idea was this we don't like the cases
where the loss functions goes very steep in one direction let's say in this one well in the other
direction it's very it's very shallow in that case it's difficult to do the update correctly
right because when we have the gradient and we update the theta by specifying the learning
create then if this gradient has one dimension where the numbers are large like thousands and
some dimensions where it's small just 0.1s or something then it's difficult to find a good
learning crate because for for these large changes I would like small learning crate probably
for these very small gradients I would like larger learning crate so that we do more progress
in the direction and while momentum helps us once we start oscillating it doesn't help us
when they are not oscillating so what we could do well a simple idea is that we could somehow
normalize the gradients so that they would have equal sizes right if you can imagine the things
okay okay so in this direction the gradients are usually thousands so I will scale them down so they
are just once usually in the other dimension the gradients could be 10 then so I could scale them up
so that they are roughly once and now if the gradients are roughly the same size for all the
theta then the progress in all the the dimensions all the directions would be the same it would also
help up with this example right because the the gradients in this direction which are very small
would be scaled up while this direction would be scaled down so instead of doing this back for
back 4 we would like point the direction more to the to the place where we actually want to go
and so how to implement this idea well in this other grad algorithm that's adaptive gradient
what we will do is we will scale each dimension of the gradient differently and we will do it by
computing the sum of the squares of the gradients right so this is being computed element wise
so formally we should write it as g times g but often you just teach it square so that's what people
mean so for each dimension we will compute for each theta we will compute the sum of squares
of the gradient for it and the idea is that if in one direction the gradient is very small
the sum of squares will be very small on the other hand if some direction the the gradient
are very large the sum will be very large and then we will normalize the gradient
by dividing each element with the square root of the sum of square so if the sum is small we
will make it larger if the sum is large we will make it smaller so after this the idea is that
the normalized gradient should roughly have comparable size for all the theta's
another way how you can look at it is that we have the gradient but the learning
crate will no longer be fixed for all the dimensions but we will change the learning crate for
all the directions and we will change the learning crate by dividing it by the square root
of the sum of squares so learning crate for one specific theta will be small if the the
gradients are very large but the other large I do not need to use such large updates in the
on the other hand if the the gradients in some direction are very small I will increase the learning
crate so that it makes more progress in the direction so that's why the algorithms are somehow
also called algorithms adaptive learning crate because we change the learning crate for each
dimension specifically like both of these views are actually very similar so that's the only
change in the algorithm right is this this this normalization thing if this would be where
would not be there it would be an SGD now we need to consider the the fact where the
sum of the gradients is zero or very close to zero that would be bad so to to guard ourselves
between some kind of explore before and against some kind of explosion or something we will add
some small plus epsilon there usually some small constant like 10 to minus 7 or 10 to minus 8
which is there just to make sure that this inversion will not explode yes the question sorry
yes so in the original algorithm our gets just larger and larger I will get to the
total problem which you are getting it on the next slide so I'll just keep it there but but that's
definitely weird thing okay so this algorithm can be shown to work very well for for
context losses where it can be shown that it converges quickly enough so even if the
R gets larger and larger it will not make any problem and it will converge nicely but exactly as
you said for non convex losses if we are not guaranteed to converge quickly enough the ever increasing
R might stall the progress of our training right it might get larger and larger and larger
making the learning creates smaller and smaller to the to the situation where our
adaptive learning will be so small that we would not make a problem there's definitely a problem
especially for neural networks which are very very non convex in most cases so to see that it's a
problem I'm just just doing some some math to show that the increase of R can make the
the learning create decrease dramatically so imagine that in the first T steps the gradients
don't really decrease so they are roughly of the same size right denoted as G 0 so after T steps
R will be roughly T times the size of the magnitude of the gradient so then when we divide
the learning create by the square root of that we are dividing by the R divided R times sorry G 0
times T so and we can take it take it out of the discriminator and pull it up so effectively we
could be decreasing the learning create well just because the error gets larger and large so this
was an obvious problem for applicability of this to the neural networks it's not like you cannot
training your own network you just need to make sure that it converges quickly enough so they were a
few papers like dozens of papers which where this worked better than a G D but it only worked for models
which converges quickly for example for the image models it wasn't so so people immediately
fix that so the slight update of that the RMS program algorithm it was never published in a paper
just mentioned in some lecture notes by Hinton or someone and but people started using it
there are thousands of models trained by if you don't need even a paper if you have a good idea
and so the how to deal with the problem well let's change R so that R is roughly an expectation
of the G squared instead of the sum of the G squared from the beginning right that's the
problem was that the R with large and larger so instead let R estimate the the the second moment of
the gradient but not by summing them but by really looking at the expectation this way it will not
get larger still the the nice property of the gradients which are large will be scaled down which
are small to the scale that will be will be kept and so how to do this how to estimate R which will
be the expectation of G squared and we would like the expectation to consider last I don't know
pens last dozens of steps we don't want to remember too much because the gradient change
but we don't we just cannot consider one step or something because then the normalization will
give us a least one or something so we need to remember some fixed history so again we could compute
the average over for example the last 20 steps we could do it is just ineffective because
we would need to remember all these 20 G squares so that we can forget things in the past and
the gradients can easily be tens tens of gigabytes of memory so we want to avoid it so instead
we will use the same or similar idea as before we will do the updating the following way let's say
that in each step we will keep 90% of our estimate from the past and the new 10% will come from
the new estimate right so in each step we will take 90% of the old estimate and the remaining
thing 10% we will take from the current gradient so this way the current gradient influenced the
output and then the influence of one gradient we will get smaller and smaller the more in the
future we are because in each step we will just take 90% of it and then again 90% of that 90%
and so on and so so this kind of averaging is called exponential moving average
right which allows you to compute the average of some some history which is efficient because
it is easy to do and the influence of things which are further in the past gets forgotten but still
on average it tracks the expectation so this is solved the problem with getting our large because
well instead of summing it we are like forgetting things in the past right like the summing thing
just sums things up this thing makes the sum of the things in the past smaller so that always
we are summing the the gradients from the past steps with weights which sum to one
so yes the question so what does the name oh yep so the question is what the name of the
algorithm means so this is root mean square so this is the like root mean is like the
E average and the square is this the prophesizing propagation many of the algorithms are called
probe something are probe back probe so so I think that's that's there but it's just the name
of how you compute the estimates or how you do the normalization or of the learning grades
great and that brings us to the last algorithm which we will talk about today and that's
Adam is an algorithm which was proposed 10 years ago and it has not only survived 10 years but it is
still the algorithm of choice when you want to start a model somehow people were able to
stumble open an algorithm which 10 years of research and hundreds of papers could not actually
efficiently surpass generally so whenever we will start an internet work we will just start with an
atom so what does atom that's atom combines as gd with momentum and the RMS probe
one thing which is problematic in RMS probe is that this R is a biased estimate of G square
is specially especially at the beginning of a training so imagine after the first step what happens
R becomes starts a zero and then after the first step we keep 90% of zero and then 10% of the
of the G square so R is 0.1 times G square but what you really expect is R to be G square
by because that the best unbiased estimate of the second moment of the gradient I have got one
sample so that should be the estimate but because it tries to remember the past then it remembers
90% of nothing which happened because we don't have any other estimates from the past so it
has a bad estimate at the beginning by saying yeah I think the G square is actually 10% of
the actually should be so this is something what Adam does right so we will somehow make sure
that our estimate of the second gradient is actually unbiased even at the beginning of the training
and I said that it also combines momentum so we will estimate not just the second moment
of the gradient but also the first moment let's like in the moment when we wanted not just
one gradient but some kind of an average of gradient for few past steps so we will do these two
estimates of the first and of the second moment we will have a different momentum like different
constants for them so 90% for for the first and something like this for the second you don't
need to fiddle with them both of the time and we will get to the cases where you where you
need to get a question so the question is why we cannot initialize the R or S with a gradient
here I have liked the instead of zero well because after the second step most of the estimate would
come from the first step this 90% while just 10% would come from the current one and that's still
seems a bit unfair because it would probably be better to take more from the second step because
it would most of that would come from the first step while just a small portion would come from
the second one so it could be done it would already be unbiased
yeah yeah so the question is whether we could do it like more uniformly start with half half
third third or something and then gradually move to the current distribution that's that's all
all good ideas which are going in the in the right directions
so the the question is this unbiased let's go the way it's a consistent estimator in a
sense that after many steps this will be the right estimate so this influence only the
beginning of the planning this is true but it's more like thousands of steps instead of plans
because the the beta 2 is usually 999 things but I will get there but the problematic part is that
this makes the initial update large but because we will be multiplying by what we will be
doing one over a square root of that and we have made it small so without the biased
correction the initial steps are larger than they should be and it can happen that the model
will actually diverge or do something bad in the beginning of the training so the SGD
is especially sensitive to the initial steps of the training if you get to a bad place
in the few few tens or hundreds of steps you will not you their cases where you will not
recover by training of any length you just gone in the right better direction at the beginning
so even if it seems like it's minor in practice it makes non trivial difference in some cases
so it's like better say than sorry in a sense that it helps sometimes and it doesn't hurt you in
the other cases so so doing it correctly seems to help it's not fundamentally large but it's
still something and why or ourselves yep
so the question is whether we want to change the beta or
yeah so it could be done if we have reformulated the algorithm to somehow like nicely increase
the beta and then things would kind of work out it's not as simple as you say it but
but it can be done as well and this is all true so there are many ways how to how to deal with this
problem and I will show you what the authors of Adam decided to do with it which is not different
from any of the proposed ocean that much so let's think about of what the very of our estimate
will be after t steps so after t steps it will be 1 minus beta 1 times g and then there will be
beta 1 times st minus 1 so the st minus 1 well that's beta 1 st minus 2 plus 1 minus beta 1
g t minus 1 right and you could do and expand this again and again
so in the end what will what will happen is that there will be gradients there and the gradients
are always multiplied by 1 minus beta 1 and then they are multiplied by some number of beta 1
times depending on how far in the past they were right so the after t steps we have
weighted combination of the gradients from the first t steps and each gradient is multiplied by
1 minus beta 1 and the beta to the number of steps how far in the history we have we have seen
this gradient so if I would be to draw on this it would look like this the reason gradient
is multiplied by 1 minus beta and then it's by minus beta times beta beta 1 1 minus beta times beta
2 and so each other column is beta times smaller than the previous one and this works out
in a sense that if we sum all the weights the sum is 1 so it's like a weighted combination
of the gradients but the one where the total sum of the weights is 1 so it's like an unbiased
estimate of whatever we want right it sums to one because well the sum of this infinite geometric
series is 1 over 1 minus beta 1 if you remember the formula it's not that it's called the proof
and and this cancels out nicely with this part so it's fine this is a nice unbiased every
so what happens when we don't have a full history well when we don't have a full history then
some of these are not there right we haven't seen them they are just zeroes and arrest them
so in the situation where we don't have the infinite history but only key steps what we would
want is we would want to compute the average only using the T last gradients and we would like
the weights of these steps or gradients to sum to 1 because the fact that these these weights
sum to something smaller than 1 is creating the bias right so what we need to do is we just need
to to normalize the weights which we use so that they always sum to sum to 1 over the last
T steps so how we are going to do that well we need to compute what the sum of these weights
is if we consider T steps so this can be computed quite easily because well this sum will be there
by multiplied by 1 and then we will have the same sum just shift it by 1 with with the opposite
sign so it will cancel out most of it so we will just get the first element of the first sum
and then the last element of the second sum so the sum is somehow nice right if you didn't follow
it can just be computed so what we just need to do is we need to divide each ST with
1 minus p to 1 to T because it's small it's too small and we can actually
exactly compute the factor by by by by how much so we can just expand it so in the first step
we will multiply it by 90% then we will multiply by something small and small and small and
generally the factor will converge to 1 right because this thing will converge to 0 exactly as
you said the bias will go away and contest the mistake so I think that what you propose also
would be working as well it's just that the authors use this so we can do the bias correction which
will make sure that our estimates are unbiased and that's it so I've got also some
animations which shows how this works so I will just show you one because I have already made
the lecture longer but I wanted to synchronize with the check parallel so here is the visualization
of the the progress of optimization of several algorithms this is a visualization which is
pre-aderm so it doesn't contain adenum but but the RMS probe there is it's like a good estimate
of what would happen and so the what happens is that the SGD has kind of trouble of making
progress if you increase the learning rate for it is rather most likely in the other direction so
this is in some sense the best way what the SGD can do with the momentum you accelerate nicely
in the direction where the decrease is is consistent on the other hand the momentum has the
property of tending to overshoot where it wants to go so it's not unusual for for the moment
to come back but the interesting thing about the RMS probe slash adem is that they try to make
the same progress in every step like independently on the steepness the algorithm is designed so
that it makes like the constant distance on the image which you are seeing right because independently
on whether things are steep or or or or shallow we always try to normalize it so that should
mean that on these images you always do like a step of a constant length independently on the shape
of the of the loss function that also has this the the the oscillation which is there in this
direction is also like one of the properties because if the gradients are very small like if you
are nearly at the minimum it's like scales them up so you might seem seen some some like what
they're talking about are these black like back and forth erotic movements here because at that
point we are already at the minimum in one of the directions so we scale the radians up but that's
the price which we pay for making the like constant amount of progress in every in every point
of the of the landscape right it's you we look at here it's nicely visible that the like
momentas the the violet and the green they are at the top and once they realize where to go you'll
start accelerating rapidly in the direction while the RMS probe tries to go the same speed independently
on whether the loss function is flat or or it goes down a lot so thank you very much thank you for
all the questions it was very interesting to answer all of them and let's see each other on the
tomorrow's practicals are on the recordings and at any point if you have any question please don't
want us a date to ask enjoy the date
